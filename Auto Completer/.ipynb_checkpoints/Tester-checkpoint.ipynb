{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ed3430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hskay\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Input, Attention, Flatten\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec09289",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        word_list = [line.strip().lower() for line in lines]\n",
    "\n",
    "# Sample data for outputs\n",
    "vocab = word_list\n",
    "characters = set(''.join(word_list))\n",
    "\n",
    "# Encoding dictionaries\n",
    "char_to_id = {char: i + 1 for i, char in enumerate(sorted(characters))}\n",
    "char_to_id[\" \"] = 0\n",
    "word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Inverse for decoding\n",
    "id_to_word = {i: word for word, i in word_to_id.items()}\n",
    "\n",
    "max_char_length = max(len(word) for word in word_list)  # Maximum length of words in `data`\n",
    "\n",
    "# Convert input words to encoded characters\n",
    "X=[]\n",
    "Y=[]\n",
    "for word in word_list:\n",
    "    for i in range(1,len(word)):\n",
    "        x=[]\n",
    "        for char in word[:i]:\n",
    "            x.append(char_to_id[char])\n",
    "        X.append(x)\n",
    "        Y.append(word_to_id[word])\n",
    "\n",
    "X_padded = pad_sequences(X, maxlen=max_char_length, padding='post')\n",
    "X_padded = np.array(X_padded)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03789328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = len(vocab)  # Number of words in the vocabulary\n",
    "char_vocab_size = 256  # Assuming ASCII characters for simplicity\n",
    "embedding_dim = 64\n",
    "rnn_units = 128\n",
    "\n",
    "def model_create():\n",
    "    # Model Definition\n",
    "    input_chars = Input(shape=(None,), dtype='int32')\n",
    "    char_embeddings = Embedding(input_dim=char_vocab_size, output_dim=embedding_dim)(input_chars)\n",
    "    rnn_out, state = SimpleRNN(units=rnn_units, return_sequences=True, return_state=True)(char_embeddings)\n",
    "\n",
    "    # Attention mechanism\n",
    "    query = tf.expand_dims(state, 1)  # Use the last RNN state as the query\n",
    "    attention_output, attention_weights = Attention(use_scale=True)([query, rnn_out], return_attention_scores=True)\n",
    "    flattened_output = Flatten()(attention_output)\n",
    "\n",
    "    # Output layer to predict the word index\n",
    "    word_pred = Dense(vocab_size, activation='softmax')(flattened_output)\n",
    "\n",
    "    model = Model(inputs=input_chars, outputs=word_pred)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4f95101",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_create()\n",
    "model2 = model_create()\n",
    "model3 = model_create()\n",
    "model4 = model_create()\n",
    "model5 = model_create()\n",
    "model6 = model_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caf42470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#model.load_weights(weights_path)\n",
    "model.load_weights('03102024Final1.h5')\n",
    "model2.load_weights('03102024Final1_2.h5')\n",
    "model3.load_weights('03102024Final2.h5')\n",
    "model4.load_weights('03102024Final2_2.h5')\n",
    "model5.load_weights('03102024Final3.h5')\n",
    "model6.load_weights('03102024Final3_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df0d6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(models,input_chars):\n",
    "    input_ids = [char_to_id.get(char, 0) for char in input_chars]\n",
    "    input_padded = pad_sequences([input_ids], maxlen=max_char_length, padding='post')\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = models.predict(input_padded)\n",
    "    # Get the top 3 word IDs and their probabilities\n",
    "    top_3_indices = np.argsort(prediction[0])[-3:][::-1]  # Sort and get top 3 indices\n",
    "    top_3_probs = np.sort(prediction[0])[-3:][::-1]  # Sort and get top 3 probabilities\n",
    "    top_3_words = [id_to_word[idx] for idx in top_3_indices]\n",
    "    top_3_percentages = [f\"{prob*100:.2f}%\" for prob in top_3_probs]\n",
    "    for word, percentage in zip(top_3_words, top_3_percentages):\n",
    "        print(f\"Word: {word}, Softmax: {percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ce98847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "Word: athletes, Softmax: 1.22%\n",
      "Word: academics, Softmax: 1.01%\n",
      "Word: academic, Softmax: 0.96%\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Word: alternatives, Softmax: 3.33%\n",
      "Word: allah, Softmax: 2.77%\n",
      "Word: alternatively, Softmax: 2.70%\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Word: allow, Softmax: 2.69%\n",
      "Word: colorado, Softmax: 2.52%\n",
      "Word: collaboration, Softmax: 2.39%\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Word: altered, Softmax: 5.39%\n",
      "Word: allowing, Softmax: 4.01%\n",
      "Word: alternatively, Softmax: 3.51%\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Word: altered, Softmax: 4.66%\n",
      "Word: almost, Softmax: 3.52%\n",
      "Word: alternatively, Softmax: 3.33%\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Word: albany, Softmax: 4.04%\n",
      "Word: alto, Softmax: 3.45%\n",
      "Word: algeria, Softmax: 3.19%\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "inputs=\"al\"\n",
    "prediction(model,inputs)  # epochs=300, batch_size=32\n",
    "prediction(model2,inputs) # epochs=1000, batch_size=32\n",
    "prediction(model3,inputs) # epochs=300, batch_size=16\n",
    "prediction(model4,inputs) # epochs=1000, batch_size=16\n",
    "prediction(model5,inputs) # epochs=300, batch_size=64\n",
    "prediction(model6,inputs) # epochs=1000, batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c055f8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
