\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}

\title{RNN}
\author{HSK}
\date{April 2024}
\begin{document}
\maketitle
\section{Introduce}
\begin{eqnarray}
\label{eqn:h}
	h_t = \Theta_h(W ~h_{t-1}~+~U~x_t ~+~b)\\
\label{eqn:omega}
        \Omega_t = V ~h_{t}~+~c \\
\label{eqn:y_hat}
        \hat{y}_t = \Theta_y(\Omega_t)\\
\label{eqn:Loss}
        L_t = -~y_t~ln(\hat{y}_t)
\end{eqnarray}
Activation functions are $\Theta_h$ which represents \textit{tanh} in \eqref{eqn:h}, and $\Theta_y$ \textit{softmax} in \eqref{eqn:y_hat}.

\section{Through Time for Recurrent Neural Network}
\subsection{Softmax}
\begin{equation}\nonumber
\text{softmax} (\Omega_{t})=\hat{y}_{t}=\frac{e^{\Omega_t}}{\sum_{k=1}^{|\mathcal{A}|} e^{\Omega_{t,k}}} ~\text{for}~ t=1, \ldots, k
\end{equation}

%Since softmax is a $\mathbb{R}^{k} \rightarrow \mathbb{R}^{k}$ mapping function, most general Jacobian matrix for it:
%
%$$
%\frac{\partial S}{\partial x}=\left[\begin{array}{ccc}
%\frac{\partial S_{1}}{\partial x_{1}} & \cdots & \frac{\partial S_{1}}{\partial x_{k}} \\
%\vdots & & \\
%\frac{\partial S_{k}}{\partial x_{1}} & \cdots & \frac{\partial S_{k}}{\partial x_{k}}
%\end{array}\right]
%$$
Let us compute $\frac{\partial }{\partial \Omega_{t,j}} (\hat{y}_{i})$ for some arbitrary $i$ and $j$ :
$$
\frac{\partial \hat{y}_{i}}{\partial \Omega_{t,j}}=\frac{\partial}{\partial \Omega_{t,j}}\bigg( \frac{e^{\Omega_{t,i}}}{\sum_{k} e^{\Omega_{t,k}}}\bigg)
$$

%Let's examine the formula for division
%\begin{align*}
%f(x)&=\frac{g(x)}{h(x)}, \quad \\ f^{\prime}(x)&=\frac{g^{\prime}(x) h(x)-g(x) h^{\prime}(x)}{h(x)^{2}} 
%\end{align*}
%In our case $g_{i}=e^{x_{i}}$ and $h_{i}=\sum e^{x_{k}}$. No matter which $x_{j}$, when we com pute the derivative of $h_{i}$ with respect to $x_{j}$, the answer will always be $e^{x_{j}}$.

Since $\frac{\partial }{\partial \Omega_{t,j}}e^{\Omega_{t,k}}=0$ for $k \neq j$, we have:
$$
\frac{\partial}{\partial \Omega_{t,j}}\bigg( \sum e^{\Omega_{t,k}} \bigg) =\sum \bigg( \frac{\partial ~e^{\Omega_{t,k}}}{\partial \Omega_{t,j}} \bigg) =e^{\Omega_{t,j}}
$$
Only meaningful derivatives is obtained for $i=j$  case in the above equation for our example presented in this chapter. Recall that in our example only one of  values is a one




\begin{align*}
\frac{\partial }{\partial \Omega_{t,j}} \bigg( \frac{e^{\Omega_{t,i}}}{\sum e^{\Omega_{t,k}}} \bigg) &=\frac{e^{\Omega_{t,i}} \sum e^{\Omega_{t,k}}-e^{\Omega_{t,j}} e^{\Omega_{t,i}}}{\left(\sum e^{\Omega_{t,k}}\right)^{2}} \\
& =\frac{e^{\Omega_{t,i}}\left(\sum e^{\Omega_{t,k}}-e^{\Omega_{t,j}}\right)}{\left(\sum e^{\Omega_{t,k}}\right)^{2}} \\
& =\frac{e^{\Omega_{t,i}}}{\sum e^{\Omega_{t,k}}} \cdot\left(\frac{\sum e^{\Omega_{t,k}}}{\sum e^{\Omega_{t,k}}}-\frac{e^{\Omega_{t,j}}}{\sum e^{\Omega_{t,k}}}\right) \\
\end{align*}

\begin{equation}
\label{eqn:partial_softmax}
    \hat{y}_{t,i}\left(1-\hat{y}_{t,j}\right)
\end{equation}
\subsection{Derivative of Loss Function w.r.t. $\Omega_t$}
%Let's examine the derivative formula for logarithm
%\begin{align*}
%    & f(x)=\ln_{y} x \\
%& f^{\prime}(x)=\frac{x^{\prime}}{x} \cdot \ln_{e} y\\
%\end{align*}
Recall that cross-entropy loss is defined as:
\begin{align*}
    L &=-\sum_{t=1}^{\mathcal{S}} y_{t} \ln (\hat{y}_t)
\end{align*} 
Let us compute the partial derivative of $L_t$ with respect to $\Omega_t$ at step $t$:
\begin{align*}
    \frac{\partial L_t}{\partial \Omega} & =-\frac{\partial}{\partial \Omega} y_{t} \ln \hat{y}_t ~=- y_{t} \frac{\partial }{\partial \Omega_{t}} \log \hat{y}_t \\
& =-\sum y_{t} \cdot \frac{1}{\hat{y}_{t}}\frac{\partial \hat{y}_t}{\partial \Omega_{t}} \\
& =- \hat{y}_{t,i}\left(1-\hat{y}_{t,j}\right) \cdot \frac{y_{t}}{\hat{y}_{t}} \\
& =-\left(1-\hat{y}_{t,j}\right) y_{t} \\
& =-\left(y_{t}-\hat{y}_{t,j} \hat{y}_{t}\right) \\
& = \hat{y}_{t,i} \hat{y}_{t}- y_{t} \\
& =\hat{y}_{t,j} \cdot \hat{y}_{t}~(?)~- y_{t} \\
& = (\hat{y}_{t}-y_{t}) \\
\end{align*}
\begin{equation}
\label{eqn:partial_L_omega}
    \frac{\partial L_t}{\partial \hat{\Omega}_t} = (\hat{y}_{t}-y_{t})
\end{equation}
\subsection{Derivative of $V$}
The weight V is consistent across the entire time sequence, allowing us to perform differentiation at each time step and then aggregate the results. 

\begin{align*} 
\frac{\partial L}{\partial V} &= \sum_{t=1}^{S} \frac{\partial L_{t}}{\partial V} \\
&= \sum_{t=1}^{S} \frac{\partial L_{t}}{\partial \hat{y}_{t}} \frac{\partial \hat{y}_{t}}{\partial \Omega_{t}} \frac{\partial \Omega_{t}}{\partial V}\\
&= \sum_{t=1}^{S} \frac{\partial L}{\partial \Omega_t}  \frac{\partial \Omega_{t}}{\partial V}
\end{align*}

We know that this formula $\frac{\partial \hat{y}_{t}}{\partial \Omega_{t}}$ from \eqref{eqn:partial_omega} and no other function exists between Omega and V, so simply taking the derivative coefficient of V yields h, thus the answer is h.

\begin{equation}
\label{eqn:partial_aV}
=\sum_{t=1}^{S} (\hat{y}_{t} - y_{t}) \cdot h_{t}^\top
\end{equation}
\subsection{Derivative of $c$}
Similar to V, but its derivative is easier to calculate since it stands alone in the function.

\begin{align*}
\frac{\partial L}{\partial c} &= \sum_{t=1}^{T} \frac{\partial L_t}{\partial c} \\
&= \sum_{t=1}^{T} \frac{\partial L_{t}}{\partial \hat{y}_{t}} \frac{\partial \hat{y}_{t}}{\partial \Omega_{t}} \frac{\partial \Omega_{t}}{\partial c}\\
 &= \sum_{t=1}^{T} \frac{\partial L}{\partial \Omega_t} \frac{\partial \Omega_{t}}{\partial c}
\end{align*}
In this case, The Analytical Derivatives of c becomes:
\begin{equation}
\label{eqn:partial_ac}
   =\sum_{t=1}^{T} (\hat{y}_{t} - y_{t})
\end{equation}
\subsection{Derivative of $W$}
This function employs recursion, therefore, computing its derivative may take some time.
\begin{align*}
	h_t &= tanh(W ~h_{t-1}~+~U~x_t ~+~b)\\
	h_1 & = tanh(W ~h_{0}~+~U~x_1 ~+~b) ~~~ h_2 = tanh(W ~h_{1}~+~U~x_2 ~+~b)\\
	h_3 &= tanh(W ~h_{2}~+~U~x_3 ~+~b)~~~h_4 = tanh(W ~h_{3}~+~U~x_4 ~+~b)\\
\end{align*}
By placing previous hidden layer terms into $h_4$, we get:
\begin{align*}
	h_4 &= tanh(W ~tanh(W ~tanh(W ~tanh(W h_{0}+Ux_1 +b)+Ux_2 +b)+Ux_3 +b) +Ux_4 +b)
\end{align*}
We start from the first step go to the last step:
%\begin{align*}
%\frac{\partial L_1}{ \partial{W}} & = ~\frac{\partial L_1}{ \partial \hat{y}_1}~\frac{\partial \hat{y}_1}{ \partial h_1}
% ~\frac{\partial h_1}{ \partial h_1} ~\frac{\partial h_1}{ \partial W} \\
%	\frac{\partial L_2}{ \partial{W}} & = ~\frac{\partial L_2}{ \partial \hat{y}_2}~\frac{\partial \hat{y}_2}{ \partial h_2}
% ~\frac{\partial h_2}{ \partial h_1} ~\frac{\partial h_1}{ \partial W}	+ ~\frac{\partial L_2}{ \partial \hat{y}_2}~\frac{\partial \hat{y}_2}{ \partial h_2}
% ~\frac{\partial h_2}{ \partial h_2} ~\frac{\partial h_2}{ \partial W} \\
% \frac{\partial L_3}{ \partial{W}} & = ~\frac{\partial L_3}{ \partial \hat{y}_3}~\frac{\partial \hat{y}_3}{ \partial h_3}
% ~\frac{\partial h_3}{ \partial h_1} ~\frac{\partial h_1}{ \partial W}	+ ~\frac{\partial L_3}{ \partial \hat{y}_3}~\frac{\partial \hat{y}_3}{ \partial h_3}
% ~\frac{\partial h_3}{ \partial h_2} ~\frac{\partial h_2}{ \partial W} +  ~\frac{\partial L_3}{ \partial \hat{y}_3}~\frac{\partial \hat{y}_3}{ \partial h_3}
% ~\frac{\partial h_3}{ \partial h_3} ~\frac{\partial h_3}{ \partial W} \\
% \frac{\partial L_4}{ \partial{W}} & = ~\frac{\partial L_4}{ \partial \hat{y}_4}~\frac{\partial \hat{y}_4}{ \partial h_4}
% ~\frac{\partial h_4}{ \partial h_1} ~\frac{\partial h_1}{ \partial W}	+ ~\frac{\partial L_4}{ \partial \hat{y}_4}~\frac{\partial \hat{y}_4}{ \partial h_4}
% ~\frac{\partial h_4}{ \partial h_2} ~\frac{\partial h_2}{ \partial W} +  ~\frac{\partial L_4}{ \partial \hat{y}_4}~\frac{\partial \hat{y}_4}{ \partial h_4}
% ~\frac{\partial h_4}{ \partial h_3} ~\frac{\partial h_3}{ \partial W} + ~\frac{\partial L_4}{ \partial \hat{y}_4}~\frac{\partial \hat{y}_4}{ \partial h_4}
% ~\frac{\partial h_4}{ \partial h_4} ~\frac{\partial h_4}{ \partial W}
%\end{align*}
%Simplifying $\frac{\partial h_i}{ \partial h_i} =1$ in the above equation set we get:
%\begin{align*}
%\frac{\partial L_1}{ \partial{W}} & = ~\frac{\partial L_1}{ \partial \hat{y}_1}~\frac{\partial \hat{y}_1}{ \partial h_1}
%  ~\frac{\partial h_1}{ \partial W} \\
%	\frac{\partial L_2}{ \partial{W}} & = ~\frac{\partial L_2}{ \partial \hat{y}_2}~\frac{\partial \hat{y}_2}{ \partial h_2}
% ~\frac{\partial h_2}{ \partial h_1} ~\frac{\partial h_1}{ \partial W}	+ ~\frac{\partial L_2}{ \partial \hat{y}_2}~\frac{\partial \hat{y}_2}{ \partial h_2}
%  ~\frac{\partial h_2}{ \partial W} \\
% \frac{\partial L_3}{ \partial{W}} & = ~\frac{\partial L_3}{ \partial \hat{y}_3}~\frac{\partial \hat{y}_3}{ \partial h_3}
% ~\frac{\partial h_3}{ \partial h_1} ~\frac{\partial h_1}{ \partial W}	+ ~\frac{\partial L_3}{ \partial \hat{y}_3}~\frac{\partial \hat{y}_3}{ \partial h_3}
% ~\frac{\partial h_3}{ \partial h_2} ~\frac{\partial h_2}{ \partial W} +  ~\frac{\partial L_3}{ \partial \hat{y}_3}~\frac{\partial \hat{y}_3}{ \partial h_3}
%  ~\frac{\partial h_3}{ \partial W} \\
% \frac{\partial L_4}{ \partial{W}} & = ~\frac{\partial L_4}{ \partial \hat{y}_4}~\frac{\partial \hat{y}_4}{ \partial h_4}
% ~\frac{\partial h_4}{ \partial h_1} ~\frac{\partial h_1}{ \partial W}	+ ~\frac{\partial L_4}{ \partial \hat{y}_4}~\frac{\partial \hat{y}_4}{ \partial h_4}
% ~\frac{\partial h_4}{ \partial h_2} ~\frac{\partial h_2}{ \partial W} +  ~\frac{\partial L_4}{ \partial \hat{y}_4}~\frac{\partial \hat{y}_4}{ \partial h_4}
% ~\frac{\partial h_4}{ \partial h_3} ~\frac{\partial h_3}{ \partial W} + ~\frac{\partial L_4}{ \partial \hat{y}_4}~\frac{\partial \hat{y}_4}{ \partial h_4}
%  ~\frac{\partial h_4}{ \partial W}
%\end{align*}
%Let us expand $\frac{h_t}{h_k}$ as 
\begin{align*}
\frac{\partial L_1}{ \partial{W}} & = ~\frac{\partial L_1}{ \partial \hat{y}_1}~\frac{\partial \hat{y}_1}{ \partial h_1}
  ~\frac{\partial h_1}{ \partial W} \\
	\frac{\partial L_2}{ \partial{W}} & = ~\frac{\partial L_2}{ \partial \hat{y}_2}~\frac{\partial \hat{y}_2}{ \partial h_2}
 ~\frac{\partial h_2}{ \partial h_1} ~\frac{\partial h_1}{ \partial W}	+ ~\frac{\partial L_2}{ \partial \hat{y}_2}~\frac{\partial \hat{y}_2}{ \partial h_2}
  ~\frac{\partial h_2}{ \partial W} \\
 \frac{\partial L_3}{ \partial{W}} & = ~\frac{\partial L_3}{ \partial \hat{y}_3}~\frac{\partial \hat{y}_3}{ \partial h_3}
 ~\frac{\partial h_3}{ \partial h_2} ~\frac{\partial h_2}{ \partial h_1} ~\frac{\partial h_1}{ \partial W}	+ ~\frac{\partial L_3}{ \partial \hat{y}_3}~\frac{\partial \hat{y}_3}{ \partial h_3}
 ~\frac{\partial h_3}{ \partial h_2} ~\frac{\partial h_2}{ \partial W} +  ~\frac{\partial L_3}{ \partial \hat{y}_3}~\frac{\partial \hat{y}_3}{ \partial h_3}
  ~\frac{\partial h_3}{ \partial W} \\
 \frac{\partial L_4}{ \partial{W}} & = ~\frac{\partial L_4}{ \partial \hat{y}_4}~\frac{\partial \hat{y}_4}{ \partial h_4}
 ~\frac{\partial h_4}{ \partial h_3}~\frac{\partial h_3}{ \partial h_2}~\frac{\partial h_2}{ \partial h_1} ~\frac{\partial h_1}{ \partial W}	+ ~\frac{\partial L_4}{ \partial \hat{y}_4}~\frac{\partial \hat{y}_4}{ \partial h_4}
 ~\frac{\partial h_4}{ \partial h_3} ~\frac{\partial h_3}{ \partial h_2} ~\frac{\partial h_2}{ \partial W} +  ~\frac{\partial L_4}{ \partial \hat{y}_4}~\frac{\partial \hat{y}_4}{ \partial h_4}
 ~\frac{\partial h_4}{ \partial h_3} ~\frac{\partial h_3}{ \partial W} + ~\frac{\partial L_4}{ \partial \hat{y}_4}~\frac{\partial \hat{y}_4}{ \partial h_4}
  ~\frac{\partial h_4}{ \partial W}
\end{align*}
Let us group them under a $\sum \prod$  for step $t$:
\begin{align*}
	\frac{\partial L_t}{ \partial{W}} & = \sum_{k=1}^{t} ~
	~\frac{\partial L_t}{\partial \hat{y}_t}
	~\frac{\partial \hat{y}_t}{\partial h_t}
    \bigg( \prod_{j=k}^{t-1} 
	~\frac{\partial h_{j+1}}{\partial h_{j}}
    \bigg)
	~\frac{\partial h_k}{\partial W}
\end{align*}
Let us now present the formula for $\mathcal{S}$ steps:
\begin{equation}
	\frac{\partial L}{ \partial{W}} = \sum_{t=1}^{\mathcal{S}} 
	\bigg(	
	 \sum_{k=1}^{t} ~
	~\frac{\partial L_t}{\partial \hat{y}_t}
	~\frac{\partial \hat{y}_t}{\partial h_t}
        \prod_{j=k}^{t-1} \bigg(
	~\frac{\partial h_{j+1}}{\partial h_{j}}
        \bigg)
	~\frac{\partial h_k}{\partial W}~
	\bigg) 
 \label{eqn:partial_L_W}
\end{equation}
Finally, we insert the individual partial derivatives to calculate our final gradients of L with respect to W, where:

\begin{align*}
        \frac{\partial L_t}{\partial \hat{y}_t} &= (y_t - \hat{y}_t)\\
        \frac{\partial \hat{y}_t}{\partial h_t}
        &= V^\top \\
        ~\frac{\partial h_{j+1}}{\partial h_{j}}
        &= W^\top ~ (1 - h_{j+1}^2) \\
        ~\frac{\partial h_k}{\partial W}&= (1-h_k^2)~ h_{k-1}
\end{align*}
In this case, The Analytical Derivatives of Eq.~\eqref{eqn:partial_L_W} becomes:
\begin{equation}
	\frac{\partial L}{ \partial{W}} = \sum_{t=1}^{\mathcal{S}} 
	\bigg(	
	 \sum_{k=1}^{t} ~
	~(y_t - \hat{y}_t)
	~V^\top
        \prod_{j=k}^{t-1} \bigg(
	~W^\top ~ (1 - h_{j+1}^2)
        \bigg)
	~(1-h_k^2)~ h_{k-1}~
	\bigg) 
 \label{eqn:partial_L_aW}
\end{equation}
\subsection{Derivative of $U$}
Now, let us compute the partial derivation of $L$ with respect to $U$. Similar to the case of $W$ in Eq.~\eqref{eqn:partial_L_W}, for $U$ we have:

\begin{equation}
	\frac{\partial L}{ \partial{U}} = \sum_{t=1}^{\mathcal{S}} 
	\bigg(	
	 \sum_{k=1}^{t} ~
	~\frac{\partial L_t}{\partial \hat{y}_t}
	~\frac{\partial \hat{y}_t}{\partial h_t}
        \prod_{j=k}^{t-1} \bigg(
	~\frac{\partial h_{j+1}}{\partial h_{j}}
        \bigg)
	~\frac{\partial h_k}{\partial U}~
	\bigg) 
 \label{eqn:partial_L_U}
\end{equation}
We insert the individual partial derivatives into Eq.~\eqref{eqn:partial_L_U} as follows:
\begin{align*}
        \text{From Eq.~\eqref{eqn:partial_L_omega}:}~~~~~&\frac{\partial L_t}{\partial \hat{y}_t} = (y_t - \hat{y}_t)  \\
        \text{From Eq.~\eqref{eqn:omega}:} ~~~~~&\frac{\partial \hat{y}_t}{\partial h_t} = V^\top \\
        \text{From Eq.~\eqref{eqn:h}:}~~~~~&\frac{\partial h_{j+1}}{\partial h_{j}} 
        = W^\top ~ (1 - h_{j+1}^2)\\
        \text{From Eq.~\eqref{eqn:h}:}~~~~~&\frac{\partial h_k}{\partial U}= (1-h_k^2)~ x_{k} 
\end{align*}
Inserting the above derivatives into Eq.~\eqref{eqn:partial_L_U}, we have:
\begin{equation}
	\frac{\partial L}{ \partial{W}} = \sum_{t=1}^{\mathcal{S}} 
	\bigg(	
	 \sum_{k=1}^{t} ~
	~(y_t - \hat{y}_t)
	~V^\top
        \prod_{j=k}^{t-1} \bigg(
	~W^\top ~ (1 - h_{j+1}^2)
        \bigg)
	~(1-h_k^2)~ h_{k-1}~
	\bigg) 
 \label{eqn:partial_L_aU}
\end{equation}

\subsection{Derivative of $b$}
In the same manner, gradient of $L$ with respect to $b$ is calculated similar to Eq.~\eqref{eqn:partial_L_U} as follows:
\begin{equation}
	\frac{\partial L}{ \partial{b}} = \sum_{t=1}^{\mathcal{S}} 
	\bigg(	
	 \sum_{k=1}^{t} ~
	~\frac{\partial L_t}{\partial \hat{y}_t}
	~\frac{\partial \hat{y}_t}{\partial h_t}
        \prod_{j=k}^{t-1} \bigg(
	~\frac{\partial h_{j+1}}{\partial h_{j}}
        \bigg)
	~\frac{\partial h_k}{\partial b}~
	\bigg) 
 \label{eqn:partial_L_b}
\end{equation}
Recall that the derivatives used in Eq.~\eqref{eqn:partial_L_b} are:
\begin{align*}
        \frac{\partial L_t}{\partial \hat{y}_t} &= (y_t - \hat{y}_t)\\
        \frac{\partial \hat{y}_t}{\partial h_t}
        &= V^\top \\
        ~\frac{\partial h_{j+1}}{\partial h_{j}}
        &= W^\top ~ (1 - h_{j+1}^2) \\
        ~\frac{\partial h_k}{\partial b}&= (1-h_k^2)
\end{align*}
In this case, Eq.~\eqref{eqn:partial_L_b} becomes:
\begin{equation}
	\frac{\partial L}{ \partial{W}} = \sum_{t=1}^{\mathcal{S}} 
	\bigg(	
	 \sum_{k=1}^{t} ~
	~(y_t - \hat{y}_t)
	~V^\top
        \prod_{j=k}^{t-1} \bigg(
	~W^\top ~ (1 - h_{j+1}^2)
        \bigg)
	~(1-h_k^2)~
	\bigg) 
 \label{eqn:partial_L_ab}
\end{equation}
\end{document}
