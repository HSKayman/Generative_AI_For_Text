{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847e13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Libraries And Required Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6df44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cf27668b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import string, os\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 5\n",
    "learning_rate = 3e-4\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "905c4744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdde7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read File& Required Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f7db125",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('theatre.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3de93bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for punc in string.punctuation:\n",
    "    text = text.replace(punc,' ')\n",
    "txt = \" \".join(t for t in text.replace('\\n',' ').split(' ')).lower()  # Remove punctuation and convert to lower case\n",
    "txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41ab3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [element for element in txt.split(' ') if not len(element) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbcfb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "words = sorted(list(set(txt)))\n",
    "vocab_size = len(words)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(words) }\n",
    "itos = { i:ch for i,ch in enumerate(words) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "#encode = lambda s: [stoi[c] for c in s.split(\" \")] # encoder: take a string, output a list of integers\n",
    "#decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bfaffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ec0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform tokenization\n",
    "data = torch.tensor(encode(txt), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8a9fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f666c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3c772c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6752fac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.548608 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da46ee85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 9.4020, val loss 9.4044\n",
      "Current Time = 16:28:26\n",
      "step 5: train loss 8.4342, val loss 8.4758\n",
      "Current Time = 16:30:54\n",
      "step 10: train loss 7.8603, val loss 7.9452\n",
      "Current Time = 16:33:16\n",
      "step 15: train loss 7.3761, val loss 7.4943\n",
      "Current Time = 16:35:40\n",
      "step 20: train loss 7.0285, val loss 7.1863\n",
      "Current Time = 16:38:00\n",
      "step 25: train loss 6.8148, val loss 7.0016\n",
      "Current Time = 16:40:21\n",
      "step 30: train loss 6.7159, val loss 6.9178\n",
      "Current Time = 16:42:42\n",
      "step 35: train loss 6.6811, val loss 6.9009\n",
      "Current Time = 16:45:04\n",
      "step 40: train loss 6.6768, val loss 6.9159\n",
      "Current Time = 16:47:25\n",
      "step 45: train loss 6.6744, val loss 6.9311\n",
      "Current Time = 16:49:45\n",
      "step 50: train loss 6.6659, val loss 6.9329\n",
      "Current Time = 16:52:07\n",
      "step 55: train loss 6.6533, val loss 6.9402\n",
      "Current Time = 16:54:28\n",
      "step 60: train loss 6.6380, val loss 6.9279\n",
      "Current Time = 16:56:50\n",
      "step 65: train loss 6.5943, val loss 6.9009\n",
      "Current Time = 16:59:11\n",
      "step 70: train loss 6.5355, val loss 6.8436\n",
      "Current Time = 17:01:34\n",
      "step 75: train loss 6.4634, val loss 6.7800\n",
      "Current Time = 17:03:56\n",
      "step 80: train loss 6.3982, val loss 6.7261\n",
      "Current Time = 17:06:19\n",
      "step 85: train loss 6.3274, val loss 6.6746\n",
      "Current Time = 17:08:44\n",
      "step 90: train loss 6.2558, val loss 6.6270\n",
      "Current Time = 17:11:08\n",
      "step 95: train loss 6.1801, val loss 6.5841\n",
      "Current Time = 17:13:32\n",
      "step 100: train loss 6.1108, val loss 6.5520\n",
      "Current Time = 17:15:56\n",
      "step 105: train loss 6.0420, val loss 6.5180\n",
      "Current Time = 17:18:22\n",
      "step 110: train loss 5.9849, val loss 6.5024\n",
      "Current Time = 17:20:49\n",
      "step 115: train loss 5.9321, val loss 6.4761\n",
      "Current Time = 17:23:14\n",
      "step 120: train loss 5.8837, val loss 6.4590\n",
      "Current Time = 17:25:39\n",
      "step 125: train loss 5.8365, val loss 6.4394\n",
      "Current Time = 17:28:07\n",
      "step 130: train loss 5.7954, val loss 6.4439\n",
      "Current Time = 17:30:35\n",
      "step 135: train loss 5.7514, val loss 6.4376\n",
      "Current Time = 17:33:02\n",
      "step 140: train loss 5.7020, val loss 6.4294\n",
      "Current Time = 17:35:30\n",
      "step 145: train loss 5.6660, val loss 6.4306\n",
      "Current Time = 17:37:57\n",
      "step 150: train loss 5.6256, val loss 6.4080\n",
      "Current Time = 17:40:22\n",
      "step 155: train loss 5.5908, val loss 6.4124\n",
      "Current Time = 17:42:51\n",
      "step 160: train loss 5.5525, val loss 6.4066\n",
      "Current Time = 17:45:15\n",
      "step 165: train loss 5.5142, val loss 6.3949\n",
      "Current Time = 17:47:43\n",
      "step 170: train loss 5.4876, val loss 6.3911\n",
      "Current Time = 17:50:08\n",
      "step 175: train loss 5.4518, val loss 6.3954\n",
      "Current Time = 17:52:35\n",
      "step 180: train loss 5.4244, val loss 6.3725\n",
      "Current Time = 17:54:59\n",
      "step 185: train loss 5.3888, val loss 6.3770\n",
      "Current Time = 17:57:24\n",
      "step 190: train loss 5.3617, val loss 6.3924\n",
      "Current Time = 17:59:49\n",
      "step 195: train loss 5.3303, val loss 6.3767\n",
      "Current Time = 18:02:13\n",
      "step 200: train loss 5.3005, val loss 6.3894\n",
      "Current Time = 18:04:37\n",
      "step 205: train loss 5.2680, val loss 6.3801\n",
      "Current Time = 18:07:04\n",
      "step 210: train loss 5.2370, val loss 6.3816\n",
      "Current Time = 18:09:31\n",
      "step 215: train loss 5.2152, val loss 6.3793\n",
      "Current Time = 18:11:57\n",
      "step 220: train loss 5.1868, val loss 6.3958\n",
      "Current Time = 18:14:24\n",
      "step 225: train loss 5.1602, val loss 6.3878\n",
      "Current Time = 18:16:52\n",
      "step 230: train loss 5.1333, val loss 6.4075\n",
      "Current Time = 18:19:19\n",
      "step 235: train loss 5.1125, val loss 6.4119\n",
      "Current Time = 18:21:49\n",
      "step 240: train loss 5.0843, val loss 6.4039\n",
      "Current Time = 18:24:15\n",
      "step 245: train loss 5.0504, val loss 6.3948\n",
      "Current Time = 18:26:43\n",
      "step 250: train loss 5.0263, val loss 6.4246\n",
      "Current Time = 18:29:11\n",
      "step 255: train loss 5.0009, val loss 6.4102\n",
      "Current Time = 18:31:38\n",
      "step 260: train loss 4.9737, val loss 6.4280\n",
      "Current Time = 18:34:05\n",
      "step 265: train loss 4.9495, val loss 6.4137\n",
      "Current Time = 18:36:34\n",
      "step 270: train loss 4.9257, val loss 6.4362\n",
      "Current Time = 18:39:09\n",
      "step 275: train loss 4.9088, val loss 6.4358\n",
      "Current Time = 18:41:39\n",
      "step 280: train loss 4.8791, val loss 6.4412\n",
      "Current Time = 18:44:11\n",
      "step 285: train loss 4.8522, val loss 6.4328\n",
      "Current Time = 18:46:42\n",
      "step 290: train loss 4.8238, val loss 6.4430\n",
      "Current Time = 18:49:16\n",
      "step 295: train loss 4.8046, val loss 6.4586\n",
      "Current Time = 18:51:51\n",
      "step 300: train loss 4.7793, val loss 6.4623\n",
      "Current Time = 18:54:25\n",
      "step 305: train loss 4.7577, val loss 6.4678\n",
      "Current Time = 18:56:56\n",
      "step 310: train loss 4.7312, val loss 6.4733\n",
      "Current Time = 18:59:27\n",
      "step 315: train loss 4.7021, val loss 6.4827\n",
      "Current Time = 19:01:58\n",
      "step 320: train loss 4.6838, val loss 6.4945\n",
      "Current Time = 19:04:29\n",
      "step 325: train loss 4.6559, val loss 6.4899\n",
      "Current Time = 19:07:01\n",
      "step 330: train loss 4.6294, val loss 6.5172\n",
      "Current Time = 19:09:35\n",
      "step 335: train loss 4.6000, val loss 6.5124\n",
      "Current Time = 19:12:10\n",
      "step 340: train loss 4.5791, val loss 6.5344\n",
      "Current Time = 19:14:44\n",
      "step 345: train loss 4.5485, val loss 6.5314\n",
      "Current Time = 19:17:17\n",
      "step 350: train loss 4.5193, val loss 6.5519\n",
      "Current Time = 19:19:49\n",
      "step 355: train loss 4.4981, val loss 6.5498\n",
      "Current Time = 19:22:22\n",
      "step 360: train loss 4.4682, val loss 6.5627\n",
      "Current Time = 19:24:53\n",
      "step 365: train loss 4.4377, val loss 6.5748\n",
      "Current Time = 19:27:27\n",
      "step 370: train loss 4.4072, val loss 6.5924\n",
      "Current Time = 19:29:59\n",
      "step 375: train loss 4.3816, val loss 6.6057\n",
      "Current Time = 19:32:30\n",
      "step 380: train loss 4.3489, val loss 6.6385\n",
      "Current Time = 19:35:01\n",
      "step 385: train loss 4.3138, val loss 6.6489\n",
      "Current Time = 19:37:31\n",
      "step 390: train loss 4.2858, val loss 6.6466\n",
      "Current Time = 19:40:02\n",
      "step 395: train loss 4.2585, val loss 6.6411\n",
      "Current Time = 19:42:31\n",
      "step 400: train loss 4.2151, val loss 6.6649\n",
      "Current Time = 19:44:59\n",
      "step 405: train loss 4.1794, val loss 6.6855\n",
      "Current Time = 19:47:29\n",
      "step 410: train loss 4.1454, val loss 6.6966\n",
      "Current Time = 19:49:55\n",
      "step 415: train loss 4.1104, val loss 6.7258\n",
      "Current Time = 19:52:20\n",
      "step 420: train loss 4.0763, val loss 6.7110\n",
      "Current Time = 19:54:46\n",
      "step 425: train loss 4.0276, val loss 6.7543\n",
      "Current Time = 19:57:15\n",
      "step 430: train loss 4.0000, val loss 6.7773\n",
      "Current Time = 19:59:43\n",
      "step 435: train loss 3.9601, val loss 6.8075\n",
      "Current Time = 20:02:13\n",
      "step 440: train loss 3.9241, val loss 6.8097\n",
      "Current Time = 20:04:44\n",
      "step 445: train loss 3.8878, val loss 6.7996\n",
      "Current Time = 20:07:18\n",
      "step 450: train loss 3.8438, val loss 6.8435\n",
      "Current Time = 20:09:52\n",
      "step 455: train loss 3.8112, val loss 6.8547\n",
      "Current Time = 20:12:26\n",
      "step 460: train loss 3.7748, val loss 6.8943\n",
      "Current Time = 20:15:01\n",
      "step 465: train loss 3.7431, val loss 6.9307\n",
      "Current Time = 20:17:30\n",
      "step 470: train loss 3.7051, val loss 6.9202\n",
      "Current Time = 20:19:59\n",
      "step 475: train loss 3.6649, val loss 6.9313\n",
      "Current Time = 20:22:25\n",
      "step 480: train loss 3.6228, val loss 6.9490\n",
      "Current Time = 20:24:51\n",
      "step 485: train loss 3.5885, val loss 6.9583\n",
      "Current Time = 20:27:16\n",
      "step 490: train loss 3.5498, val loss 6.9928\n",
      "Current Time = 20:29:42\n",
      "step 495: train loss 3.5104, val loss 7.0306\n",
      "Current Time = 20:32:06\n",
      "step 500: train loss 3.4694, val loss 7.0357\n",
      "Current Time = 20:34:31\n",
      "step 505: train loss 3.4327, val loss 7.0563\n",
      "Current Time = 20:36:54\n",
      "step 510: train loss 3.3838, val loss 7.0701\n",
      "Current Time = 20:39:16\n",
      "step 515: train loss 3.3453, val loss 7.0963\n",
      "Current Time = 20:41:37\n",
      "step 520: train loss 3.3004, val loss 7.1232\n",
      "Current Time = 20:44:00\n",
      "step 525: train loss 3.2619, val loss 7.1897\n",
      "Current Time = 20:46:27\n",
      "step 530: train loss 3.2169, val loss 7.1736\n",
      "Current Time = 20:48:56\n",
      "step 535: train loss 3.1768, val loss 7.2386\n",
      "Current Time = 20:51:25\n",
      "step 540: train loss 3.1435, val loss 7.2368\n",
      "Current Time = 20:53:54\n",
      "step 545: train loss 3.1065, val loss 7.2820\n",
      "Current Time = 20:56:24\n",
      "step 550: train loss 3.0571, val loss 7.3051\n",
      "Current Time = 20:58:53\n",
      "step 555: train loss 3.0228, val loss 7.3409\n",
      "Current Time = 21:01:19\n",
      "step 560: train loss 2.9804, val loss 7.3268\n",
      "Current Time = 21:03:46\n",
      "step 565: train loss 2.9532, val loss 7.3923\n",
      "Current Time = 21:06:11\n",
      "step 570: train loss 2.9068, val loss 7.3804\n",
      "Current Time = 21:08:38\n",
      "step 575: train loss 2.8708, val loss 7.4149\n",
      "Current Time = 21:11:07\n",
      "step 580: train loss 2.8277, val loss 7.4233\n",
      "Current Time = 21:13:37\n",
      "step 585: train loss 2.7840, val loss 7.4582\n",
      "Current Time = 21:16:06\n",
      "step 590: train loss 2.7484, val loss 7.4501\n",
      "Current Time = 21:18:37\n",
      "step 595: train loss 2.7039, val loss 7.5013\n",
      "Current Time = 21:21:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600: train loss 2.6657, val loss 7.5522\n",
      "Current Time = 21:23:41\n",
      "step 605: train loss 2.6338, val loss 7.5889\n",
      "Current Time = 21:26:14\n",
      "step 610: train loss 2.5872, val loss 7.6188\n",
      "Current Time = 21:28:46\n",
      "step 615: train loss 2.5465, val loss 7.6258\n",
      "Current Time = 21:31:17\n",
      "step 620: train loss 2.5082, val loss 7.6632\n",
      "Current Time = 21:33:47\n",
      "step 625: train loss 2.4702, val loss 7.6901\n",
      "Current Time = 21:36:19\n",
      "step 630: train loss 2.4323, val loss 7.7024\n",
      "Current Time = 21:38:50\n",
      "step 635: train loss 2.4067, val loss 7.7265\n",
      "Current Time = 21:41:19\n",
      "step 640: train loss 2.3677, val loss 7.7477\n",
      "Current Time = 21:43:49\n",
      "step 645: train loss 2.3242, val loss 7.7972\n",
      "Current Time = 21:46:21\n",
      "step 650: train loss 2.3028, val loss 7.7992\n",
      "Current Time = 21:48:50\n",
      "step 655: train loss 2.2557, val loss 7.8223\n",
      "Current Time = 21:51:19\n",
      "step 660: train loss 2.2164, val loss 7.8744\n",
      "Current Time = 21:53:48\n",
      "step 665: train loss 2.1879, val loss 7.8906\n",
      "Current Time = 21:56:16\n",
      "step 670: train loss 2.1466, val loss 7.9313\n",
      "Current Time = 21:58:46\n",
      "step 675: train loss 2.1156, val loss 7.9281\n",
      "Current Time = 22:01:13\n",
      "step 680: train loss 2.0753, val loss 7.9856\n",
      "Current Time = 22:03:40\n",
      "step 685: train loss 2.0426, val loss 8.0380\n",
      "Current Time = 22:06:07\n",
      "step 690: train loss 2.0090, val loss 8.0806\n",
      "Current Time = 22:08:35\n",
      "step 695: train loss 1.9775, val loss 8.0937\n",
      "Current Time = 22:11:02\n",
      "step 700: train loss 1.9338, val loss 8.1228\n",
      "Current Time = 22:13:28\n",
      "step 705: train loss 1.9015, val loss 8.1167\n",
      "Current Time = 22:15:56\n",
      "step 710: train loss 1.8711, val loss 8.1285\n",
      "Current Time = 22:18:25\n",
      "step 715: train loss 1.8532, val loss 8.1612\n",
      "Current Time = 22:20:52\n",
      "step 720: train loss 1.8018, val loss 8.1812\n",
      "Current Time = 22:23:18\n",
      "step 725: train loss 1.7798, val loss 8.2147\n",
      "Current Time = 22:25:45\n",
      "step 730: train loss 1.7514, val loss 8.2007\n",
      "Current Time = 22:28:12\n",
      "step 735: train loss 1.7270, val loss 8.2601\n",
      "Current Time = 22:30:37\n",
      "step 740: train loss 1.6937, val loss 8.2962\n",
      "Current Time = 22:33:02\n",
      "step 745: train loss 1.6608, val loss 8.3270\n",
      "Current Time = 22:35:30\n",
      "step 750: train loss 1.6380, val loss 8.3228\n",
      "Current Time = 22:37:58\n",
      "step 755: train loss 1.6114, val loss 8.3294\n",
      "Current Time = 22:40:23\n",
      "step 760: train loss 1.5802, val loss 8.3978\n",
      "Current Time = 22:42:49\n",
      "step 765: train loss 1.5540, val loss 8.4226\n",
      "Current Time = 22:45:13\n",
      "step 770: train loss 1.5293, val loss 8.4124\n",
      "Current Time = 22:47:39\n",
      "step 775: train loss 1.5025, val loss 8.4493\n",
      "Current Time = 22:50:04\n",
      "step 780: train loss 1.4759, val loss 8.5037\n",
      "Current Time = 22:52:29\n",
      "step 785: train loss 1.4543, val loss 8.4753\n",
      "Current Time = 22:54:57\n",
      "step 790: train loss 1.4325, val loss 8.5223\n",
      "Current Time = 22:57:22\n",
      "step 795: train loss 1.3918, val loss 8.5489\n",
      "Current Time = 22:59:48\n",
      "step 800: train loss 1.3690, val loss 8.5598\n",
      "Current Time = 23:02:15\n",
      "step 805: train loss 1.3420, val loss 8.5939\n",
      "Current Time = 23:04:41\n",
      "step 810: train loss 1.3154, val loss 8.6012\n",
      "Current Time = 23:07:07\n",
      "step 815: train loss 1.3013, val loss 8.6753\n",
      "Current Time = 23:09:35\n",
      "step 820: train loss 1.2737, val loss 8.6919\n",
      "Current Time = 23:12:02\n",
      "step 825: train loss 1.2536, val loss 8.7176\n",
      "Current Time = 23:14:29\n",
      "step 830: train loss 1.2358, val loss 8.7041\n",
      "Current Time = 23:16:57\n",
      "step 835: train loss 1.2033, val loss 8.7547\n",
      "Current Time = 23:19:27\n",
      "step 840: train loss 1.1766, val loss 8.7172\n",
      "Current Time = 23:22:01\n",
      "step 845: train loss 1.1623, val loss 8.7762\n",
      "Current Time = 23:24:34\n",
      "step 850: train loss 1.1458, val loss 8.7737\n",
      "Current Time = 23:27:08\n",
      "step 855: train loss 1.1208, val loss 8.8410\n",
      "Current Time = 23:29:44\n",
      "step 860: train loss 1.0988, val loss 8.8609\n",
      "Current Time = 23:32:16\n",
      "step 865: train loss 1.0769, val loss 8.8730\n",
      "Current Time = 23:34:50\n",
      "step 870: train loss 1.0629, val loss 8.9131\n",
      "Current Time = 23:37:23\n",
      "step 875: train loss 1.0391, val loss 8.8874\n",
      "Current Time = 23:39:58\n",
      "step 880: train loss 1.0235, val loss 8.9125\n",
      "Current Time = 23:42:31\n",
      "step 885: train loss 1.0085, val loss 8.9370\n",
      "Current Time = 23:45:07\n",
      "step 890: train loss 0.9880, val loss 8.9583\n",
      "Current Time = 23:47:43\n",
      "step 895: train loss 0.9707, val loss 8.9601\n",
      "Current Time = 23:50:17\n",
      "step 900: train loss 0.9518, val loss 9.0117\n",
      "Current Time = 23:52:50\n",
      "step 905: train loss 0.9301, val loss 9.0545\n",
      "Current Time = 23:55:21\n",
      "step 910: train loss 0.9207, val loss 9.0629\n",
      "Current Time = 23:57:57\n",
      "step 915: train loss 0.9034, val loss 9.0722\n",
      "Current Time = 00:00:30\n",
      "step 920: train loss 0.8855, val loss 9.1249\n",
      "Current Time = 00:03:02\n",
      "step 925: train loss 0.8699, val loss 9.1071\n",
      "Current Time = 00:05:34\n",
      "step 930: train loss 0.8526, val loss 9.1937\n",
      "Current Time = 00:08:06\n",
      "step 935: train loss 0.8351, val loss 9.1615\n",
      "Current Time = 00:10:37\n",
      "step 940: train loss 0.8277, val loss 9.2008\n",
      "Current Time = 00:13:06\n",
      "step 945: train loss 0.8100, val loss 9.2248\n",
      "Current Time = 00:15:38\n",
      "step 950: train loss 0.7947, val loss 9.2343\n",
      "Current Time = 00:18:09\n",
      "step 955: train loss 0.7863, val loss 9.2617\n",
      "Current Time = 00:20:40\n",
      "step 960: train loss 0.7676, val loss 9.3011\n",
      "Current Time = 00:23:11\n",
      "step 965: train loss 0.7533, val loss 9.2640\n",
      "Current Time = 00:25:41\n",
      "step 970: train loss 0.7385, val loss 9.2902\n",
      "Current Time = 00:28:11\n",
      "step 975: train loss 0.7245, val loss 9.3309\n",
      "Current Time = 00:30:44\n",
      "step 980: train loss 0.7161, val loss 9.3330\n",
      "Current Time = 00:33:15\n",
      "step 985: train loss 0.7059, val loss 9.3674\n",
      "Current Time = 00:35:47\n",
      "step 990: train loss 0.6876, val loss 9.4404\n",
      "Current Time = 00:38:15\n",
      "step 995: train loss 0.6855, val loss 9.4023\n",
      "Current Time = 00:40:48\n",
      "step 1000: train loss 0.6675, val loss 9.4533\n",
      "Current Time = 00:43:18\n",
      "step 1005: train loss 0.6588, val loss 9.4804\n",
      "Current Time = 00:45:46\n",
      "step 1010: train loss 0.6527, val loss 9.4760\n",
      "Current Time = 00:48:16\n",
      "step 1015: train loss 0.6416, val loss 9.4586\n",
      "Current Time = 00:50:45\n",
      "step 1020: train loss 0.6274, val loss 9.4889\n",
      "Current Time = 00:53:16\n",
      "step 1025: train loss 0.6188, val loss 9.5419\n",
      "Current Time = 00:55:44\n",
      "step 1030: train loss 0.6076, val loss 9.5136\n",
      "Current Time = 00:58:14\n",
      "step 1035: train loss 0.6002, val loss 9.5142\n",
      "Current Time = 01:00:42\n",
      "step 1040: train loss 0.5871, val loss 9.5352\n",
      "Current Time = 01:03:12\n",
      "step 1045: train loss 0.5828, val loss 9.5791\n",
      "Current Time = 01:05:42\n",
      "step 1050: train loss 0.5710, val loss 9.5664\n",
      "Current Time = 01:08:13\n",
      "step 1055: train loss 0.5699, val loss 9.5479\n",
      "Current Time = 01:10:43\n",
      "step 1060: train loss 0.5544, val loss 9.6072\n",
      "Current Time = 01:13:14\n",
      "step 1065: train loss 0.5518, val loss 9.6286\n",
      "Current Time = 01:15:47\n",
      "step 1070: train loss 0.5372, val loss 9.6783\n",
      "Current Time = 01:18:16\n",
      "step 1075: train loss 0.5299, val loss 9.6344\n",
      "Current Time = 01:20:47\n",
      "step 1080: train loss 0.5218, val loss 9.6597\n",
      "Current Time = 01:23:19\n",
      "step 1085: train loss 0.5155, val loss 9.6939\n",
      "Current Time = 01:25:50\n",
      "step 1090: train loss 0.5067, val loss 9.7181\n",
      "Current Time = 01:28:25\n",
      "step 1095: train loss 0.5031, val loss 9.7674\n",
      "Current Time = 01:30:59\n",
      "step 1100: train loss 0.4972, val loss 9.7759\n",
      "Current Time = 01:33:35\n",
      "step 1105: train loss 0.4919, val loss 9.7332\n",
      "Current Time = 01:36:08\n",
      "step 1110: train loss 0.4841, val loss 9.7960\n",
      "Current Time = 01:38:41\n",
      "step 1115: train loss 0.4807, val loss 9.8073\n",
      "Current Time = 01:41:17\n",
      "step 1120: train loss 0.4719, val loss 9.8087\n",
      "Current Time = 01:43:51\n",
      "step 1125: train loss 0.4717, val loss 9.8103\n",
      "Current Time = 01:46:25\n",
      "step 1130: train loss 0.4642, val loss 9.8024\n",
      "Current Time = 01:48:58\n",
      "step 1135: train loss 0.4562, val loss 9.8519\n",
      "Current Time = 01:51:32\n",
      "step 1140: train loss 0.4509, val loss 9.9163\n",
      "Current Time = 01:54:05\n",
      "step 1145: train loss 0.4469, val loss 9.8856\n",
      "Current Time = 01:56:37\n",
      "step 1150: train loss 0.4398, val loss 9.8977\n",
      "Current Time = 01:59:08\n",
      "step 1155: train loss 0.4384, val loss 9.9212\n",
      "Current Time = 02:01:42\n",
      "step 1160: train loss 0.4332, val loss 9.9215\n",
      "Current Time = 02:04:14\n",
      "step 1165: train loss 0.4274, val loss 9.9176\n",
      "Current Time = 02:06:47\n",
      "step 1170: train loss 0.4239, val loss 9.9354\n",
      "Current Time = 02:09:18\n",
      "step 1175: train loss 0.4160, val loss 9.9812\n",
      "Current Time = 02:11:46\n",
      "step 1180: train loss 0.4122, val loss 10.0163\n",
      "Current Time = 02:14:16\n",
      "step 1185: train loss 0.4075, val loss 9.9573\n",
      "Current Time = 02:16:46\n",
      "step 1190: train loss 0.4016, val loss 9.9915\n",
      "Current Time = 02:19:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1195: train loss 0.4002, val loss 10.0925\n",
      "Current Time = 02:21:43\n",
      "step 1200: train loss 0.3968, val loss 10.0621\n",
      "Current Time = 02:24:12\n",
      "step 1205: train loss 0.3957, val loss 10.0822\n",
      "Current Time = 02:26:41\n",
      "step 1210: train loss 0.3878, val loss 10.1474\n",
      "Current Time = 02:29:11\n",
      "step 1215: train loss 0.3852, val loss 10.1154\n",
      "Current Time = 02:31:39\n",
      "step 1220: train loss 0.3807, val loss 10.0874\n",
      "Current Time = 02:34:07\n",
      "step 1225: train loss 0.3785, val loss 10.1874\n",
      "Current Time = 02:36:36\n",
      "step 1230: train loss 0.3777, val loss 10.2047\n",
      "Current Time = 02:39:04\n",
      "step 1235: train loss 0.3731, val loss 10.1072\n",
      "Current Time = 02:41:31\n",
      "step 1240: train loss 0.3666, val loss 10.1593\n",
      "Current Time = 02:43:58\n",
      "step 1245: train loss 0.3628, val loss 10.1703\n",
      "Current Time = 02:46:25\n",
      "step 1250: train loss 0.3588, val loss 10.2236\n",
      "Current Time = 02:48:52\n",
      "step 1255: train loss 0.3593, val loss 10.1485\n",
      "Current Time = 02:51:22\n",
      "step 1260: train loss 0.3547, val loss 10.2077\n",
      "Current Time = 02:53:49\n",
      "step 1265: train loss 0.3533, val loss 10.2096\n",
      "Current Time = 02:56:16\n",
      "step 1270: train loss 0.3492, val loss 10.2115\n",
      "Current Time = 02:58:44\n",
      "step 1275: train loss 0.3457, val loss 10.2699\n",
      "Current Time = 03:01:13\n",
      "step 1280: train loss 0.3434, val loss 10.2772\n",
      "Current Time = 03:03:44\n",
      "step 1285: train loss 0.3449, val loss 10.1755\n",
      "Current Time = 03:06:14\n",
      "step 1290: train loss 0.3402, val loss 10.2136\n",
      "Current Time = 03:08:42\n",
      "step 1295: train loss 0.3364, val loss 10.2727\n",
      "Current Time = 03:11:13\n",
      "step 1300: train loss 0.3361, val loss 10.2546\n",
      "Current Time = 03:13:42\n",
      "step 1305: train loss 0.3310, val loss 10.2719\n",
      "Current Time = 03:16:11\n",
      "step 1310: train loss 0.3263, val loss 10.2962\n",
      "Current Time = 03:18:41\n",
      "step 1315: train loss 0.3236, val loss 10.3334\n",
      "Current Time = 03:21:12\n",
      "step 1320: train loss 0.3223, val loss 10.3263\n",
      "Current Time = 03:23:43\n",
      "step 1325: train loss 0.3213, val loss 10.3131\n",
      "Current Time = 03:26:12\n",
      "step 1330: train loss 0.3161, val loss 10.3154\n",
      "Current Time = 03:28:41\n",
      "step 1335: train loss 0.3164, val loss 10.2790\n",
      "Current Time = 03:31:06\n",
      "step 1340: train loss 0.3144, val loss 10.3403\n",
      "Current Time = 03:33:35\n",
      "step 1345: train loss 0.3143, val loss 10.3897\n",
      "Current Time = 03:36:02\n",
      "step 1350: train loss 0.3127, val loss 10.3575\n",
      "Current Time = 03:38:29\n",
      "step 1355: train loss 0.3087, val loss 10.3769\n",
      "Current Time = 03:40:57\n",
      "step 1360: train loss 0.3084, val loss 10.3872\n",
      "Current Time = 03:43:24\n",
      "step 1365: train loss 0.3062, val loss 10.3702\n",
      "Current Time = 03:45:53\n",
      "step 1370: train loss 0.3029, val loss 10.4061\n",
      "Current Time = 03:48:19\n",
      "step 1375: train loss 0.2995, val loss 10.4620\n",
      "Current Time = 03:50:45\n",
      "step 1380: train loss 0.2994, val loss 10.4262\n",
      "Current Time = 03:53:12\n",
      "step 1385: train loss 0.2980, val loss 10.5061\n",
      "Current Time = 03:55:36\n",
      "step 1390: train loss 0.2940, val loss 10.4739\n",
      "Current Time = 03:58:02\n",
      "step 1395: train loss 0.2927, val loss 10.4746\n",
      "Current Time = 04:00:29\n",
      "step 1400: train loss 0.2910, val loss 10.4449\n",
      "Current Time = 04:02:54\n",
      "step 1405: train loss 0.2897, val loss 10.5236\n",
      "Current Time = 04:05:20\n",
      "step 1410: train loss 0.2878, val loss 10.5392\n",
      "Current Time = 04:07:46\n",
      "step 1415: train loss 0.2861, val loss 10.5537\n",
      "Current Time = 04:10:10\n",
      "step 1420: train loss 0.2870, val loss 10.4677\n",
      "Current Time = 04:12:35\n",
      "step 1425: train loss 0.2885, val loss 10.4803\n",
      "Current Time = 04:15:00\n",
      "step 1430: train loss 0.2842, val loss 10.5272\n",
      "Current Time = 04:17:27\n",
      "step 1435: train loss 0.2820, val loss 10.5272\n",
      "Current Time = 04:19:51\n",
      "step 1440: train loss 0.2829, val loss 10.4967\n",
      "Current Time = 04:22:14\n",
      "step 1445: train loss 0.2787, val loss 10.5799\n",
      "Current Time = 04:24:41\n",
      "step 1450: train loss 0.2775, val loss 10.5297\n",
      "Current Time = 04:27:06\n",
      "step 1455: train loss 0.2773, val loss 10.5810\n",
      "Current Time = 04:29:31\n",
      "step 1460: train loss 0.2748, val loss 10.6044\n",
      "Current Time = 04:31:55\n",
      "step 1465: train loss 0.2723, val loss 10.6177\n",
      "Current Time = 04:34:20\n",
      "step 1470: train loss 0.2725, val loss 10.5853\n",
      "Current Time = 04:36:44\n",
      "step 1475: train loss 0.2735, val loss 10.5624\n",
      "Current Time = 04:39:09\n",
      "step 1480: train loss 0.2692, val loss 10.6367\n",
      "Current Time = 04:41:35\n",
      "step 1485: train loss 0.2694, val loss 10.5988\n",
      "Current Time = 04:43:58\n",
      "step 1490: train loss 0.2690, val loss 10.6673\n",
      "Current Time = 04:46:23\n",
      "step 1495: train loss 0.2668, val loss 10.6860\n",
      "Current Time = 04:48:47\n",
      "step 1500: train loss 0.2655, val loss 10.6761\n",
      "Current Time = 04:51:12\n",
      "step 1505: train loss 0.2651, val loss 10.6874\n",
      "Current Time = 04:53:36\n",
      "step 1510: train loss 0.2625, val loss 10.6482\n",
      "Current Time = 04:55:59\n",
      "step 1515: train loss 0.2621, val loss 10.6643\n",
      "Current Time = 04:58:24\n",
      "step 1520: train loss 0.2601, val loss 10.7891\n",
      "Current Time = 05:00:47\n",
      "step 1525: train loss 0.2611, val loss 10.7455\n",
      "Current Time = 05:03:12\n",
      "step 1530: train loss 0.2593, val loss 10.7588\n",
      "Current Time = 05:05:36\n",
      "step 1535: train loss 0.2573, val loss 10.7000\n",
      "Current Time = 05:08:00\n",
      "step 1540: train loss 0.2571, val loss 10.7678\n",
      "Current Time = 05:10:24\n",
      "step 1545: train loss 0.2562, val loss 10.7500\n",
      "Current Time = 05:12:49\n",
      "step 1550: train loss 0.2550, val loss 10.7445\n",
      "Current Time = 05:15:13\n",
      "step 1555: train loss 0.2549, val loss 10.7431\n",
      "Current Time = 05:17:36\n",
      "step 1560: train loss 0.2530, val loss 10.7789\n",
      "Current Time = 05:20:01\n",
      "step 1565: train loss 0.2496, val loss 10.8290\n",
      "Current Time = 05:22:26\n",
      "step 1570: train loss 0.2502, val loss 10.7466\n",
      "Current Time = 05:24:51\n",
      "step 1575: train loss 0.2503, val loss 10.7362\n",
      "Current Time = 05:27:15\n",
      "step 1580: train loss 0.2477, val loss 10.8267\n",
      "Current Time = 05:29:38\n",
      "step 1585: train loss 0.2459, val loss 10.7673\n",
      "Current Time = 05:32:02\n",
      "step 1590: train loss 0.2459, val loss 10.7970\n",
      "Current Time = 05:34:27\n",
      "step 1595: train loss 0.2442, val loss 10.8393\n",
      "Current Time = 05:36:52\n",
      "step 1600: train loss 0.2446, val loss 10.7986\n",
      "Current Time = 05:39:17\n",
      "step 1605: train loss 0.2446, val loss 10.8788\n",
      "Current Time = 05:41:42\n",
      "step 1610: train loss 0.2439, val loss 10.8182\n",
      "Current Time = 05:44:07\n",
      "step 1615: train loss 0.2404, val loss 10.8312\n",
      "Current Time = 05:46:32\n",
      "step 1620: train loss 0.2406, val loss 10.8150\n",
      "Current Time = 05:48:55\n",
      "step 1625: train loss 0.2394, val loss 10.8670\n",
      "Current Time = 05:51:19\n",
      "step 1630: train loss 0.2394, val loss 10.8829\n",
      "Current Time = 05:53:43\n",
      "step 1635: train loss 0.2388, val loss 10.8591\n",
      "Current Time = 05:56:07\n",
      "step 1640: train loss 0.2386, val loss 10.9295\n",
      "Current Time = 05:58:31\n",
      "step 1645: train loss 0.2357, val loss 10.9001\n",
      "Current Time = 06:00:55\n",
      "step 1650: train loss 0.2367, val loss 10.8693\n",
      "Current Time = 06:03:19\n",
      "step 1655: train loss 0.2363, val loss 10.9192\n",
      "Current Time = 06:05:42\n",
      "step 1660: train loss 0.2347, val loss 10.9542\n",
      "Current Time = 06:08:06\n",
      "step 1665: train loss 0.2341, val loss 10.9534\n",
      "Current Time = 06:10:30\n",
      "step 1670: train loss 0.2339, val loss 10.9464\n",
      "Current Time = 06:12:53\n",
      "step 1675: train loss 0.2333, val loss 10.9616\n",
      "Current Time = 06:15:18\n",
      "step 1680: train loss 0.2328, val loss 10.9414\n",
      "Current Time = 06:17:42\n",
      "step 1685: train loss 0.2332, val loss 10.9629\n",
      "Current Time = 06:20:05\n",
      "step 1690: train loss 0.2314, val loss 10.9688\n",
      "Current Time = 06:22:29\n",
      "step 1695: train loss 0.2293, val loss 10.9824\n",
      "Current Time = 06:24:52\n",
      "step 1700: train loss 0.2293, val loss 10.9278\n",
      "Current Time = 06:27:16\n",
      "step 1705: train loss 0.2288, val loss 10.9660\n",
      "Current Time = 06:29:40\n",
      "step 1710: train loss 0.2270, val loss 10.9007\n",
      "Current Time = 06:32:02\n",
      "step 1715: train loss 0.2283, val loss 10.9777\n",
      "Current Time = 06:34:28\n",
      "step 1720: train loss 0.2263, val loss 11.0172\n",
      "Current Time = 06:36:51\n",
      "step 1725: train loss 0.2245, val loss 11.0369\n",
      "Current Time = 06:39:13\n",
      "step 1730: train loss 0.2243, val loss 11.0530\n",
      "Current Time = 06:41:37\n",
      "step 1735: train loss 0.2254, val loss 10.9768\n",
      "Current Time = 06:44:01\n",
      "step 1740: train loss 0.2235, val loss 11.0579\n",
      "Current Time = 06:46:26\n",
      "step 1745: train loss 0.2218, val loss 11.0863\n",
      "Current Time = 06:48:50\n",
      "step 1750: train loss 0.2221, val loss 11.0541\n",
      "Current Time = 06:51:13\n",
      "step 1755: train loss 0.2225, val loss 11.0015\n",
      "Current Time = 06:53:37\n",
      "step 1760: train loss 0.2215, val loss 11.1059\n",
      "Current Time = 06:56:00\n",
      "step 1765: train loss 0.2229, val loss 11.0467\n",
      "Current Time = 06:58:22\n",
      "step 1770: train loss 0.2202, val loss 11.0696\n",
      "Current Time = 07:00:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1775: train loss 0.2206, val loss 11.0479\n",
      "Current Time = 07:03:11\n",
      "step 1780: train loss 0.2200, val loss 11.1028\n",
      "Current Time = 07:05:35\n",
      "step 1785: train loss 0.2183, val loss 11.1005\n",
      "Current Time = 07:07:58\n",
      "step 1790: train loss 0.2179, val loss 11.1151\n",
      "Current Time = 07:10:22\n",
      "step 1795: train loss 0.2175, val loss 11.1098\n",
      "Current Time = 07:12:46\n",
      "step 1800: train loss 0.2173, val loss 11.1151\n",
      "Current Time = 07:15:10\n",
      "step 1805: train loss 0.2164, val loss 11.1296\n",
      "Current Time = 07:17:34\n",
      "step 1810: train loss 0.2174, val loss 11.0861\n",
      "Current Time = 07:19:59\n",
      "step 1815: train loss 0.2148, val loss 11.1425\n",
      "Current Time = 07:22:23\n",
      "step 1820: train loss 0.2141, val loss 11.1266\n",
      "Current Time = 07:24:47\n",
      "step 1825: train loss 0.2131, val loss 11.1343\n",
      "Current Time = 07:27:10\n",
      "step 1830: train loss 0.2117, val loss 11.2118\n",
      "Current Time = 07:29:35\n",
      "step 1835: train loss 0.2133, val loss 11.1146\n",
      "Current Time = 07:31:59\n",
      "step 1840: train loss 0.2116, val loss 11.1634\n",
      "Current Time = 07:34:24\n",
      "step 1845: train loss 0.2118, val loss 11.1957\n",
      "Current Time = 07:36:49\n",
      "step 1850: train loss 0.2112, val loss 11.1591\n",
      "Current Time = 07:39:12\n",
      "step 1855: train loss 0.2098, val loss 11.1961\n",
      "Current Time = 07:41:39\n",
      "step 1860: train loss 0.2107, val loss 11.2146\n",
      "Current Time = 07:44:03\n",
      "step 1865: train loss 0.2099, val loss 11.1594\n",
      "Current Time = 07:46:29\n",
      "step 1870: train loss 0.2094, val loss 11.1360\n",
      "Current Time = 07:48:54\n",
      "step 1875: train loss 0.2078, val loss 11.1891\n",
      "Current Time = 07:51:19\n",
      "step 1880: train loss 0.2084, val loss 11.2447\n",
      "Current Time = 07:53:44\n",
      "step 1885: train loss 0.2070, val loss 11.2192\n",
      "Current Time = 07:56:09\n",
      "step 1890: train loss 0.2076, val loss 11.2714\n",
      "Current Time = 07:58:31\n",
      "step 1895: train loss 0.2069, val loss 11.1866\n",
      "Current Time = 08:00:54\n",
      "step 1900: train loss 0.2071, val loss 11.2462\n",
      "Current Time = 08:03:17\n",
      "step 1905: train loss 0.2059, val loss 11.1895\n",
      "Current Time = 08:05:41\n",
      "step 1910: train loss 0.2052, val loss 11.2514\n",
      "Current Time = 08:08:05\n",
      "step 1915: train loss 0.2074, val loss 11.2106\n",
      "Current Time = 08:10:29\n",
      "step 1920: train loss 0.2041, val loss 11.2406\n",
      "Current Time = 08:12:54\n",
      "step 1925: train loss 0.2032, val loss 11.2707\n",
      "Current Time = 08:15:20\n",
      "step 1930: train loss 0.2057, val loss 11.2804\n",
      "Current Time = 08:17:44\n",
      "step 1935: train loss 0.2035, val loss 11.2175\n",
      "Current Time = 08:20:07\n",
      "step 1940: train loss 0.2025, val loss 11.2275\n",
      "Current Time = 08:22:32\n",
      "step 1945: train loss 0.2023, val loss 11.2499\n",
      "Current Time = 08:24:56\n",
      "step 1950: train loss 0.2022, val loss 11.3437\n",
      "Current Time = 08:27:18\n",
      "step 1955: train loss 0.2016, val loss 11.2727\n",
      "Current Time = 08:29:41\n",
      "step 1960: train loss 0.2007, val loss 11.3139\n",
      "Current Time = 08:32:03\n",
      "step 1965: train loss 0.2004, val loss 11.2567\n",
      "Current Time = 08:34:26\n",
      "step 1970: train loss 0.2004, val loss 11.2547\n",
      "Current Time = 08:36:49\n",
      "step 1975: train loss 0.1986, val loss 11.3312\n",
      "Current Time = 08:39:11\n",
      "step 1980: train loss 0.1977, val loss 11.2910\n",
      "Current Time = 08:41:34\n",
      "step 1985: train loss 0.2006, val loss 11.2623\n",
      "Current Time = 08:43:56\n",
      "step 1990: train loss 0.1996, val loss 11.3132\n",
      "Current Time = 08:46:20\n",
      "step 1995: train loss 0.1998, val loss 11.2921\n",
      "Current Time = 08:48:41\n",
      "step 2000: train loss 0.1957, val loss 11.3099\n",
      "Current Time = 08:51:05\n",
      "step 2005: train loss 0.1967, val loss 11.3126\n",
      "Current Time = 08:53:26\n",
      "step 2010: train loss 0.1949, val loss 11.3180\n",
      "Current Time = 08:55:49\n",
      "step 2015: train loss 0.1960, val loss 11.3111\n",
      "Current Time = 08:58:12\n",
      "step 2020: train loss 0.1963, val loss 11.3422\n",
      "Current Time = 09:00:33\n",
      "step 2025: train loss 0.1945, val loss 11.3801\n",
      "Current Time = 09:02:55\n",
      "step 2030: train loss 0.1944, val loss 11.3444\n",
      "Current Time = 09:05:17\n",
      "step 2035: train loss 0.1940, val loss 11.3596\n",
      "Current Time = 09:07:39\n",
      "step 2040: train loss 0.1945, val loss 11.3253\n",
      "Current Time = 09:10:02\n",
      "step 2045: train loss 0.1953, val loss 11.3022\n",
      "Current Time = 09:12:24\n",
      "step 2050: train loss 0.1934, val loss 11.4062\n",
      "Current Time = 09:14:47\n",
      "step 2055: train loss 0.1928, val loss 11.4709\n",
      "Current Time = 09:17:09\n",
      "step 2060: train loss 0.1935, val loss 11.4294\n",
      "Current Time = 09:19:31\n",
      "step 2065: train loss 0.1910, val loss 11.4295\n",
      "Current Time = 09:21:52\n",
      "step 2070: train loss 0.1921, val loss 11.3902\n",
      "Current Time = 09:24:15\n",
      "step 2075: train loss 0.1910, val loss 11.3890\n",
      "Current Time = 09:26:38\n",
      "step 2080: train loss 0.1911, val loss 11.4231\n",
      "Current Time = 09:29:01\n",
      "step 2085: train loss 0.1899, val loss 11.3793\n",
      "Current Time = 09:31:21\n",
      "step 2090: train loss 0.1878, val loss 11.4300\n",
      "Current Time = 09:33:44\n",
      "step 2095: train loss 0.1894, val loss 11.4053\n",
      "Current Time = 09:36:05\n",
      "step 2100: train loss 0.1886, val loss 11.4679\n",
      "Current Time = 09:38:28\n",
      "step 2105: train loss 0.1879, val loss 11.4538\n",
      "Current Time = 09:40:51\n",
      "step 2110: train loss 0.1871, val loss 11.4912\n",
      "Current Time = 09:43:13\n",
      "step 2115: train loss 0.1888, val loss 11.3849\n",
      "Current Time = 09:45:36\n",
      "step 2120: train loss 0.1890, val loss 11.5112\n",
      "Current Time = 09:47:57\n",
      "step 2125: train loss 0.1874, val loss 11.4882\n",
      "Current Time = 09:50:20\n",
      "step 2130: train loss 0.1847, val loss 11.4654\n",
      "Current Time = 09:52:42\n",
      "step 2135: train loss 0.1866, val loss 11.4822\n",
      "Current Time = 09:55:04\n",
      "step 2140: train loss 0.1866, val loss 11.5059\n",
      "Current Time = 09:57:25\n",
      "step 2145: train loss 0.1855, val loss 11.4746\n",
      "Current Time = 09:59:47\n",
      "step 2150: train loss 0.1855, val loss 11.5230\n",
      "Current Time = 10:02:10\n",
      "step 2155: train loss 0.1855, val loss 11.4587\n",
      "Current Time = 10:04:31\n",
      "step 2160: train loss 0.1866, val loss 11.4824\n",
      "Current Time = 10:06:53\n",
      "step 2165: train loss 0.1838, val loss 11.5319\n",
      "Current Time = 10:09:15\n",
      "step 2170: train loss 0.1833, val loss 11.4867\n",
      "Current Time = 10:11:37\n",
      "step 2175: train loss 0.1817, val loss 11.5485\n",
      "Current Time = 10:13:59\n",
      "step 2180: train loss 0.1828, val loss 11.5374\n",
      "Current Time = 10:16:21\n",
      "step 2185: train loss 0.1830, val loss 11.5381\n",
      "Current Time = 10:18:44\n",
      "step 2190: train loss 0.1826, val loss 11.5519\n",
      "Current Time = 10:21:05\n",
      "step 2195: train loss 0.1809, val loss 11.5341\n",
      "Current Time = 10:23:27\n",
      "step 2200: train loss 0.1828, val loss 11.5397\n",
      "Current Time = 10:25:49\n",
      "step 2205: train loss 0.1815, val loss 11.5094\n",
      "Current Time = 10:28:12\n",
      "step 2210: train loss 0.1805, val loss 11.5127\n",
      "Current Time = 10:30:32\n",
      "step 2215: train loss 0.1815, val loss 11.5402\n",
      "Current Time = 10:32:54\n",
      "step 2220: train loss 0.1806, val loss 11.5890\n",
      "Current Time = 10:35:15\n",
      "step 2225: train loss 0.1806, val loss 11.5572\n",
      "Current Time = 10:37:37\n",
      "step 2230: train loss 0.1796, val loss 11.5441\n",
      "Current Time = 10:40:00\n",
      "step 2235: train loss 0.1806, val loss 11.5909\n",
      "Current Time = 10:42:21\n",
      "step 2240: train loss 0.1801, val loss 11.5702\n",
      "Current Time = 10:44:42\n",
      "step 2245: train loss 0.1781, val loss 11.6162\n",
      "Current Time = 10:47:05\n",
      "step 2250: train loss 0.1775, val loss 11.5680\n",
      "Current Time = 10:49:26\n",
      "step 2255: train loss 0.1773, val loss 11.6236\n",
      "Current Time = 10:51:47\n",
      "step 2260: train loss 0.1765, val loss 11.5369\n",
      "Current Time = 10:54:09\n",
      "step 2265: train loss 0.1777, val loss 11.5857\n",
      "Current Time = 10:56:32\n",
      "step 2270: train loss 0.1778, val loss 11.5578\n",
      "Current Time = 10:58:52\n",
      "step 2275: train loss 0.1762, val loss 11.5669\n",
      "Current Time = 11:01:15\n",
      "step 2280: train loss 0.1779, val loss 11.5412\n",
      "Current Time = 11:03:37\n",
      "step 2285: train loss 0.1763, val loss 11.6066\n",
      "Current Time = 11:05:58\n",
      "step 2290: train loss 0.1772, val loss 11.6194\n",
      "Current Time = 11:08:20\n",
      "step 2295: train loss 0.1769, val loss 11.5891\n",
      "Current Time = 11:10:43\n",
      "step 2300: train loss 0.1761, val loss 11.6086\n",
      "Current Time = 11:13:05\n",
      "step 2305: train loss 0.1767, val loss 11.6201\n",
      "Current Time = 11:15:27\n",
      "step 2310: train loss 0.1750, val loss 11.6214\n",
      "Current Time = 11:17:49\n",
      "step 2315: train loss 0.1753, val loss 11.6899\n",
      "Current Time = 11:20:11\n",
      "step 2320: train loss 0.1750, val loss 11.5688\n",
      "Current Time = 11:22:34\n",
      "step 2325: train loss 0.1736, val loss 11.6755\n",
      "Current Time = 11:24:56\n",
      "step 2330: train loss 0.1747, val loss 11.5918\n",
      "Current Time = 11:27:18\n",
      "step 2335: train loss 0.1742, val loss 11.6257\n",
      "Current Time = 11:29:41\n",
      "step 2340: train loss 0.1740, val loss 11.6413\n",
      "Current Time = 11:32:04\n",
      "step 2345: train loss 0.1740, val loss 11.6774\n",
      "Current Time = 11:34:26\n",
      "step 2350: train loss 0.1726, val loss 11.6423\n",
      "Current Time = 11:36:49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2355: train loss 0.1742, val loss 11.6413\n",
      "Current Time = 11:39:11\n",
      "step 2360: train loss 0.1725, val loss 11.6231\n",
      "Current Time = 11:41:34\n",
      "step 2365: train loss 0.1725, val loss 11.7026\n",
      "Current Time = 11:43:56\n",
      "step 2370: train loss 0.1709, val loss 11.6851\n",
      "Current Time = 11:46:18\n",
      "step 2375: train loss 0.1706, val loss 11.6862\n",
      "Current Time = 11:48:41\n",
      "step 2380: train loss 0.1710, val loss 11.6820\n",
      "Current Time = 11:51:04\n",
      "step 2385: train loss 0.1703, val loss 11.7448\n",
      "Current Time = 11:53:27\n",
      "step 2390: train loss 0.1720, val loss 11.7633\n",
      "Current Time = 11:55:50\n",
      "step 2395: train loss 0.1701, val loss 11.7576\n",
      "Current Time = 11:58:13\n",
      "step 2400: train loss 0.1699, val loss 11.7949\n",
      "Current Time = 12:00:36\n",
      "step 2405: train loss 0.1705, val loss 11.7330\n",
      "Current Time = 12:02:59\n",
      "step 2410: train loss 0.1690, val loss 11.7185\n",
      "Current Time = 12:05:21\n",
      "step 2415: train loss 0.1697, val loss 11.6615\n",
      "Current Time = 12:07:42\n",
      "step 2420: train loss 0.1702, val loss 11.7403\n",
      "Current Time = 12:10:04\n",
      "step 2425: train loss 0.1695, val loss 11.7475\n",
      "Current Time = 12:12:26\n",
      "step 2430: train loss 0.1709, val loss 11.6712\n",
      "Current Time = 12:14:49\n",
      "step 2435: train loss 0.1684, val loss 11.7642\n",
      "Current Time = 12:17:11\n",
      "step 2440: train loss 0.1685, val loss 11.7707\n",
      "Current Time = 12:19:33\n",
      "step 2445: train loss 0.1682, val loss 11.7704\n",
      "Current Time = 12:21:55\n",
      "step 2450: train loss 0.1692, val loss 11.7022\n",
      "Current Time = 12:24:16\n",
      "step 2455: train loss 0.1671, val loss 11.7328\n",
      "Current Time = 12:26:38\n",
      "step 2460: train loss 0.1696, val loss 11.7605\n",
      "Current Time = 12:29:00\n",
      "step 2465: train loss 0.1670, val loss 11.7923\n",
      "Current Time = 12:31:23\n",
      "step 2470: train loss 0.1663, val loss 11.8321\n",
      "Current Time = 12:33:44\n",
      "step 2475: train loss 0.1673, val loss 11.7305\n",
      "Current Time = 12:36:04\n",
      "step 2480: train loss 0.1680, val loss 11.7526\n",
      "Current Time = 12:38:26\n",
      "step 2485: train loss 0.1669, val loss 11.8083\n",
      "Current Time = 12:40:49\n",
      "step 2490: train loss 0.1657, val loss 11.7694\n",
      "Current Time = 12:43:10\n",
      "step 2495: train loss 0.1657, val loss 11.7628\n",
      "Current Time = 12:45:30\n",
      "step 2500: train loss 0.1653, val loss 11.7845\n",
      "Current Time = 12:47:51\n",
      "step 2505: train loss 0.1654, val loss 11.8069\n",
      "Current Time = 12:50:11\n",
      "step 2510: train loss 0.1626, val loss 11.8173\n",
      "Current Time = 12:52:33\n",
      "step 2515: train loss 0.1647, val loss 11.7249\n",
      "Current Time = 12:54:53\n",
      "step 2520: train loss 0.1638, val loss 11.8626\n",
      "Current Time = 12:57:14\n",
      "step 2525: train loss 0.1638, val loss 11.7986\n",
      "Current Time = 12:59:36\n",
      "step 2530: train loss 0.1640, val loss 11.8009\n",
      "Current Time = 13:01:56\n",
      "step 2535: train loss 0.1650, val loss 11.8321\n",
      "Current Time = 13:04:17\n",
      "step 2540: train loss 0.1628, val loss 11.8014\n",
      "Current Time = 13:06:37\n",
      "step 2545: train loss 0.1624, val loss 11.8125\n",
      "Current Time = 13:08:58\n",
      "step 2550: train loss 0.1618, val loss 11.8772\n",
      "Current Time = 13:11:19\n",
      "step 2555: train loss 0.1631, val loss 11.7978\n",
      "Current Time = 13:13:38\n",
      "step 2560: train loss 0.1635, val loss 11.8189\n",
      "Current Time = 13:15:58\n",
      "step 2565: train loss 0.1636, val loss 11.8549\n",
      "Current Time = 13:18:18\n",
      "step 2570: train loss 0.1623, val loss 11.8676\n",
      "Current Time = 13:20:38\n",
      "step 2575: train loss 0.1614, val loss 11.8599\n",
      "Current Time = 13:22:58\n",
      "step 2580: train loss 0.1608, val loss 11.8640\n",
      "Current Time = 13:25:19\n",
      "step 2585: train loss 0.1608, val loss 11.8645\n",
      "Current Time = 13:27:38\n",
      "step 2590: train loss 0.1615, val loss 11.8731\n",
      "Current Time = 13:29:59\n",
      "step 2595: train loss 0.1608, val loss 11.8849\n",
      "Current Time = 13:32:17\n",
      "step 2600: train loss 0.1604, val loss 11.8057\n",
      "Current Time = 13:34:36\n",
      "step 2605: train loss 0.1598, val loss 11.8897\n",
      "Current Time = 13:36:56\n",
      "step 2610: train loss 0.1596, val loss 11.8610\n",
      "Current Time = 13:39:15\n",
      "step 2615: train loss 0.1610, val loss 11.8478\n",
      "Current Time = 13:41:33\n",
      "step 2620: train loss 0.1584, val loss 11.9582\n",
      "Current Time = 13:43:53\n",
      "step 2625: train loss 0.1604, val loss 11.8781\n",
      "Current Time = 13:46:11\n",
      "step 2630: train loss 0.1575, val loss 11.8964\n",
      "Current Time = 13:48:31\n",
      "step 2635: train loss 0.1589, val loss 11.8617\n",
      "Current Time = 13:50:50\n",
      "step 2640: train loss 0.1589, val loss 11.9385\n",
      "Current Time = 13:53:10\n",
      "step 2645: train loss 0.1587, val loss 11.8951\n",
      "Current Time = 13:55:29\n",
      "step 2650: train loss 0.1584, val loss 11.9110\n",
      "Current Time = 13:57:49\n",
      "step 2655: train loss 0.1587, val loss 11.8824\n",
      "Current Time = 14:00:08\n",
      "step 2660: train loss 0.1566, val loss 11.9997\n",
      "Current Time = 14:02:27\n",
      "step 2665: train loss 0.1588, val loss 11.8774\n",
      "Current Time = 14:04:46\n",
      "step 2670: train loss 0.1578, val loss 11.9328\n",
      "Current Time = 14:07:06\n",
      "step 2675: train loss 0.1571, val loss 11.8810\n",
      "Current Time = 14:09:26\n",
      "step 2680: train loss 0.1574, val loss 11.9889\n",
      "Current Time = 14:11:46\n",
      "step 2685: train loss 0.1574, val loss 11.8661\n",
      "Current Time = 14:14:06\n",
      "step 2690: train loss 0.1571, val loss 11.9423\n",
      "Current Time = 14:16:25\n",
      "step 2695: train loss 0.1564, val loss 11.9991\n",
      "Current Time = 14:18:46\n",
      "step 2700: train loss 0.1565, val loss 11.9054\n",
      "Current Time = 14:21:05\n",
      "step 2705: train loss 0.1547, val loss 11.9817\n",
      "Current Time = 14:23:23\n",
      "step 2710: train loss 0.1548, val loss 11.9161\n",
      "Current Time = 14:25:42\n",
      "step 2715: train loss 0.1555, val loss 11.9698\n",
      "Current Time = 14:28:02\n",
      "step 2720: train loss 0.1559, val loss 11.8723\n",
      "Current Time = 14:30:21\n",
      "step 2725: train loss 0.1549, val loss 11.9811\n",
      "Current Time = 14:32:42\n",
      "step 2730: train loss 0.1557, val loss 11.9523\n",
      "Current Time = 14:35:01\n",
      "step 2735: train loss 0.1552, val loss 11.9241\n",
      "Current Time = 14:37:21\n",
      "step 2740: train loss 0.1550, val loss 12.0011\n",
      "Current Time = 14:39:40\n",
      "step 2745: train loss 0.1550, val loss 11.8949\n",
      "Current Time = 14:41:59\n",
      "step 2750: train loss 0.1549, val loss 11.9795\n",
      "Current Time = 14:44:17\n",
      "step 2755: train loss 0.1550, val loss 11.9540\n",
      "Current Time = 14:46:36\n",
      "step 2760: train loss 0.1539, val loss 11.9758\n",
      "Current Time = 14:48:55\n",
      "step 2765: train loss 0.1548, val loss 11.9567\n",
      "Current Time = 14:51:14\n",
      "step 2770: train loss 0.1530, val loss 12.0077\n",
      "Current Time = 14:53:33\n",
      "step 2775: train loss 0.1533, val loss 12.0418\n",
      "Current Time = 14:55:52\n",
      "step 2780: train loss 0.1542, val loss 11.9196\n",
      "Current Time = 14:58:12\n",
      "step 2785: train loss 0.1533, val loss 12.0195\n",
      "Current Time = 15:00:32\n",
      "step 2790: train loss 0.1523, val loss 11.9546\n",
      "Current Time = 15:02:53\n",
      "step 2795: train loss 0.1530, val loss 12.0552\n",
      "Current Time = 15:05:12\n",
      "step 2800: train loss 0.1520, val loss 12.0729\n",
      "Current Time = 15:07:32\n",
      "step 2805: train loss 0.1526, val loss 11.9468\n",
      "Current Time = 15:09:52\n",
      "step 2810: train loss 0.1519, val loss 12.0435\n",
      "Current Time = 15:12:11\n",
      "step 2815: train loss 0.1528, val loss 12.0467\n",
      "Current Time = 15:14:32\n",
      "step 2820: train loss 0.1526, val loss 12.0646\n",
      "Current Time = 15:16:52\n",
      "step 2825: train loss 0.1543, val loss 12.0074\n",
      "Current Time = 15:19:13\n",
      "step 2830: train loss 0.1511, val loss 12.0728\n",
      "Current Time = 15:21:32\n",
      "step 2835: train loss 0.1531, val loss 11.9772\n",
      "Current Time = 15:23:52\n",
      "step 2840: train loss 0.1512, val loss 12.1253\n",
      "Current Time = 15:26:13\n",
      "step 2845: train loss 0.1527, val loss 12.0485\n",
      "Current Time = 15:28:33\n",
      "step 2850: train loss 0.1503, val loss 12.0707\n",
      "Current Time = 15:30:54\n",
      "step 2855: train loss 0.1517, val loss 12.0381\n",
      "Current Time = 15:33:15\n",
      "step 2860: train loss 0.1499, val loss 12.0424\n",
      "Current Time = 15:35:35\n",
      "step 2865: train loss 0.1494, val loss 12.1120\n",
      "Current Time = 15:37:57\n",
      "step 2870: train loss 0.1501, val loss 12.0361\n",
      "Current Time = 15:40:17\n",
      "step 2875: train loss 0.1487, val loss 12.0670\n",
      "Current Time = 15:42:38\n",
      "step 2880: train loss 0.1483, val loss 12.0646\n",
      "Current Time = 15:45:00\n",
      "step 2885: train loss 0.1503, val loss 12.0961\n",
      "Current Time = 15:47:20\n",
      "step 2890: train loss 0.1498, val loss 12.0115\n",
      "Current Time = 15:49:42\n",
      "step 2895: train loss 0.1482, val loss 12.1162\n",
      "Current Time = 15:52:04\n",
      "step 2900: train loss 0.1494, val loss 12.0667\n",
      "Current Time = 15:54:26\n",
      "step 2905: train loss 0.1483, val loss 12.0944\n",
      "Current Time = 15:56:48\n",
      "step 2910: train loss 0.1493, val loss 12.0372\n",
      "Current Time = 15:59:10\n",
      "step 2915: train loss 0.1485, val loss 12.0914\n",
      "Current Time = 16:01:33\n",
      "step 2920: train loss 0.1482, val loss 12.0982\n",
      "Current Time = 16:03:54\n",
      "step 2925: train loss 0.1479, val loss 12.1636\n",
      "Current Time = 16:06:15\n",
      "step 2930: train loss 0.1457, val loss 12.1300\n",
      "Current Time = 16:08:35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2935: train loss 0.1467, val loss 12.0660\n",
      "Current Time = 16:10:55\n",
      "step 2940: train loss 0.1471, val loss 12.1348\n",
      "Current Time = 16:13:15\n",
      "step 2945: train loss 0.1475, val loss 12.0506\n",
      "Current Time = 16:15:35\n",
      "step 2950: train loss 0.1474, val loss 12.1397\n",
      "Current Time = 16:17:55\n",
      "step 2955: train loss 0.1466, val loss 12.1211\n",
      "Current Time = 16:20:14\n",
      "step 2960: train loss 0.1465, val loss 12.0998\n",
      "Current Time = 16:22:34\n",
      "step 2965: train loss 0.1460, val loss 12.1961\n",
      "Current Time = 16:24:55\n",
      "step 2970: train loss 0.1469, val loss 12.1028\n",
      "Current Time = 16:27:14\n",
      "step 2975: train loss 0.1456, val loss 12.2032\n",
      "Current Time = 16:29:34\n",
      "step 2980: train loss 0.1485, val loss 12.1207\n",
      "Current Time = 16:31:53\n",
      "step 2985: train loss 0.1471, val loss 12.1704\n",
      "Current Time = 16:34:14\n",
      "step 2990: train loss 0.1468, val loss 12.1767\n",
      "Current Time = 16:36:32\n",
      "step 2995: train loss 0.1466, val loss 12.1381\n",
      "Current Time = 16:38:52\n",
      "step 3000: train loss 0.1453, val loss 12.1514\n",
      "Current Time = 16:41:11\n",
      "step 3005: train loss 0.1447, val loss 12.1996\n",
      "Current Time = 16:43:30\n",
      "step 3010: train loss 0.1451, val loss 12.1999\n",
      "Current Time = 16:45:50\n",
      "step 3015: train loss 0.1440, val loss 12.2042\n",
      "Current Time = 16:48:08\n",
      "step 3020: train loss 0.1457, val loss 12.0710\n",
      "Current Time = 16:50:27\n",
      "step 3025: train loss 0.1440, val loss 12.1712\n",
      "Current Time = 16:52:46\n",
      "step 3030: train loss 0.1436, val loss 12.1729\n",
      "Current Time = 16:55:05\n",
      "step 3035: train loss 0.1440, val loss 12.1398\n",
      "Current Time = 16:57:24\n",
      "step 3040: train loss 0.1461, val loss 12.1297\n",
      "Current Time = 16:59:43\n",
      "step 3045: train loss 0.1441, val loss 12.2415\n",
      "Current Time = 17:02:01\n",
      "step 3050: train loss 0.1444, val loss 12.1614\n",
      "Current Time = 17:04:22\n",
      "step 3055: train loss 0.1437, val loss 12.2457\n",
      "Current Time = 17:06:41\n",
      "step 3060: train loss 0.1457, val loss 12.2160\n",
      "Current Time = 17:09:00\n",
      "step 3065: train loss 0.1437, val loss 12.2137\n",
      "Current Time = 17:11:20\n",
      "step 3070: train loss 0.1429, val loss 12.1798\n",
      "Current Time = 17:13:39\n",
      "step 3075: train loss 0.1415, val loss 12.2567\n",
      "Current Time = 17:15:59\n",
      "step 3080: train loss 0.1422, val loss 12.2100\n",
      "Current Time = 17:18:17\n",
      "step 3085: train loss 0.1441, val loss 12.1995\n",
      "Current Time = 17:20:38\n",
      "step 3090: train loss 0.1420, val loss 12.2062\n",
      "Current Time = 17:22:57\n",
      "step 3095: train loss 0.1416, val loss 12.1758\n",
      "Current Time = 17:25:17\n",
      "step 3100: train loss 0.1418, val loss 12.1720\n",
      "Current Time = 17:27:36\n",
      "step 3105: train loss 0.1415, val loss 12.1897\n",
      "Current Time = 17:29:54\n",
      "step 3110: train loss 0.1416, val loss 12.2155\n",
      "Current Time = 17:32:14\n",
      "step 3115: train loss 0.1422, val loss 12.2487\n",
      "Current Time = 17:34:34\n",
      "step 3120: train loss 0.1406, val loss 12.2589\n",
      "Current Time = 17:36:54\n",
      "step 3125: train loss 0.1418, val loss 12.2246\n",
      "Current Time = 17:39:13\n",
      "step 3130: train loss 0.1411, val loss 12.2402\n",
      "Current Time = 17:41:33\n",
      "step 3135: train loss 0.1395, val loss 12.2263\n",
      "Current Time = 17:43:51\n",
      "step 3140: train loss 0.1407, val loss 12.2025\n",
      "Current Time = 17:46:10\n",
      "step 3145: train loss 0.1406, val loss 12.2931\n",
      "Current Time = 17:48:28\n",
      "step 3150: train loss 0.1419, val loss 12.2480\n",
      "Current Time = 17:50:48\n",
      "step 3155: train loss 0.1400, val loss 12.2964\n",
      "Current Time = 17:53:07\n",
      "step 3160: train loss 0.1423, val loss 12.2290\n",
      "Current Time = 17:55:27\n",
      "step 3165: train loss 0.1400, val loss 12.3231\n",
      "Current Time = 17:57:45\n",
      "step 3170: train loss 0.1394, val loss 12.1948\n",
      "Current Time = 18:00:05\n",
      "step 3175: train loss 0.1399, val loss 12.2204\n",
      "Current Time = 18:02:23\n",
      "step 3180: train loss 0.1398, val loss 12.2985\n",
      "Current Time = 18:04:43\n",
      "step 3185: train loss 0.1399, val loss 12.3108\n",
      "Current Time = 18:07:01\n",
      "step 3190: train loss 0.1402, val loss 12.2308\n",
      "Current Time = 18:09:20\n",
      "step 3195: train loss 0.1389, val loss 12.4222\n",
      "Current Time = 18:11:39\n",
      "step 3200: train loss 0.1406, val loss 12.2256\n",
      "Current Time = 18:13:59\n",
      "step 3205: train loss 0.1396, val loss 12.2940\n",
      "Current Time = 18:16:19\n",
      "step 3210: train loss 0.1402, val loss 12.2731\n",
      "Current Time = 18:18:38\n",
      "step 3215: train loss 0.1374, val loss 12.3224\n",
      "Current Time = 18:20:57\n",
      "step 3220: train loss 0.1367, val loss 12.2833\n",
      "Current Time = 18:23:17\n",
      "step 3225: train loss 0.1397, val loss 12.2290\n",
      "Current Time = 18:25:36\n",
      "step 3230: train loss 0.1384, val loss 12.3184\n",
      "Current Time = 18:27:54\n",
      "step 3235: train loss 0.1386, val loss 12.2781\n",
      "Current Time = 18:30:13\n",
      "step 3240: train loss 0.1375, val loss 12.3336\n",
      "Current Time = 18:32:31\n",
      "step 3245: train loss 0.1372, val loss 12.3461\n",
      "Current Time = 18:34:50\n",
      "step 3250: train loss 0.1378, val loss 12.2715\n",
      "Current Time = 18:37:08\n",
      "step 3255: train loss 0.1370, val loss 12.4056\n",
      "Current Time = 18:39:26\n",
      "step 3260: train loss 0.1386, val loss 12.3178\n",
      "Current Time = 18:41:45\n",
      "step 3265: train loss 0.1382, val loss 12.4013\n",
      "Current Time = 18:44:03\n",
      "step 3270: train loss 0.1371, val loss 12.3401\n",
      "Current Time = 18:46:22\n",
      "step 3275: train loss 0.1370, val loss 12.3892\n",
      "Current Time = 18:48:41\n",
      "step 3280: train loss 0.1361, val loss 12.3292\n",
      "Current Time = 18:50:58\n",
      "step 3285: train loss 0.1369, val loss 12.4072\n",
      "Current Time = 18:53:17\n",
      "step 3290: train loss 0.1371, val loss 12.3446\n",
      "Current Time = 18:55:35\n",
      "step 3295: train loss 0.1377, val loss 12.3733\n",
      "Current Time = 18:57:54\n",
      "step 3300: train loss 0.1379, val loss 12.3442\n",
      "Current Time = 19:00:14\n",
      "step 3305: train loss 0.1373, val loss 12.3874\n",
      "Current Time = 19:02:32\n",
      "step 3310: train loss 0.1379, val loss 12.3804\n",
      "Current Time = 19:04:52\n",
      "step 3315: train loss 0.1360, val loss 12.4674\n",
      "Current Time = 19:07:10\n",
      "step 3320: train loss 0.1364, val loss 12.3299\n",
      "Current Time = 19:09:29\n",
      "step 3325: train loss 0.1371, val loss 12.3967\n",
      "Current Time = 19:11:48\n",
      "step 3330: train loss 0.1361, val loss 12.3989\n",
      "Current Time = 19:14:08\n",
      "step 3335: train loss 0.1359, val loss 12.3927\n",
      "Current Time = 19:16:28\n",
      "step 3340: train loss 0.1354, val loss 12.3357\n",
      "Current Time = 19:18:47\n",
      "step 3345: train loss 0.1352, val loss 12.4067\n",
      "Current Time = 19:21:06\n",
      "step 3350: train loss 0.1360, val loss 12.3928\n",
      "Current Time = 19:23:26\n",
      "step 3355: train loss 0.1351, val loss 12.4175\n",
      "Current Time = 19:25:45\n",
      "step 3360: train loss 0.1352, val loss 12.4099\n",
      "Current Time = 19:28:05\n",
      "step 3365: train loss 0.1351, val loss 12.3996\n",
      "Current Time = 19:30:23\n",
      "step 3370: train loss 0.1338, val loss 12.4265\n",
      "Current Time = 19:32:42\n",
      "step 3375: train loss 0.1339, val loss 12.4139\n",
      "Current Time = 19:35:02\n",
      "step 3380: train loss 0.1346, val loss 12.4070\n",
      "Current Time = 19:37:21\n",
      "step 3385: train loss 0.1330, val loss 12.4398\n",
      "Current Time = 19:39:41\n",
      "step 3390: train loss 0.1334, val loss 12.3809\n",
      "Current Time = 19:41:59\n",
      "step 3395: train loss 0.1329, val loss 12.4307\n",
      "Current Time = 19:44:18\n",
      "step 3400: train loss 0.1343, val loss 12.4081\n",
      "Current Time = 19:46:38\n",
      "step 3405: train loss 0.1330, val loss 12.4432\n",
      "Current Time = 19:48:57\n",
      "step 3410: train loss 0.1321, val loss 12.4149\n",
      "Current Time = 19:51:17\n",
      "step 3415: train loss 0.1344, val loss 12.3889\n",
      "Current Time = 19:53:36\n",
      "step 3420: train loss 0.1329, val loss 12.4576\n",
      "Current Time = 19:55:54\n",
      "step 3425: train loss 0.1335, val loss 12.4197\n",
      "Current Time = 19:58:14\n",
      "step 3430: train loss 0.1342, val loss 12.4344\n",
      "Current Time = 20:00:32\n",
      "step 3435: train loss 0.1336, val loss 12.4128\n",
      "Current Time = 20:02:52\n",
      "step 3440: train loss 0.1350, val loss 12.3844\n",
      "Current Time = 20:05:12\n",
      "step 3445: train loss 0.1324, val loss 12.3908\n",
      "Current Time = 20:07:30\n",
      "step 3450: train loss 0.1332, val loss 12.4419\n",
      "Current Time = 20:09:49\n",
      "step 3455: train loss 0.1332, val loss 12.4061\n",
      "Current Time = 20:12:07\n",
      "step 3460: train loss 0.1329, val loss 12.4955\n",
      "Current Time = 20:14:26\n",
      "step 3465: train loss 0.1330, val loss 12.4835\n",
      "Current Time = 20:16:44\n",
      "step 3470: train loss 0.1303, val loss 12.5173\n",
      "Current Time = 20:19:03\n",
      "step 3475: train loss 0.1301, val loss 12.4954\n",
      "Current Time = 20:21:22\n",
      "step 3480: train loss 0.1317, val loss 12.4745\n",
      "Current Time = 20:23:40\n",
      "step 3485: train loss 0.1314, val loss 12.4811\n",
      "Current Time = 20:25:59\n",
      "step 3490: train loss 0.1313, val loss 12.5263\n",
      "Current Time = 20:28:17\n",
      "step 3495: train loss 0.1325, val loss 12.4445\n",
      "Current Time = 20:30:36\n",
      "step 3500: train loss 0.1335, val loss 12.4183\n",
      "Current Time = 20:32:55\n",
      "step 3505: train loss 0.1319, val loss 12.5073\n",
      "Current Time = 20:35:14\n",
      "step 3510: train loss 0.1310, val loss 12.5423\n",
      "Current Time = 20:37:33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3515: train loss 0.1321, val loss 12.4707\n",
      "Current Time = 20:39:52\n",
      "step 3520: train loss 0.1306, val loss 12.5682\n",
      "Current Time = 20:42:12\n",
      "step 3525: train loss 0.1327, val loss 12.4993\n",
      "Current Time = 20:44:31\n",
      "step 3530: train loss 0.1307, val loss 12.5153\n",
      "Current Time = 20:46:48\n",
      "step 3535: train loss 0.1312, val loss 12.5445\n",
      "Current Time = 20:49:08\n",
      "step 3540: train loss 0.1312, val loss 12.5174\n",
      "Current Time = 20:51:27\n",
      "step 3545: train loss 0.1308, val loss 12.5526\n",
      "Current Time = 20:53:45\n",
      "step 3550: train loss 0.1318, val loss 12.4360\n",
      "Current Time = 20:56:03\n",
      "step 3555: train loss 0.1308, val loss 12.5265\n",
      "Current Time = 20:58:23\n",
      "step 3560: train loss 0.1303, val loss 12.4936\n",
      "Current Time = 21:00:41\n",
      "step 3565: train loss 0.1297, val loss 12.5707\n",
      "Current Time = 21:03:01\n",
      "step 3570: train loss 0.1296, val loss 12.5016\n",
      "Current Time = 21:05:20\n",
      "step 3575: train loss 0.1310, val loss 12.4830\n",
      "Current Time = 21:07:39\n",
      "step 3580: train loss 0.1295, val loss 12.4986\n",
      "Current Time = 21:09:59\n",
      "step 3585: train loss 0.1293, val loss 12.5222\n",
      "Current Time = 21:12:19\n",
      "step 3590: train loss 0.1287, val loss 12.5439\n",
      "Current Time = 21:14:39\n",
      "step 3595: train loss 0.1293, val loss 12.5306\n",
      "Current Time = 21:16:57\n",
      "step 3600: train loss 0.1302, val loss 12.5185\n",
      "Current Time = 21:19:16\n",
      "step 3605: train loss 0.1285, val loss 12.5730\n",
      "Current Time = 21:21:36\n",
      "step 3610: train loss 0.1287, val loss 12.5922\n",
      "Current Time = 21:23:54\n",
      "step 3615: train loss 0.1300, val loss 12.5768\n",
      "Current Time = 21:26:15\n",
      "step 3620: train loss 0.1292, val loss 12.5233\n",
      "Current Time = 21:28:34\n",
      "step 3625: train loss 0.1282, val loss 12.5234\n",
      "Current Time = 21:30:52\n",
      "step 3630: train loss 0.1278, val loss 12.5054\n",
      "Current Time = 21:33:12\n",
      "step 3635: train loss 0.1292, val loss 12.5189\n",
      "Current Time = 21:35:32\n",
      "step 3640: train loss 0.1273, val loss 12.5219\n",
      "Current Time = 21:37:51\n",
      "step 3645: train loss 0.1279, val loss 12.5425\n",
      "Current Time = 21:40:11\n",
      "step 3650: train loss 0.1270, val loss 12.5949\n",
      "Current Time = 21:42:30\n",
      "step 3655: train loss 0.1274, val loss 12.5790\n",
      "Current Time = 21:44:50\n",
      "step 3660: train loss 0.1277, val loss 12.5986\n",
      "Current Time = 21:47:08\n",
      "step 3665: train loss 0.1273, val loss 12.5625\n",
      "Current Time = 21:49:28\n",
      "step 3670: train loss 0.1282, val loss 12.5462\n",
      "Current Time = 21:51:46\n",
      "step 3675: train loss 0.1274, val loss 12.5992\n",
      "Current Time = 21:54:06\n",
      "step 3680: train loss 0.1273, val loss 12.6152\n",
      "Current Time = 21:56:25\n",
      "step 3685: train loss 0.1276, val loss 12.5948\n",
      "Current Time = 21:58:45\n",
      "step 3690: train loss 0.1266, val loss 12.5434\n",
      "Current Time = 22:01:04\n",
      "step 3695: train loss 0.1272, val loss 12.5573\n",
      "Current Time = 22:03:24\n",
      "step 3700: train loss 0.1257, val loss 12.6171\n",
      "Current Time = 22:05:43\n",
      "step 3705: train loss 0.1276, val loss 12.6179\n",
      "Current Time = 22:08:02\n",
      "step 3710: train loss 0.1273, val loss 12.6058\n",
      "Current Time = 22:10:22\n",
      "step 3715: train loss 0.1263, val loss 12.6245\n",
      "Current Time = 22:12:41\n",
      "step 3720: train loss 0.1270, val loss 12.6172\n",
      "Current Time = 22:15:00\n",
      "step 3725: train loss 0.1267, val loss 12.6218\n",
      "Current Time = 22:17:20\n",
      "step 3730: train loss 0.1264, val loss 12.6320\n",
      "Current Time = 22:19:38\n",
      "step 3735: train loss 0.1262, val loss 12.5936\n",
      "Current Time = 22:21:58\n",
      "step 3740: train loss 0.1252, val loss 12.6676\n",
      "Current Time = 22:24:17\n",
      "step 3745: train loss 0.1254, val loss 12.6489\n",
      "Current Time = 22:26:37\n",
      "step 3750: train loss 0.1266, val loss 12.6220\n",
      "Current Time = 22:28:57\n",
      "step 3755: train loss 0.1250, val loss 12.6549\n",
      "Current Time = 22:31:16\n",
      "step 3760: train loss 0.1251, val loss 12.6665\n",
      "Current Time = 22:33:36\n",
      "step 3765: train loss 0.1259, val loss 12.6177\n",
      "Current Time = 22:35:55\n",
      "step 3770: train loss 0.1254, val loss 12.6435\n",
      "Current Time = 22:38:15\n",
      "step 3775: train loss 0.1270, val loss 12.6596\n",
      "Current Time = 22:40:35\n",
      "step 3780: train loss 0.1254, val loss 12.6323\n",
      "Current Time = 22:42:53\n",
      "step 3785: train loss 0.1259, val loss 12.6739\n",
      "Current Time = 22:45:12\n",
      "step 3790: train loss 0.1260, val loss 12.6193\n",
      "Current Time = 22:47:31\n",
      "step 3795: train loss 0.1250, val loss 12.6526\n",
      "Current Time = 22:49:51\n",
      "step 3800: train loss 0.1261, val loss 12.6833\n",
      "Current Time = 22:52:10\n",
      "step 3805: train loss 0.1257, val loss 12.5800\n",
      "Current Time = 22:54:31\n",
      "step 3810: train loss 0.1256, val loss 12.7021\n",
      "Current Time = 22:56:53\n",
      "step 3815: train loss 0.1250, val loss 12.6958\n",
      "Current Time = 22:59:12\n",
      "step 3820: train loss 0.1250, val loss 12.6824\n",
      "Current Time = 23:01:35\n",
      "step 3825: train loss 0.1259, val loss 12.6909\n",
      "Current Time = 23:03:59\n",
      "step 3830: train loss 0.1239, val loss 12.6660\n",
      "Current Time = 23:06:22\n",
      "step 3835: train loss 0.1242, val loss 12.7000\n",
      "Current Time = 23:08:45\n",
      "step 3840: train loss 0.1247, val loss 12.7184\n",
      "Current Time = 23:11:07\n",
      "step 3845: train loss 0.1240, val loss 12.7078\n",
      "Current Time = 23:13:30\n",
      "step 3850: train loss 0.1228, val loss 12.6913\n",
      "Current Time = 23:15:54\n",
      "step 3855: train loss 0.1242, val loss 12.6521\n",
      "Current Time = 23:18:17\n",
      "step 3860: train loss 0.1228, val loss 12.7318\n",
      "Current Time = 23:20:39\n",
      "step 3865: train loss 0.1241, val loss 12.7005\n",
      "Current Time = 23:23:01\n",
      "step 3870: train loss 0.1241, val loss 12.7271\n",
      "Current Time = 23:25:22\n",
      "step 3875: train loss 0.1244, val loss 12.7114\n",
      "Current Time = 23:27:46\n",
      "step 3880: train loss 0.1237, val loss 12.7447\n",
      "Current Time = 23:30:09\n",
      "step 3885: train loss 0.1244, val loss 12.6653\n",
      "Current Time = 23:32:33\n",
      "step 3890: train loss 0.1242, val loss 12.7040\n",
      "Current Time = 23:34:55\n",
      "step 3895: train loss 0.1231, val loss 12.6872\n",
      "Current Time = 23:37:18\n",
      "step 3900: train loss 0.1235, val loss 12.6394\n",
      "Current Time = 23:39:41\n",
      "step 3905: train loss 0.1230, val loss 12.7448\n",
      "Current Time = 23:42:04\n",
      "step 3910: train loss 0.1231, val loss 12.6952\n",
      "Current Time = 23:44:27\n",
      "step 3915: train loss 0.1238, val loss 12.6988\n",
      "Current Time = 23:46:50\n",
      "step 3920: train loss 0.1231, val loss 12.6563\n",
      "Current Time = 23:49:12\n",
      "step 3925: train loss 0.1223, val loss 12.6175\n",
      "Current Time = 23:51:36\n",
      "step 3930: train loss 0.1215, val loss 12.7308\n",
      "Current Time = 23:54:01\n",
      "step 3935: train loss 0.1221, val loss 12.6743\n",
      "Current Time = 23:56:27\n",
      "step 3940: train loss 0.1212, val loss 12.7129\n",
      "Current Time = 23:58:52\n",
      "step 3945: train loss 0.1234, val loss 12.6998\n",
      "Current Time = 00:01:18\n",
      "step 3950: train loss 0.1217, val loss 12.7172\n",
      "Current Time = 00:03:43\n",
      "step 3955: train loss 0.1220, val loss 12.7473\n",
      "Current Time = 00:06:09\n",
      "step 3960: train loss 0.1227, val loss 12.7129\n",
      "Current Time = 00:08:36\n",
      "step 3965: train loss 0.1220, val loss 12.7427\n",
      "Current Time = 00:11:02\n",
      "step 3970: train loss 0.1223, val loss 12.7854\n",
      "Current Time = 00:13:27\n",
      "step 3975: train loss 0.1217, val loss 12.7849\n",
      "Current Time = 00:15:52\n",
      "step 3980: train loss 0.1216, val loss 12.7440\n",
      "Current Time = 00:18:17\n",
      "step 3985: train loss 0.1211, val loss 12.7271\n",
      "Current Time = 00:20:42\n",
      "step 3990: train loss 0.1214, val loss 12.7268\n",
      "Current Time = 00:23:09\n",
      "step 3995: train loss 0.1219, val loss 12.6930\n",
      "Current Time = 00:25:34\n",
      "step 4000: train loss 0.1224, val loss 12.7828\n",
      "Current Time = 00:27:59\n",
      "step 4005: train loss 0.1222, val loss 12.7191\n",
      "Current Time = 00:30:24\n",
      "step 4010: train loss 0.1217, val loss 12.7792\n",
      "Current Time = 00:32:49\n",
      "step 4015: train loss 0.1198, val loss 12.7562\n",
      "Current Time = 00:35:14\n",
      "step 4020: train loss 0.1205, val loss 12.8206\n",
      "Current Time = 00:37:38\n",
      "step 4025: train loss 0.1212, val loss 12.8167\n",
      "Current Time = 00:40:04\n",
      "step 4030: train loss 0.1205, val loss 12.7428\n",
      "Current Time = 00:42:28\n",
      "step 4035: train loss 0.1201, val loss 12.8147\n",
      "Current Time = 00:44:53\n",
      "step 4040: train loss 0.1202, val loss 12.7634\n",
      "Current Time = 00:47:17\n",
      "step 4045: train loss 0.1202, val loss 12.7253\n",
      "Current Time = 00:49:41\n",
      "step 4050: train loss 0.1214, val loss 12.8123\n",
      "Current Time = 00:52:09\n",
      "step 4055: train loss 0.1202, val loss 12.7431\n",
      "Current Time = 00:54:34\n",
      "step 4060: train loss 0.1208, val loss 12.7953\n",
      "Current Time = 00:57:01\n",
      "step 4065: train loss 0.1201, val loss 12.7892\n",
      "Current Time = 00:59:25\n",
      "step 4070: train loss 0.1209, val loss 12.8006\n",
      "Current Time = 01:01:52\n",
      "step 4075: train loss 0.1204, val loss 12.7874\n",
      "Current Time = 01:04:18\n",
      "step 4080: train loss 0.1202, val loss 12.7938\n",
      "Current Time = 01:06:45\n",
      "step 4085: train loss 0.1193, val loss 12.7300\n",
      "Current Time = 01:09:12\n",
      "step 4090: train loss 0.1187, val loss 12.8322\n",
      "Current Time = 01:11:38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4095: train loss 0.1188, val loss 12.8323\n",
      "Current Time = 01:14:05\n",
      "step 4100: train loss 0.1188, val loss 12.8375\n",
      "Current Time = 01:16:32\n",
      "step 4105: train loss 0.1193, val loss 12.7884\n",
      "Current Time = 01:18:58\n",
      "step 4110: train loss 0.1179, val loss 12.8705\n",
      "Current Time = 01:21:25\n",
      "step 4115: train loss 0.1179, val loss 12.7625\n",
      "Current Time = 01:23:50\n",
      "step 4120: train loss 0.1192, val loss 12.7647\n",
      "Current Time = 01:26:17\n",
      "step 4125: train loss 0.1186, val loss 12.8456\n",
      "Current Time = 01:28:42\n",
      "step 4130: train loss 0.1204, val loss 12.7741\n",
      "Current Time = 01:31:09\n",
      "step 4135: train loss 0.1172, val loss 12.8141\n",
      "Current Time = 01:33:34\n",
      "step 4140: train loss 0.1191, val loss 12.7863\n",
      "Current Time = 01:35:57\n",
      "step 4145: train loss 0.1188, val loss 12.8245\n",
      "Current Time = 01:38:21\n",
      "step 4150: train loss 0.1185, val loss 12.8662\n",
      "Current Time = 01:40:45\n",
      "step 4155: train loss 0.1201, val loss 12.7764\n",
      "Current Time = 01:43:11\n",
      "step 4160: train loss 0.1191, val loss 12.8306\n",
      "Current Time = 01:45:35\n",
      "step 4165: train loss 0.1200, val loss 12.8523\n",
      "Current Time = 01:48:00\n",
      "step 4170: train loss 0.1189, val loss 12.8757\n",
      "Current Time = 01:50:26\n",
      "step 4175: train loss 0.1181, val loss 12.8160\n",
      "Current Time = 01:52:49\n",
      "step 4180: train loss 0.1174, val loss 12.8855\n",
      "Current Time = 01:55:14\n",
      "step 4185: train loss 0.1176, val loss 12.8751\n",
      "Current Time = 01:57:39\n",
      "step 4190: train loss 0.1168, val loss 12.8486\n",
      "Current Time = 02:00:05\n",
      "step 4195: train loss 0.1177, val loss 12.8423\n",
      "Current Time = 02:02:30\n",
      "step 4200: train loss 0.1178, val loss 12.8913\n",
      "Current Time = 02:04:55\n",
      "step 4205: train loss 0.1172, val loss 12.8830\n",
      "Current Time = 02:07:20\n",
      "step 4210: train loss 0.1182, val loss 12.8873\n",
      "Current Time = 02:09:46\n",
      "step 4215: train loss 0.1178, val loss 12.8139\n",
      "Current Time = 02:12:11\n",
      "step 4220: train loss 0.1175, val loss 12.9251\n",
      "Current Time = 02:14:37\n",
      "step 4225: train loss 0.1181, val loss 12.8831\n",
      "Current Time = 02:17:01\n",
      "step 4230: train loss 0.1174, val loss 12.8239\n",
      "Current Time = 02:19:26\n",
      "step 4235: train loss 0.1174, val loss 12.8528\n",
      "Current Time = 02:21:52\n",
      "step 4240: train loss 0.1160, val loss 12.8453\n",
      "Current Time = 02:24:15\n",
      "step 4245: train loss 0.1181, val loss 12.8483\n",
      "Current Time = 02:26:40\n",
      "step 4250: train loss 0.1164, val loss 12.9554\n",
      "Current Time = 02:29:05\n",
      "step 4255: train loss 0.1167, val loss 12.8701\n",
      "Current Time = 02:31:29\n",
      "step 4260: train loss 0.1158, val loss 12.9766\n",
      "Current Time = 02:33:53\n",
      "step 4265: train loss 0.1163, val loss 12.8988\n",
      "Current Time = 02:36:17\n",
      "step 4270: train loss 0.1157, val loss 12.8992\n",
      "Current Time = 02:38:40\n",
      "step 4275: train loss 0.1160, val loss 12.8901\n",
      "Current Time = 02:41:07\n",
      "step 4280: train loss 0.1170, val loss 12.8520\n",
      "Current Time = 02:43:30\n",
      "step 4285: train loss 0.1160, val loss 12.9400\n",
      "Current Time = 02:45:55\n",
      "step 4290: train loss 0.1160, val loss 12.8579\n",
      "Current Time = 02:48:19\n",
      "step 4295: train loss 0.1170, val loss 12.8804\n",
      "Current Time = 02:50:44\n",
      "step 4300: train loss 0.1162, val loss 12.9377\n",
      "Current Time = 02:53:09\n",
      "step 4305: train loss 0.1165, val loss 12.9476\n",
      "Current Time = 02:55:34\n",
      "step 4310: train loss 0.1177, val loss 12.9057\n",
      "Current Time = 02:57:58\n",
      "step 4315: train loss 0.1150, val loss 12.9756\n",
      "Current Time = 03:00:24\n",
      "step 4320: train loss 0.1169, val loss 12.8858\n",
      "Current Time = 03:02:48\n",
      "step 4325: train loss 0.1153, val loss 12.9624\n",
      "Current Time = 03:05:15\n",
      "step 4330: train loss 0.1162, val loss 12.8713\n",
      "Current Time = 03:07:40\n",
      "step 4335: train loss 0.1151, val loss 13.0320\n",
      "Current Time = 03:10:06\n",
      "step 4340: train loss 0.1147, val loss 12.9175\n",
      "Current Time = 03:12:32\n",
      "step 4345: train loss 0.1139, val loss 13.0187\n",
      "Current Time = 03:14:57\n",
      "step 4350: train loss 0.1163, val loss 12.9068\n",
      "Current Time = 03:17:23\n",
      "step 4355: train loss 0.1154, val loss 12.9620\n",
      "Current Time = 03:19:48\n",
      "step 4360: train loss 0.1149, val loss 12.9570\n",
      "Current Time = 03:22:13\n",
      "step 4365: train loss 0.1159, val loss 12.9670\n",
      "Current Time = 03:24:39\n",
      "step 4370: train loss 0.1147, val loss 12.9288\n",
      "Current Time = 03:27:05\n",
      "step 4375: train loss 0.1154, val loss 12.9174\n",
      "Current Time = 03:29:29\n",
      "step 4380: train loss 0.1144, val loss 12.9708\n",
      "Current Time = 03:31:53\n",
      "step 4385: train loss 0.1154, val loss 12.9487\n",
      "Current Time = 03:34:17\n",
      "step 4390: train loss 0.1136, val loss 12.9848\n",
      "Current Time = 03:36:42\n",
      "step 4395: train loss 0.1138, val loss 13.0086\n",
      "Current Time = 03:39:08\n",
      "step 4400: train loss 0.1144, val loss 13.0010\n",
      "Current Time = 03:41:33\n",
      "step 4405: train loss 0.1152, val loss 13.0300\n",
      "Current Time = 03:43:58\n",
      "step 4410: train loss 0.1151, val loss 13.0355\n",
      "Current Time = 03:46:22\n",
      "step 4415: train loss 0.1140, val loss 13.0277\n",
      "Current Time = 03:48:47\n",
      "step 4420: train loss 0.1124, val loss 12.9936\n",
      "Current Time = 03:51:12\n",
      "step 4425: train loss 0.1137, val loss 13.0343\n",
      "Current Time = 03:53:36\n",
      "step 4430: train loss 0.1140, val loss 12.9558\n",
      "Current Time = 03:56:00\n",
      "step 4435: train loss 0.1139, val loss 12.9735\n",
      "Current Time = 03:58:26\n",
      "step 4440: train loss 0.1135, val loss 12.9662\n",
      "Current Time = 04:00:49\n",
      "step 4445: train loss 0.1123, val loss 13.0416\n",
      "Current Time = 04:03:14\n",
      "step 4450: train loss 0.1138, val loss 12.9658\n",
      "Current Time = 04:05:40\n",
      "step 4455: train loss 0.1132, val loss 12.9617\n",
      "Current Time = 04:08:06\n",
      "step 4460: train loss 0.1133, val loss 13.0265\n",
      "Current Time = 04:10:33\n",
      "step 4465: train loss 0.1135, val loss 13.0065\n",
      "Current Time = 04:12:58\n",
      "step 4470: train loss 0.1129, val loss 13.0780\n",
      "Current Time = 04:15:24\n",
      "step 4475: train loss 0.1133, val loss 13.0378\n",
      "Current Time = 04:17:51\n",
      "step 4480: train loss 0.1135, val loss 13.0133\n",
      "Current Time = 04:20:18\n",
      "step 4485: train loss 0.1130, val loss 13.0461\n",
      "Current Time = 04:22:46\n",
      "step 4490: train loss 0.1135, val loss 13.0677\n",
      "Current Time = 04:25:14\n",
      "step 4495: train loss 0.1127, val loss 13.0727\n",
      "Current Time = 04:27:41\n",
      "step 4500: train loss 0.1135, val loss 13.0151\n",
      "Current Time = 04:30:09\n",
      "step 4505: train loss 0.1130, val loss 13.0641\n",
      "Current Time = 04:32:36\n",
      "step 4510: train loss 0.1132, val loss 12.9945\n",
      "Current Time = 04:35:02\n",
      "step 4515: train loss 0.1143, val loss 13.0329\n",
      "Current Time = 04:37:29\n",
      "step 4520: train loss 0.1130, val loss 13.0394\n",
      "Current Time = 04:39:57\n",
      "step 4525: train loss 0.1111, val loss 13.0897\n",
      "Current Time = 04:42:24\n",
      "step 4530: train loss 0.1129, val loss 13.0714\n",
      "Current Time = 04:44:52\n",
      "step 4535: train loss 0.1109, val loss 13.0691\n",
      "Current Time = 04:47:17\n",
      "step 4540: train loss 0.1128, val loss 13.0890\n",
      "Current Time = 04:49:43\n",
      "step 4545: train loss 0.1131, val loss 13.0440\n",
      "Current Time = 04:52:07\n",
      "step 4550: train loss 0.1117, val loss 13.1019\n",
      "Current Time = 04:54:32\n",
      "step 4555: train loss 0.1124, val loss 13.0284\n",
      "Current Time = 04:56:56\n",
      "step 4560: train loss 0.1118, val loss 13.0945\n",
      "Current Time = 04:59:21\n",
      "step 4565: train loss 0.1124, val loss 13.0813\n",
      "Current Time = 05:01:47\n",
      "step 4570: train loss 0.1116, val loss 13.0936\n",
      "Current Time = 05:04:13\n",
      "step 4575: train loss 0.1112, val loss 13.1624\n",
      "Current Time = 05:06:39\n",
      "step 4580: train loss 0.1111, val loss 13.0410\n",
      "Current Time = 05:09:04\n",
      "step 4585: train loss 0.1123, val loss 13.1102\n",
      "Current Time = 05:11:31\n",
      "step 4590: train loss 0.1119, val loss 13.0998\n",
      "Current Time = 05:13:56\n",
      "step 4595: train loss 0.1120, val loss 13.0613\n",
      "Current Time = 05:16:21\n",
      "step 4600: train loss 0.1120, val loss 13.1134\n",
      "Current Time = 05:18:47\n",
      "step 4605: train loss 0.1123, val loss 13.0601\n",
      "Current Time = 05:21:13\n",
      "step 4610: train loss 0.1110, val loss 13.1276\n",
      "Current Time = 05:23:39\n",
      "step 4615: train loss 0.1112, val loss 13.1424\n",
      "Current Time = 05:26:03\n",
      "step 4620: train loss 0.1118, val loss 13.0707\n",
      "Current Time = 05:28:29\n",
      "step 4625: train loss 0.1116, val loss 13.0196\n",
      "Current Time = 05:30:54\n",
      "step 4630: train loss 0.1110, val loss 13.1163\n",
      "Current Time = 05:33:21\n",
      "step 4635: train loss 0.1120, val loss 13.0418\n",
      "Current Time = 05:35:47\n",
      "step 4640: train loss 0.1111, val loss 13.0409\n",
      "Current Time = 05:38:12\n",
      "step 4645: train loss 0.1091, val loss 13.0854\n",
      "Current Time = 05:40:39\n",
      "step 4650: train loss 0.1113, val loss 13.0089\n",
      "Current Time = 05:43:03\n",
      "step 4655: train loss 0.1103, val loss 13.0701\n",
      "Current Time = 05:45:29\n",
      "step 4660: train loss 0.1103, val loss 13.1167\n",
      "Current Time = 05:47:56\n",
      "step 4665: train loss 0.1109, val loss 13.0562\n",
      "Current Time = 05:50:20\n",
      "step 4670: train loss 0.1106, val loss 13.1503\n",
      "Current Time = 05:52:44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4675: train loss 0.1108, val loss 13.0774\n",
      "Current Time = 05:55:11\n",
      "step 4680: train loss 0.1104, val loss 13.1858\n",
      "Current Time = 05:57:37\n",
      "step 4685: train loss 0.1105, val loss 13.1142\n",
      "Current Time = 06:00:03\n",
      "step 4690: train loss 0.1096, val loss 13.1672\n",
      "Current Time = 06:02:29\n",
      "step 4695: train loss 0.1112, val loss 13.0749\n",
      "Current Time = 06:04:53\n",
      "step 4700: train loss 0.1115, val loss 13.1124\n",
      "Current Time = 06:07:19\n",
      "step 4705: train loss 0.1107, val loss 13.1220\n",
      "Current Time = 06:09:45\n",
      "step 4710: train loss 0.1106, val loss 13.1293\n",
      "Current Time = 06:12:10\n",
      "step 4715: train loss 0.1116, val loss 13.0927\n",
      "Current Time = 06:14:36\n",
      "step 4720: train loss 0.1092, val loss 13.1060\n",
      "Current Time = 06:17:02\n",
      "step 4725: train loss 0.1108, val loss 13.0930\n",
      "Current Time = 06:19:26\n",
      "step 4730: train loss 0.1099, val loss 13.1424\n",
      "Current Time = 06:21:51\n",
      "step 4735: train loss 0.1094, val loss 13.1162\n",
      "Current Time = 06:24:15\n",
      "step 4740: train loss 0.1102, val loss 13.1058\n",
      "Current Time = 06:26:40\n",
      "step 4745: train loss 0.1094, val loss 13.1564\n",
      "Current Time = 06:29:06\n",
      "step 4750: train loss 0.1092, val loss 13.1385\n",
      "Current Time = 06:31:33\n",
      "step 4755: train loss 0.1094, val loss 13.1447\n",
      "Current Time = 06:33:58\n",
      "step 4760: train loss 0.1096, val loss 13.1660\n",
      "Current Time = 06:36:23\n",
      "step 4765: train loss 0.1098, val loss 13.1472\n",
      "Current Time = 06:38:47\n",
      "step 4770: train loss 0.1093, val loss 13.2120\n",
      "Current Time = 06:41:13\n",
      "step 4775: train loss 0.1104, val loss 13.1090\n",
      "Current Time = 06:43:38\n",
      "step 4780: train loss 0.1094, val loss 13.0758\n",
      "Current Time = 06:46:03\n",
      "step 4785: train loss 0.1087, val loss 13.2493\n",
      "Current Time = 06:48:29\n",
      "step 4790: train loss 0.1095, val loss 13.1347\n",
      "Current Time = 06:50:54\n",
      "step 4795: train loss 0.1095, val loss 13.1486\n",
      "Current Time = 06:53:19\n",
      "step 4800: train loss 0.1078, val loss 13.2347\n",
      "Current Time = 06:55:47\n",
      "step 4805: train loss 0.1097, val loss 13.2001\n",
      "Current Time = 06:58:11\n",
      "step 4810: train loss 0.1091, val loss 13.2132\n",
      "Current Time = 07:00:37\n",
      "step 4815: train loss 0.1083, val loss 13.2140\n",
      "Current Time = 07:03:03\n",
      "step 4820: train loss 0.1083, val loss 13.2250\n",
      "Current Time = 07:05:28\n",
      "step 4825: train loss 0.1082, val loss 13.1888\n",
      "Current Time = 07:07:55\n",
      "step 4830: train loss 0.1084, val loss 13.1756\n",
      "Current Time = 07:10:22\n",
      "step 4835: train loss 0.1087, val loss 13.2119\n",
      "Current Time = 07:12:47\n",
      "step 4840: train loss 0.1089, val loss 13.1733\n",
      "Current Time = 07:15:15\n",
      "step 4845: train loss 0.1079, val loss 13.2136\n",
      "Current Time = 07:17:41\n",
      "step 4850: train loss 0.1074, val loss 13.2512\n",
      "Current Time = 07:20:08\n",
      "step 4855: train loss 0.1077, val loss 13.2023\n",
      "Current Time = 07:22:34\n",
      "step 4860: train loss 0.1076, val loss 13.2619\n",
      "Current Time = 07:25:00\n",
      "step 4865: train loss 0.1079, val loss 13.1676\n",
      "Current Time = 07:27:27\n",
      "step 4870: train loss 0.1088, val loss 13.1515\n",
      "Current Time = 07:29:53\n",
      "step 4875: train loss 0.1087, val loss 13.2500\n",
      "Current Time = 07:32:19\n",
      "step 4880: train loss 0.1076, val loss 13.1892\n",
      "Current Time = 07:34:47\n",
      "step 4885: train loss 0.1070, val loss 13.2184\n",
      "Current Time = 07:37:13\n",
      "step 4890: train loss 0.1080, val loss 13.2065\n",
      "Current Time = 07:39:39\n",
      "step 4895: train loss 0.1076, val loss 13.2480\n",
      "Current Time = 07:42:06\n",
      "step 4900: train loss 0.1073, val loss 13.2753\n",
      "Current Time = 07:44:32\n",
      "step 4905: train loss 0.1070, val loss 13.2698\n",
      "Current Time = 07:46:57\n",
      "step 4910: train loss 0.1076, val loss 13.2199\n",
      "Current Time = 07:49:22\n",
      "step 4915: train loss 0.1071, val loss 13.2382\n",
      "Current Time = 07:51:48\n",
      "step 4920: train loss 0.1069, val loss 13.2718\n",
      "Current Time = 07:54:14\n",
      "step 4925: train loss 0.1080, val loss 13.2047\n",
      "Current Time = 07:56:40\n",
      "step 4930: train loss 0.1074, val loss 13.2572\n",
      "Current Time = 07:59:06\n",
      "step 4935: train loss 0.1079, val loss 13.2597\n",
      "Current Time = 08:01:30\n",
      "step 4940: train loss 0.1082, val loss 13.2039\n",
      "Current Time = 08:03:56\n",
      "step 4945: train loss 0.1071, val loss 13.2614\n",
      "Current Time = 08:06:22\n",
      "step 4950: train loss 0.1071, val loss 13.2081\n",
      "Current Time = 08:08:48\n",
      "step 4955: train loss 0.1066, val loss 13.1957\n",
      "Current Time = 08:11:14\n",
      "step 4960: train loss 0.1076, val loss 13.2351\n",
      "Current Time = 08:13:41\n",
      "step 4965: train loss 0.1064, val loss 13.2708\n",
      "Current Time = 08:16:06\n",
      "step 4970: train loss 0.1067, val loss 13.2326\n",
      "Current Time = 08:18:33\n",
      "step 4975: train loss 0.1073, val loss 13.2328\n",
      "Current Time = 08:20:59\n",
      "step 4980: train loss 0.1064, val loss 13.2564\n",
      "Current Time = 08:23:23\n",
      "step 4985: train loss 0.1066, val loss 13.2531\n",
      "Current Time = 08:25:49\n",
      "step 4990: train loss 0.1074, val loss 13.2563\n",
      "Current Time = 08:28:13\n",
      "step 4995: train loss 0.1066, val loss 13.3055\n",
      "Current Time = 08:30:39\n",
      "step 4999: train loss 0.1053, val loss 13.2643\n",
      "Current Time = 08:33:01\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S\")\n",
    "        print(\"Current Time =\", current_time)\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "torch.save(model.state_dict(), \"Next Word Predictor_EPOCH400_v31.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a3423e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('token_embedding_table.weight',\n",
       "              tensor([[ 0.0017, -0.0828, -0.0049,  ...,  0.0091, -0.0147, -0.0039],\n",
       "                      [ 0.0139,  0.0362, -0.0046,  ..., -0.0213,  0.0186, -0.0461],\n",
       "                      [ 0.0651,  0.0144,  0.0237,  ..., -0.0483, -0.0168, -0.0025],\n",
       "                      ...,\n",
       "                      [ 0.0082, -0.0229,  0.0010,  ...,  0.0071, -0.0334, -0.0250],\n",
       "                      [ 0.0495,  0.0679,  0.0529,  ...,  0.0258, -0.0249,  0.0179],\n",
       "                      [ 0.0362,  0.0431,  0.0139,  ...,  0.0130, -0.0115, -0.0587]],\n",
       "                     device='cuda:0')),\n",
       "             ('position_embedding_table.weight',\n",
       "              tensor([[-9.1582e-02, -2.1402e-01,  1.7561e-02,  ...,  4.4659e-02,\n",
       "                       -7.2588e-02,  6.5580e-03],\n",
       "                      [-1.7427e-03, -5.6298e-02, -9.3286e-03,  ...,  1.4173e-02,\n",
       "                        9.9526e-03, -2.2528e-02],\n",
       "                      [ 3.7378e-03,  1.1351e-02,  1.4113e-02,  ...,  1.0865e-02,\n",
       "                        1.9216e-02, -8.7103e-03],\n",
       "                      ...,\n",
       "                      [-6.4180e-03,  1.8669e-02,  1.1354e-02,  ..., -5.2636e-03,\n",
       "                       -7.0237e-03, -3.1749e-03],\n",
       "                      [-3.4594e-03,  1.9548e-02,  1.3494e-02,  ..., -9.1002e-03,\n",
       "                       -8.2720e-03, -4.4021e-03],\n",
       "                      [-4.3692e-03,  1.0031e-02,  2.3211e-03,  ..., -6.4442e-03,\n",
       "                       -1.9664e-04, -6.9378e-03]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.0.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.0.key.weight',\n",
       "              tensor([[ 0.0206, -0.0155, -0.0214,  ..., -0.0052, -0.0073, -0.0180],\n",
       "                      [ 0.0170,  0.0670, -0.0232,  ..., -0.0151,  0.0256,  0.0762],\n",
       "                      [ 0.0368, -0.0459,  0.0326,  ..., -0.0719, -0.0101, -0.0586],\n",
       "                      ...,\n",
       "                      [ 0.0367, -0.0492, -0.0208,  ...,  0.0062, -0.0156,  0.0410],\n",
       "                      [-0.0162, -0.0221, -0.0181,  ..., -0.0489, -0.0418,  0.0501],\n",
       "                      [-0.0082, -0.0176, -0.0269,  ..., -0.0100,  0.0331,  0.0234]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.0.query.weight',\n",
       "              tensor([[ 0.0335,  0.0536, -0.0069,  ...,  0.0619, -0.0107,  0.0521],\n",
       "                      [-0.0737, -0.0276, -0.0142,  ..., -0.0274, -0.0050, -0.0927],\n",
       "                      [ 0.0506, -0.0108, -0.0138,  ..., -0.0555,  0.0613, -0.0314],\n",
       "                      ...,\n",
       "                      [-0.0046,  0.0035, -0.0075,  ...,  0.0676,  0.0023, -0.0703],\n",
       "                      [ 0.0560, -0.0102,  0.0065,  ...,  0.0741,  0.0923, -0.0179],\n",
       "                      [ 0.0036,  0.0769,  0.0233,  ..., -0.0025, -0.0005,  0.0202]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.0.value.weight',\n",
       "              tensor([[-0.0245,  0.0020,  0.0262,  ...,  0.0084, -0.0003, -0.0061],\n",
       "                      [-0.0094, -0.0048, -0.0013,  ...,  0.0018,  0.0093,  0.0137],\n",
       "                      [ 0.0121,  0.0024, -0.0183,  ...,  0.0130, -0.0094,  0.0404],\n",
       "                      ...,\n",
       "                      [-0.0266,  0.0114,  0.0077,  ..., -0.0039, -0.0051,  0.0059],\n",
       "                      [ 0.0394,  0.0389,  0.0036,  ...,  0.0072,  0.0057, -0.0112],\n",
       "                      [-0.0281,  0.0123, -0.0167,  ...,  0.0009,  0.0067, -0.0120]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.1.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.1.key.weight',\n",
       "              tensor([[ 0.0022, -0.0374, -0.0378,  ...,  0.0133,  0.0009,  0.0074],\n",
       "                      [ 0.0569, -0.0640, -0.0722,  ...,  0.0196, -0.0454,  0.0164],\n",
       "                      [ 0.1088,  0.0204, -0.0151,  ..., -0.0642, -0.0306, -0.0343],\n",
       "                      ...,\n",
       "                      [-0.0056,  0.0158, -0.0062,  ...,  0.0560, -0.0052, -0.0175],\n",
       "                      [ 0.0172,  0.0077, -0.0374,  ...,  0.0412,  0.0086,  0.0353],\n",
       "                      [ 0.0539,  0.0285,  0.0049,  ...,  0.0395, -0.0544,  0.0594]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.1.query.weight',\n",
       "              tensor([[ 0.0115, -0.0945, -0.0480,  ..., -0.0207,  0.0790, -0.0884],\n",
       "                      [ 0.0745,  0.0870,  0.0580,  ...,  0.0062,  0.0125,  0.0335],\n",
       "                      [-0.0550,  0.0022,  0.0256,  ..., -0.0117,  0.0527,  0.0681],\n",
       "                      ...,\n",
       "                      [ 0.0862,  0.0235,  0.0649,  ...,  0.0041,  0.0016, -0.0837],\n",
       "                      [ 0.0495, -0.0525,  0.0158,  ..., -0.0671,  0.0044,  0.0713],\n",
       "                      [-0.0082, -0.1108, -0.0965,  ..., -0.0064, -0.0130, -0.0634]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.1.value.weight',\n",
       "              tensor([[ 0.0061,  0.0234, -0.0205,  ..., -0.0160,  0.0218,  0.0152],\n",
       "                      [ 0.0062,  0.0093, -0.0095,  ..., -0.0003, -0.0094,  0.0025],\n",
       "                      [ 0.0009,  0.0197, -0.0154,  ...,  0.0013,  0.0202, -0.0416],\n",
       "                      ...,\n",
       "                      [ 0.0006, -0.0168, -0.0203,  ...,  0.0364, -0.0141, -0.0409],\n",
       "                      [ 0.0244, -0.0036, -0.0214,  ..., -0.0140, -0.0247, -0.0144],\n",
       "                      [ 0.0100, -0.0077, -0.0141,  ..., -0.0104,  0.0092, -0.0089]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.2.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.2.key.weight',\n",
       "              tensor([[-0.0061,  0.0351, -0.0346,  ...,  0.0310, -0.0116,  0.0136],\n",
       "                      [ 0.0189,  0.0229, -0.0495,  ...,  0.0377, -0.0183, -0.0501],\n",
       "                      [-0.0180,  0.0175, -0.0575,  ...,  0.0342, -0.0558,  0.0587],\n",
       "                      ...,\n",
       "                      [ 0.0354,  0.0038, -0.0224,  ..., -0.0169,  0.0775, -0.0432],\n",
       "                      [-0.0498,  0.0528, -0.0489,  ..., -0.0951,  0.1198, -0.0182],\n",
       "                      [-0.0005, -0.0289,  0.0375,  ...,  0.0530, -0.0630, -0.0356]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.2.query.weight',\n",
       "              tensor([[-0.0454,  0.0168, -0.0799,  ...,  0.0146, -0.0350, -0.0368],\n",
       "                      [ 0.0019, -0.0312,  0.0781,  ..., -0.0343, -0.0301,  0.0218],\n",
       "                      [-0.0422, -0.0332,  0.0259,  ...,  0.0126, -0.0875,  0.0470],\n",
       "                      ...,\n",
       "                      [ 0.0004, -0.0469,  0.0020,  ...,  0.0452, -0.0005,  0.0187],\n",
       "                      [ 0.0079, -0.0330,  0.0352,  ..., -0.0011,  0.0670, -0.0303],\n",
       "                      [ 0.0291, -0.0017, -0.0180,  ..., -0.0245,  0.0297, -0.0439]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.2.value.weight',\n",
       "              tensor([[ 0.0296,  0.0008,  0.0405,  ...,  0.0152,  0.0090, -0.0044],\n",
       "                      [-0.0080, -0.0069,  0.0143,  ..., -0.0016,  0.0017,  0.0425],\n",
       "                      [ 0.0154,  0.0115, -0.0151,  ...,  0.0085,  0.0125,  0.0033],\n",
       "                      ...,\n",
       "                      [ 0.0026, -0.0008,  0.0054,  ...,  0.0250, -0.0039, -0.0085],\n",
       "                      [ 0.0240, -0.0131, -0.0141,  ...,  0.0003,  0.0046,  0.0251],\n",
       "                      [ 0.0176,  0.0083,  0.0029,  ..., -0.0015,  0.0133,  0.0112]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.3.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.3.key.weight',\n",
       "              tensor([[ 0.0846, -0.0445, -0.0015,  ..., -0.0339, -0.0783, -0.0447],\n",
       "                      [-0.0332,  0.0137,  0.0575,  ...,  0.0098, -0.0131,  0.0628],\n",
       "                      [ 0.0665, -0.0402, -0.0451,  ..., -0.0294,  0.0223,  0.0484],\n",
       "                      ...,\n",
       "                      [-0.0261, -0.0306, -0.0789,  ..., -0.0868, -0.0030, -0.0238],\n",
       "                      [-0.0229,  0.0227,  0.0395,  ...,  0.0029, -0.0309,  0.0313],\n",
       "                      [ 0.0060,  0.0248, -0.0218,  ...,  0.0434, -0.0047,  0.0168]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.3.query.weight',\n",
       "              tensor([[-0.0318,  0.0266,  0.0062,  ...,  0.0021, -0.0710,  0.0329],\n",
       "                      [ 0.0852,  0.0796, -0.0184,  ..., -0.1628, -0.0435, -0.0179],\n",
       "                      [-0.0498,  0.0484, -0.0200,  ...,  0.0631, -0.0137,  0.0358],\n",
       "                      ...,\n",
       "                      [-0.0348,  0.0252, -0.0642,  ..., -0.0309, -0.0373,  0.0336],\n",
       "                      [ 0.0426,  0.0088, -0.0394,  ..., -0.0421,  0.0625,  0.0872],\n",
       "                      [-0.0003, -0.0228, -0.0681,  ..., -0.0395, -0.0662, -0.0627]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.3.value.weight',\n",
       "              tensor([[-0.0256, -0.0224,  0.0049,  ..., -0.0334,  0.0092,  0.0232],\n",
       "                      [ 0.0201,  0.0337, -0.0242,  ..., -0.0017,  0.0036,  0.0188],\n",
       "                      [-0.0200,  0.0102,  0.0112,  ...,  0.0088, -0.0055, -0.0391],\n",
       "                      ...,\n",
       "                      [-0.0133,  0.0044,  0.0188,  ...,  0.0020,  0.0085,  0.0298],\n",
       "                      [-0.0254, -0.0082, -0.0162,  ..., -0.0035,  0.0129, -0.0122],\n",
       "                      [ 0.0214, -0.0052, -0.0046,  ...,  0.0129,  0.0224,  0.0140]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.4.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.4.key.weight',\n",
       "              tensor([[-0.0003, -0.0096, -0.0435,  ..., -0.0349,  0.0278,  0.0362],\n",
       "                      [-0.0123,  0.0659, -0.0441,  ...,  0.1106,  0.0349,  0.0338],\n",
       "                      [-0.0315,  0.0927,  0.0167,  ...,  0.0048, -0.0804, -0.0006],\n",
       "                      ...,\n",
       "                      [ 0.0063,  0.0299,  0.0440,  ..., -0.0342, -0.0085,  0.0031],\n",
       "                      [-0.0784,  0.0185,  0.0099,  ..., -0.0206,  0.0250, -0.0314],\n",
       "                      [-0.0372, -0.0854, -0.0008,  ..., -0.0284, -0.0020, -0.0359]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.4.query.weight',\n",
       "              tensor([[ 0.0362, -0.1155, -0.0236,  ..., -0.0859, -0.0708, -0.0609],\n",
       "                      [-0.0203, -0.0371,  0.0351,  ...,  0.0937,  0.0170,  0.0960],\n",
       "                      [ 0.0017, -0.0433,  0.1104,  ...,  0.0670,  0.0295, -0.0142],\n",
       "                      ...,\n",
       "                      [ 0.1031, -0.0513,  0.0863,  ..., -0.0954, -0.0045,  0.0469],\n",
       "                      [-0.0038, -0.0131,  0.0300,  ..., -0.0120,  0.0715, -0.0115],\n",
       "                      [-0.0762, -0.0484,  0.0099,  ...,  0.0322,  0.0156,  0.0737]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.4.value.weight',\n",
       "              tensor([[ 5.4929e-03, -7.3713e-03, -3.3679e-02,  ...,  1.0778e-02,\n",
       "                        1.4820e-02,  1.8114e-02],\n",
       "                      [-4.2390e-03, -1.9673e-02, -1.4025e-02,  ..., -3.0050e-02,\n",
       "                       -7.3081e-05,  6.9752e-03],\n",
       "                      [-1.4546e-02, -2.8654e-02, -3.9097e-03,  ...,  1.9641e-02,\n",
       "                        5.7440e-03,  2.5108e-03],\n",
       "                      ...,\n",
       "                      [ 1.7993e-02, -2.2208e-03, -2.3520e-04,  ...,  3.2262e-03,\n",
       "                       -1.6189e-02, -7.2897e-04],\n",
       "                      [-1.0303e-02, -3.3377e-02, -1.9891e-02,  ...,  2.1355e-02,\n",
       "                       -1.7474e-02,  6.1538e-03],\n",
       "                      [-3.8295e-03,  1.2452e-03, -2.5673e-03,  ..., -7.3037e-03,\n",
       "                       -1.1992e-02,  9.2847e-03]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.5.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.5.key.weight',\n",
       "              tensor([[ 0.0039, -0.0629, -0.0441,  ..., -0.0371,  0.0842,  0.0023],\n",
       "                      [ 0.0364,  0.0679, -0.0585,  ...,  0.0489, -0.0029,  0.0106],\n",
       "                      [ 0.0577, -0.0255, -0.0201,  ..., -0.0672,  0.0904, -0.1112],\n",
       "                      ...,\n",
       "                      [-0.0329, -0.0163, -0.0350,  ...,  0.0115, -0.0783,  0.0305],\n",
       "                      [-0.0670,  0.0666, -0.0205,  ...,  0.0628,  0.0006,  0.0327],\n",
       "                      [-0.0367, -0.0563, -0.0129,  ..., -0.0233, -0.0075, -0.0507]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.5.query.weight',\n",
       "              tensor([[-6.0539e-03, -1.6559e-01,  2.3034e-02,  ...,  3.6263e-02,\n",
       "                        2.3039e-02,  1.0077e-01],\n",
       "                      [-1.2898e-04,  2.2849e-03,  3.7685e-02,  ..., -6.6909e-02,\n",
       "                        1.5347e-02, -3.1439e-02],\n",
       "                      [-6.4477e-02, -1.7601e-02,  3.5808e-02,  ...,  6.1470e-02,\n",
       "                        2.6169e-02,  9.1536e-02],\n",
       "                      ...,\n",
       "                      [ 1.6744e-02, -5.4688e-02, -3.1047e-02,  ..., -5.2079e-02,\n",
       "                       -1.0828e-02, -2.5137e-02],\n",
       "                      [ 2.2282e-02,  8.5732e-02, -5.9528e-02,  ...,  5.7662e-02,\n",
       "                       -4.8827e-02, -2.6268e-02],\n",
       "                      [ 1.5636e-03,  1.6808e-02,  3.4333e-02,  ...,  1.0052e-03,\n",
       "                       -2.8844e-02, -3.0479e-02]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.5.value.weight',\n",
       "              tensor([[-0.0090,  0.0022, -0.0034,  ..., -0.0187, -0.0182,  0.0044],\n",
       "                      [-0.0125,  0.0018, -0.0002,  ...,  0.0283, -0.0295,  0.0081],\n",
       "                      [-0.0038, -0.0054, -0.0382,  ..., -0.0140,  0.0031, -0.0315],\n",
       "                      ...,\n",
       "                      [-0.0146,  0.0107,  0.0151,  ..., -0.0072,  0.0143,  0.0313],\n",
       "                      [-0.0139,  0.0051,  0.0034,  ...,  0.0076, -0.0203, -0.0070],\n",
       "                      [-0.0265, -0.0322, -0.0150,  ...,  0.0112,  0.0243, -0.0071]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.proj.weight',\n",
       "              tensor([[-0.0097,  0.0112,  0.0024,  ..., -0.0089,  0.0109,  0.0109],\n",
       "                      [-0.0021,  0.0076, -0.0103,  ..., -0.0130,  0.0179,  0.0023],\n",
       "                      [ 0.0137, -0.0080,  0.0065,  ...,  0.0072, -0.0049, -0.0033],\n",
       "                      ...,\n",
       "                      [ 0.0052,  0.0064,  0.0133,  ...,  0.0135, -0.0008, -0.0085],\n",
       "                      [-0.0064,  0.0020,  0.0159,  ..., -0.0131, -0.0010,  0.0012],\n",
       "                      [-0.0048,  0.0030, -0.0022,  ..., -0.0137,  0.0201, -0.0200]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.proj.bias',\n",
       "              tensor([ 1.0803e-03,  1.0548e-03,  3.7357e-04,  1.4668e-03, -1.1606e-03,\n",
       "                       5.8491e-03, -2.6637e-03, -4.4314e-03, -2.9048e-03, -5.2631e-04,\n",
       "                       1.0615e-04,  1.4118e-03, -2.4547e-03, -9.6997e-04,  2.3704e-03,\n",
       "                      -3.5465e-03, -3.3994e-03,  1.3053e-03,  2.0143e-03,  9.9425e-04,\n",
       "                      -4.3223e-04, -1.5631e-03, -3.5222e-04,  3.0236e-04,  4.4194e-04,\n",
       "                       9.3791e-04, -1.4193e-03, -3.1869e-03,  2.4268e-03,  1.0898e-03,\n",
       "                       2.1153e-03, -1.2191e-03, -9.4187e-04,  2.7496e-03, -2.5910e-03,\n",
       "                       3.1756e-04, -1.7102e-03, -3.3427e-03, -2.6311e-03,  7.6890e-04,\n",
       "                       1.6007e-03,  9.0997e-04,  4.1408e-03, -2.9104e-03,  3.1965e-04,\n",
       "                      -2.8548e-04, -2.8737e-03,  5.5041e-04,  2.0356e-03,  2.4023e-03,\n",
       "                      -2.0545e-03, -1.8946e-03, -1.0825e-04,  2.7747e-03, -4.3356e-04,\n",
       "                       1.5171e-03,  1.5331e-03, -2.0723e-03, -1.0774e-03,  8.4618e-05,\n",
       "                       8.8118e-04, -1.2807e-03, -1.4970e-04,  1.5128e-03, -1.6264e-03,\n",
       "                       1.1445e-03,  1.0230e-03, -1.0785e-03,  2.3433e-03, -5.0543e-03,\n",
       "                      -1.6130e-03, -3.7111e-03,  4.8983e-04,  1.5857e-03,  7.9827e-04,\n",
       "                       2.2199e-03,  1.6670e-03,  4.0369e-04,  1.8256e-04,  2.3027e-03,\n",
       "                      -1.3791e-03,  3.8639e-03,  5.1881e-03, -5.8646e-04, -6.6626e-04,\n",
       "                       1.6080e-03,  5.4631e-04, -5.3936e-03,  6.2527e-04,  1.2639e-03,\n",
       "                       1.8705e-03, -9.3512e-04,  2.0698e-03,  2.5833e-03,  4.2010e-04,\n",
       "                       9.1055e-03, -5.7005e-04,  4.0548e-04,  5.5229e-04, -1.7266e-03,\n",
       "                      -4.0422e-03,  3.7253e-04,  2.0857e-03, -2.4092e-03, -1.4709e-04,\n",
       "                       2.3796e-03,  6.0342e-04, -3.0139e-03,  3.1190e-03,  1.2489e-04,\n",
       "                      -1.0041e-03,  3.6763e-04,  6.2191e-05, -4.1932e-03, -2.3069e-03,\n",
       "                      -1.3471e-03, -3.4183e-03, -3.1451e-03,  7.5146e-04,  2.6105e-03,\n",
       "                       2.3508e-03,  2.7761e-04, -2.8298e-03,  5.4544e-04, -5.6908e-03,\n",
       "                      -1.1552e-03,  1.8796e-03, -1.9296e-03,  5.5595e-04,  3.5837e-04,\n",
       "                      -6.9225e-04, -5.7768e-04, -1.1943e-03,  3.7412e-03, -8.6579e-04,\n",
       "                      -3.7682e-04,  1.2469e-03,  2.5305e-04,  1.1907e-04,  1.3004e-03,\n",
       "                      -3.8669e-03,  2.7419e-03,  2.1214e-03,  9.0981e-04,  1.3042e-03,\n",
       "                      -1.2371e-03, -2.0425e-03,  1.1742e-03, -1.0100e-05, -3.2210e-03,\n",
       "                       1.3399e-03,  4.7304e-03, -1.5044e-03, -1.5111e-03, -1.8660e-03,\n",
       "                       6.2869e-04,  2.8770e-03,  4.8528e-04,  3.5470e-04,  4.6312e-03,\n",
       "                       3.8298e-03, -1.1090e-03, -3.1865e-03, -1.7357e-03, -2.3497e-03,\n",
       "                       6.2495e-04, -1.0304e-03,  8.8052e-04,  4.3943e-04,  4.7998e-04,\n",
       "                       3.1990e-04, -8.2799e-04,  2.0558e-03,  3.1467e-03, -1.3177e-03,\n",
       "                       1.2287e-03,  5.5396e-03, -1.3309e-03, -3.9069e-05, -4.7625e-03,\n",
       "                       3.0153e-03,  2.2478e-03,  2.5637e-03, -1.7839e-03, -2.4328e-04,\n",
       "                       2.4692e-03,  1.4724e-03, -8.3148e-04, -1.3173e-04,  2.3777e-04,\n",
       "                       5.8792e-04,  1.7703e-03,  2.9952e-03,  4.1324e-03,  3.6937e-03,\n",
       "                       2.5925e-04, -5.6563e-03, -7.6414e-04, -1.5776e-03, -1.0129e-03,\n",
       "                      -1.1763e-03, -1.1389e-03,  7.5761e-04,  8.5435e-04,  2.6842e-03,\n",
       "                      -4.8193e-03, -1.1852e-03, -4.0713e-03,  2.8493e-03, -1.2953e-03,\n",
       "                       1.7478e-03,  2.9381e-03, -3.0393e-03, -2.7467e-03, -3.1482e-03,\n",
       "                      -2.1121e-03, -7.2820e-05, -1.5816e-03, -4.4179e-03, -2.1285e-03,\n",
       "                      -1.7857e-03, -5.1656e-03, -6.7753e-04, -1.9987e-03,  2.8079e-03,\n",
       "                      -2.0248e-03, -1.4167e-03, -5.4778e-03,  6.7548e-04,  3.1778e-03,\n",
       "                      -2.4321e-03,  2.0453e-03, -6.6364e-04,  3.8337e-03,  1.5088e-03,\n",
       "                       2.2529e-03,  2.9878e-03,  9.2923e-04,  8.2850e-04, -4.7791e-04,\n",
       "                      -2.5945e-03, -7.1992e-04,  2.5746e-03,  7.2249e-04, -7.1834e-03,\n",
       "                      -4.4929e-05,  4.5542e-03,  5.7358e-04, -4.3686e-04,  1.6623e-03,\n",
       "                      -1.3617e-03, -1.0123e-04,  6.4620e-04,  3.8029e-04,  5.2846e-04,\n",
       "                      -2.2530e-04,  3.3964e-03, -1.9440e-04,  1.5854e-03,  1.1725e-03,\n",
       "                       3.8513e-05,  3.8174e-04, -4.0167e-04,  3.5392e-04, -1.2283e-03,\n",
       "                      -3.2200e-04, -3.8849e-03,  3.8944e-04,  3.1538e-03,  5.0239e-05,\n",
       "                       5.9274e-04,  2.1310e-03,  3.6126e-05, -8.1346e-04, -1.8529e-03,\n",
       "                       1.2216e-05, -9.6129e-04, -7.8742e-04,  9.3869e-04,  3.3869e-03,\n",
       "                      -1.9558e-03, -2.6898e-03, -2.4073e-03,  6.5241e-04,  8.5969e-04,\n",
       "                      -3.7994e-04, -2.0281e-04,  1.9699e-03,  8.1378e-04,  1.6258e-03,\n",
       "                       1.0893e-03,  1.6585e-03,  3.2445e-03,  4.0702e-03, -3.0996e-04,\n",
       "                      -1.9556e-03,  1.3359e-03, -1.4425e-03,  2.4282e-03,  7.1551e-04,\n",
       "                      -1.0058e-03, -1.5391e-04, -3.3906e-03, -3.8795e-03,  5.0510e-04,\n",
       "                      -5.3115e-03, -3.3954e-03,  1.9854e-03, -9.0189e-04,  2.2105e-03,\n",
       "                       1.2225e-04,  1.0762e-03,  5.7119e-04,  7.9498e-04,  2.9517e-03,\n",
       "                       5.3837e-04, -7.7919e-04, -3.2390e-03, -1.9013e-03,  3.0306e-03,\n",
       "                      -6.5565e-04,  1.3466e-03, -3.0111e-03,  3.4110e-04, -1.9111e-03,\n",
       "                       5.2325e-04,  1.5180e-03,  1.8688e-03, -1.6036e-04,  8.7966e-04,\n",
       "                       6.5263e-05, -2.0278e-03,  4.3223e-03, -1.7805e-03,  2.1034e-04,\n",
       "                      -3.9894e-03, -3.5140e-04,  1.5418e-03, -3.5402e-03,  8.4369e-04,\n",
       "                       2.5973e-03,  3.1552e-03,  2.0101e-04,  3.7639e-03, -2.3306e-03,\n",
       "                       2.0409e-05, -7.1896e-03, -1.0608e-03, -1.1144e-03, -1.7638e-04,\n",
       "                      -8.0135e-04,  8.1223e-06, -3.6556e-05,  1.4180e-03, -2.4276e-03,\n",
       "                      -3.6490e-03, -2.0774e-03, -1.4243e-03, -5.1161e-04,  2.6233e-03,\n",
       "                      -1.4935e-03, -9.7739e-05,  1.0467e-03,  3.7570e-03,  3.9029e-03,\n",
       "                      -2.7566e-03,  7.7716e-04, -3.2617e-03,  9.0074e-04, -1.1340e-03,\n",
       "                       6.3475e-04, -7.6536e-04,  1.4632e-03, -1.1099e-04, -6.4405e-04,\n",
       "                       2.0046e-03, -4.0889e-03, -2.2581e-03,  2.4026e-04,  1.6623e-03,\n",
       "                       6.6275e-04, -6.0361e-04,  1.1912e-03,  2.4357e-03], device='cuda:0')),\n",
       "             ('blocks.0.ffwd.net.0.weight',\n",
       "              tensor([[-0.0235, -0.0087,  0.0407,  ..., -0.0347, -0.0210, -0.0400],\n",
       "                      [-0.0044, -0.0045, -0.0531,  ...,  0.0004,  0.0694,  0.0129],\n",
       "                      [-0.0280,  0.0221, -0.0369,  ...,  0.0248,  0.0109, -0.0066],\n",
       "                      ...,\n",
       "                      [ 0.0112, -0.0030,  0.0216,  ...,  0.0184,  0.0256,  0.0121],\n",
       "                      [-0.0529,  0.0324, -0.0410,  ..., -0.0080, -0.0331,  0.0314],\n",
       "                      [-0.0188,  0.0141, -0.0155,  ..., -0.0223, -0.0443,  0.0100]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.ffwd.net.0.bias',\n",
       "              tensor([-0.0070, -0.0181, -0.0261,  ..., -0.0239, -0.0166, -0.0234],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.ffwd.net.2.weight',\n",
       "              tensor([[ 0.0483, -0.0439,  0.0172,  ..., -0.0107,  0.0390,  0.0015],\n",
       "                      [ 0.0032, -0.0174,  0.0085,  ...,  0.0368, -0.0045, -0.0767],\n",
       "                      [-0.0010, -0.0257, -0.0167,  ..., -0.0103,  0.0529, -0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0125,  0.0340, -0.0015,  ..., -0.0248,  0.0023, -0.0173],\n",
       "                      [-0.0162, -0.0024, -0.0068,  ..., -0.0628, -0.0104,  0.0046],\n",
       "                      [ 0.0288,  0.0254, -0.0010,  ...,  0.0409, -0.0109,  0.0240]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.ffwd.net.2.bias',\n",
       "              tensor([-2.6970e-03,  1.4640e-03, -3.5484e-03,  1.4605e-03,  6.1299e-03,\n",
       "                       2.3336e-03, -2.7155e-03,  1.6379e-03, -6.5505e-03, -1.0083e-03,\n",
       "                       2.6381e-03, -1.2988e-03,  1.0335e-03, -5.5398e-04,  7.5947e-03,\n",
       "                       1.2687e-03, -1.2555e-03,  1.4550e-03,  1.0189e-03,  5.5798e-03,\n",
       "                      -1.6857e-03, -1.8404e-03, -1.2783e-03,  8.3973e-04, -8.8998e-03,\n",
       "                       3.2974e-04, -1.6048e-03, -5.4321e-03, -6.1137e-04, -4.9253e-04,\n",
       "                      -3.9293e-04,  3.7954e-03,  2.1184e-03,  7.7135e-03,  4.2351e-03,\n",
       "                       1.5196e-03,  2.0537e-03,  7.8814e-04, -2.3460e-04, -1.2219e-03,\n",
       "                      -3.3834e-03, -9.7138e-04, -6.4933e-03, -7.7737e-03, -1.3626e-03,\n",
       "                       6.4121e-04,  2.0384e-03,  2.2785e-03,  2.7564e-03,  1.0742e-03,\n",
       "                      -3.2714e-04, -6.8669e-04,  4.4604e-03,  2.3823e-03, -5.3028e-03,\n",
       "                       2.2140e-03, -4.7798e-03, -4.3527e-04, -1.6690e-03, -1.8568e-03,\n",
       "                       5.2433e-03, -1.6668e-03, -8.2186e-04,  3.0173e-03, -4.0461e-04,\n",
       "                       7.5215e-03, -2.9866e-03,  1.3235e-03,  5.4758e-04,  3.9146e-04,\n",
       "                      -1.1673e-03, -1.8012e-03, -2.4491e-03, -4.0254e-04,  4.0262e-04,\n",
       "                       2.5383e-03, -5.6305e-03,  3.7772e-03, -1.8318e-03, -1.6315e-03,\n",
       "                      -3.7324e-03, -5.7091e-04, -3.3305e-03,  5.4671e-04, -3.4868e-03,\n",
       "                      -1.7203e-04,  5.7122e-03,  2.5349e-03,  4.0558e-03, -5.9266e-04,\n",
       "                      -4.1767e-03, -2.1561e-03, -1.4794e-03, -3.8606e-03,  1.0172e-03,\n",
       "                       3.3947e-03, -8.3495e-03, -1.0202e-02, -5.3211e-03, -2.7895e-03,\n",
       "                       5.7809e-04,  5.2134e-03,  7.8722e-03,  8.2625e-03, -1.7703e-03,\n",
       "                      -7.0642e-03,  7.8115e-03,  8.1017e-03, -1.3827e-03,  4.9071e-04,\n",
       "                       3.3114e-03,  1.2554e-02,  6.1108e-03,  3.8008e-03, -3.1623e-03,\n",
       "                       6.9370e-03,  3.6782e-03,  2.4543e-03, -7.1326e-04, -8.3569e-03,\n",
       "                      -4.0105e-03, -4.6191e-03, -2.8513e-03, -3.6747e-03,  4.5631e-03,\n",
       "                      -5.7648e-03, -6.4594e-03,  5.7220e-04,  4.2310e-03,  1.9641e-03,\n",
       "                       6.9326e-03,  8.7671e-03,  2.8896e-03,  6.0114e-03,  2.9790e-03,\n",
       "                       5.4286e-03,  1.1703e-03, -1.1915e-03, -4.9643e-03, -1.6168e-03,\n",
       "                      -7.7873e-04,  4.4462e-03,  6.7707e-03,  1.7993e-03, -8.1552e-04,\n",
       "                       3.8252e-03, -8.0945e-03, -2.7192e-03, -9.1154e-03, -3.0710e-03,\n",
       "                      -3.5015e-04, -1.0245e-02, -4.0557e-03, -2.0753e-04, -4.7727e-03,\n",
       "                       1.2142e-03,  3.4205e-03, -4.8533e-03, -6.0864e-03, -3.5798e-03,\n",
       "                      -1.2203e-02,  8.5602e-03, -5.4051e-03,  9.4307e-04,  6.8679e-03,\n",
       "                       2.1523e-03, -3.7316e-03,  6.7047e-03,  4.7530e-03,  5.2777e-03,\n",
       "                       5.7471e-03,  2.7766e-03,  6.1443e-03,  2.9655e-03, -2.2631e-03,\n",
       "                       2.1880e-03,  2.7523e-03, -2.5681e-03, -4.6955e-04, -1.3994e-03,\n",
       "                      -3.6238e-03,  1.2162e-03,  3.4641e-03,  3.4887e-03, -1.5735e-03,\n",
       "                       7.7386e-04, -1.4639e-03, -3.1583e-03, -2.2271e-03, -1.6888e-03,\n",
       "                      -4.3504e-03, -4.8281e-03, -7.6932e-03,  2.3983e-03, -2.6368e-03,\n",
       "                      -4.1272e-03, -3.6629e-03, -1.5620e-03,  8.2462e-04, -4.0757e-04,\n",
       "                       5.0029e-03, -7.0848e-03, -1.7063e-05, -9.5088e-03,  6.5616e-03,\n",
       "                       2.5982e-04, -4.7756e-03, -7.2708e-03, -2.7349e-03, -3.5094e-03,\n",
       "                      -2.1359e-03,  7.2320e-03, -4.4855e-03, -2.1204e-03, -4.3804e-03,\n",
       "                       1.0177e-02,  6.1578e-03, -3.2877e-03,  1.3425e-03, -1.0158e-03,\n",
       "                       1.3285e-03, -4.0577e-03, -2.7380e-03, -2.2675e-03,  3.6941e-03,\n",
       "                       1.1332e-03,  4.1013e-03, -5.6098e-03,  8.3238e-04,  3.5821e-03,\n",
       "                      -4.5207e-03,  3.5714e-03,  1.0180e-03,  2.5936e-03, -2.3662e-05,\n",
       "                      -2.2787e-05, -3.1594e-03, -3.4653e-03, -4.7479e-03, -4.1737e-03,\n",
       "                      -2.8695e-03,  6.9306e-03,  3.2815e-04,  1.0899e-02, -6.7695e-04,\n",
       "                       5.2713e-03, -2.0014e-03, -7.2693e-03, -2.2484e-04, -1.7031e-03,\n",
       "                      -2.5765e-03, -1.1191e-03, -1.1342e-03, -4.5875e-03, -3.4692e-03,\n",
       "                      -1.2281e-02, -3.3929e-04,  1.6657e-03,  8.5986e-03, -4.8624e-03,\n",
       "                       3.5482e-03, -2.7914e-03, -8.7031e-04, -3.1503e-04,  3.7370e-03,\n",
       "                       9.1597e-03, -3.5355e-03,  3.7320e-03,  1.3016e-03,  9.7947e-03,\n",
       "                      -4.4910e-03,  6.7655e-03,  1.7575e-04,  5.4366e-04,  3.9191e-04,\n",
       "                       4.1447e-03, -2.0495e-03, -5.2353e-04, -8.2595e-04,  3.9937e-03,\n",
       "                      -1.5279e-04, -3.8367e-04, -4.4149e-03,  5.7549e-04,  5.8368e-03,\n",
       "                      -2.1759e-03, -8.0343e-03, -4.5528e-05, -8.7369e-03,  6.7898e-03,\n",
       "                       3.3124e-03,  7.3453e-03, -3.4182e-03,  2.1370e-03,  5.7255e-04,\n",
       "                       3.9188e-03,  4.5643e-04, -2.2050e-03,  1.8435e-03,  7.9588e-04,\n",
       "                      -3.0058e-03, -2.4687e-04, -6.2627e-04,  1.9633e-03,  3.6117e-04,\n",
       "                       1.3095e-03,  1.0023e-03, -3.4222e-03,  3.8872e-03,  9.1619e-03,\n",
       "                       5.7499e-03, -5.9915e-03, -3.2511e-03, -3.0023e-03,  2.2728e-03,\n",
       "                       7.9741e-04,  2.9309e-04,  2.9418e-03, -4.2328e-03, -6.4714e-03,\n",
       "                       4.7897e-03, -3.7812e-03, -5.6593e-04,  8.0647e-03, -7.9210e-03,\n",
       "                       7.5960e-03, -3.6399e-03,  1.4610e-03, -3.4179e-03,  4.3605e-03,\n",
       "                      -3.6161e-03,  5.8587e-03, -5.4475e-03,  5.9282e-03, -5.1349e-03,\n",
       "                      -6.9882e-03,  3.2158e-03, -1.0607e-04,  3.4746e-03,  1.2187e-03,\n",
       "                      -2.9594e-03, -5.0148e-03,  1.0102e-03, -4.0070e-03, -3.3986e-03,\n",
       "                       3.6212e-03,  6.6252e-04,  2.8560e-03,  1.9920e-03,  4.6570e-04,\n",
       "                      -2.1990e-03, -2.6218e-03,  5.3376e-03,  2.0204e-03,  1.5831e-03,\n",
       "                      -3.7231e-03,  4.0509e-04, -2.5842e-03, -3.7302e-03,  1.0501e-03,\n",
       "                       3.4387e-03,  7.1640e-04, -2.0896e-04, -5.0454e-03, -2.8553e-03,\n",
       "                      -8.6689e-03, -6.9867e-03,  6.7201e-04, -7.2954e-04, -3.5377e-03,\n",
       "                       1.9251e-03,  5.1008e-03, -2.2773e-03,  5.8355e-03,  1.3871e-03,\n",
       "                       5.3588e-03, -1.8351e-03,  5.7189e-03,  2.2628e-03, -1.8878e-03,\n",
       "                       1.9494e-03, -2.9177e-03,  2.4139e-03, -2.0009e-03], device='cuda:0')),\n",
       "             ('blocks.0.ln1.weight',\n",
       "              tensor([0.9818, 0.9619, 0.9439, 0.9596, 0.9535, 0.9822, 0.9449, 0.9594, 0.9319,\n",
       "                      0.9739, 0.9756, 0.9659, 0.9720, 0.9798, 0.9509, 0.9715, 0.9693, 0.9677,\n",
       "                      0.9612, 0.9713, 0.9727, 0.9393, 0.9868, 0.9645, 0.9805, 0.9776, 0.9730,\n",
       "                      0.9598, 0.9513, 0.9335, 0.9568, 0.9511, 0.9650, 0.9914, 0.9709, 0.9375,\n",
       "                      0.9594, 0.9549, 0.9708, 0.9563, 0.9582, 0.9424, 0.9935, 0.9468, 0.9335,\n",
       "                      0.9693, 0.9205, 0.9881, 0.9828, 0.9925, 0.9723, 0.9446, 0.9605, 0.9812,\n",
       "                      0.9670, 0.9573, 0.9798, 0.9722, 0.9933, 0.9755, 0.9930, 0.9931, 0.9827,\n",
       "                      0.9687, 0.9711, 0.9464, 0.9714, 0.9683, 0.9563, 0.9686, 0.9480, 1.0289,\n",
       "                      0.9640, 0.9346, 0.9654, 0.9748, 0.9574, 0.9631, 0.9508, 0.9730, 0.9860,\n",
       "                      0.9571, 0.9739, 0.9800, 0.9571, 0.9789, 0.9803, 0.9487, 0.9543, 0.9774,\n",
       "                      0.9904, 0.9914, 0.9729, 0.9654, 0.9692, 0.9679, 1.0068, 0.9850, 0.9804,\n",
       "                      0.9492, 0.9587, 0.9678, 0.9599, 0.9638, 0.9591, 0.9786, 0.9532, 0.9821,\n",
       "                      0.9759, 0.9945, 0.9635, 0.9493, 0.9615, 0.9616, 0.9616, 0.9616, 0.9696,\n",
       "                      0.9890, 0.9579, 0.9566, 0.9577, 0.9528, 1.0591, 0.9650, 0.9361, 0.9592,\n",
       "                      0.9923, 0.9732, 0.9718, 0.9763, 0.9571, 0.9687, 0.9685, 0.9685, 0.9692,\n",
       "                      0.9454, 0.9746, 0.9684, 0.9534, 0.9580, 0.9670, 0.9816, 0.9571, 0.9816,\n",
       "                      0.9567, 0.9827, 0.9670, 0.9801, 0.9952, 0.9813, 0.9542, 0.9668, 0.9517,\n",
       "                      0.9670, 0.9722, 0.9662, 0.9738, 0.9685, 0.9996, 0.9893, 0.9599, 0.9722,\n",
       "                      0.9624, 0.9960, 0.9556, 0.9512, 0.9910, 0.9389, 0.9707, 0.9638, 0.9608,\n",
       "                      0.9711, 0.9821, 0.9578, 0.9860, 0.9729, 0.9651, 0.9800, 0.9830, 0.9767,\n",
       "                      0.9661, 0.9667, 0.9718, 0.9414, 0.9718, 0.9826, 0.9551, 0.9630, 0.9679,\n",
       "                      0.9447, 0.9844, 0.9625, 0.9543, 0.9760, 0.9979, 0.9752, 0.9438, 0.9435,\n",
       "                      0.9832, 0.9918, 0.9735, 0.9729, 0.9877, 0.9695, 0.9762, 0.9669, 1.0186,\n",
       "                      0.9764, 0.9639, 0.9479, 0.9745, 0.9684, 0.9508, 0.9546, 0.9939, 0.9492,\n",
       "                      0.9720, 0.9599, 0.9660, 0.9654, 0.9825, 0.9890, 0.9675, 0.9605, 0.9598,\n",
       "                      0.9674, 0.9616, 0.9597, 0.9360, 0.9630, 0.9742, 0.9537, 0.9417, 0.9744,\n",
       "                      0.9476, 0.9747, 0.9538, 0.9536, 0.9565, 0.9863, 0.9738, 0.9499, 0.9556,\n",
       "                      0.9708, 0.9720, 0.9627, 0.9812, 0.9598, 0.9430, 0.9558, 0.9370, 0.9597,\n",
       "                      0.9706, 0.9846, 0.9836, 0.9278, 0.9721, 0.9367, 0.9824, 0.9734, 0.9555,\n",
       "                      0.9733, 0.9263, 0.9321, 0.9531, 0.9899, 0.9685, 0.9866, 0.9766, 0.9347,\n",
       "                      0.9651, 1.0288, 0.9515, 0.9711, 0.9799, 0.9804, 0.9638, 0.9762, 0.9640,\n",
       "                      0.9669, 0.9735, 0.9845, 0.9708, 1.0388, 0.9583, 0.9794, 0.9545, 0.9558,\n",
       "                      0.9622, 0.9666, 0.9765, 0.9605, 0.9493, 0.9861, 0.9645, 0.9390, 1.0040,\n",
       "                      0.9907, 0.9707, 0.9697, 0.9525, 0.9799, 0.9883, 0.9460, 0.9591, 0.9628,\n",
       "                      0.9804, 0.9581, 0.9514, 0.9576, 0.9805, 0.9554, 0.9415, 0.9948, 0.9784,\n",
       "                      0.9523, 0.9500, 0.9716, 0.9701, 0.9578, 0.9660, 0.9653, 0.9743, 0.9791,\n",
       "                      0.9708, 0.9592, 0.9657, 0.9814, 0.9774, 0.9902, 0.9852, 0.9785, 0.9770,\n",
       "                      0.9710, 0.9707, 0.9614, 0.9614, 0.9602, 0.9846, 0.9616, 0.9495, 0.9943,\n",
       "                      0.9613, 0.9599, 0.9638, 0.9656, 0.9483, 0.9388, 0.9701, 0.9496, 0.9645,\n",
       "                      0.9426, 0.9528, 0.9601, 0.9915, 0.9463, 0.9619, 0.9435, 0.9734, 0.9707,\n",
       "                      0.9636, 0.9915, 0.9836, 0.9845, 0.9526, 0.9662, 0.9715, 0.9885, 0.9673,\n",
       "                      0.9654, 0.9867, 0.9551, 0.9523, 0.9645, 0.9698, 0.9398, 0.9421, 0.9387,\n",
       "                      0.9733, 0.9868, 0.9857, 0.9720, 0.9445, 0.9807], device='cuda:0')),\n",
       "             ('blocks.0.ln1.bias',\n",
       "              tensor([-3.9483e-03,  3.0887e-02,  5.0773e-03, -7.1758e-03,  1.2280e-02,\n",
       "                       2.5070e-02,  4.0295e-03, -6.9222e-04, -1.7609e-02, -1.6325e-03,\n",
       "                       1.4390e-02,  9.3350e-04, -1.5664e-02, -5.8732e-03,  1.3488e-02,\n",
       "                      -3.8459e-03, -8.8234e-04,  8.5534e-03,  5.6697e-03, -2.7593e-03,\n",
       "                       1.9869e-02, -1.9591e-02, -1.1956e-02, -6.5237e-03, -1.6627e-04,\n",
       "                       5.0687e-04,  1.8705e-03, -7.1920e-03,  1.7693e-03,  8.7710e-03,\n",
       "                      -7.6676e-03, -3.5387e-03, -6.1029e-03,  1.9038e-02, -5.8325e-03,\n",
       "                       8.2657e-03, -1.2917e-02,  1.2165e-02, -1.6874e-02, -1.4115e-02,\n",
       "                       1.7140e-03,  2.3082e-03,  1.8157e-03, -1.3933e-02,  1.3784e-02,\n",
       "                      -1.2696e-02, -1.5366e-02,  4.0879e-03,  1.2023e-02,  3.9052e-02,\n",
       "                       4.3058e-04, -6.3371e-03,  2.3634e-03,  9.0188e-03,  3.1293e-03,\n",
       "                      -3.8963e-03,  9.9086e-03, -2.2469e-03, -1.6821e-02, -1.3917e-02,\n",
       "                      -1.4461e-03,  7.3948e-03, -4.0413e-03, -1.2656e-02, -5.7548e-03,\n",
       "                       1.7293e-03, -8.4986e-03,  3.8028e-03,  3.4444e-03,  9.1236e-04,\n",
       "                      -1.3050e-02, -1.9800e-02, -2.3896e-03, -3.0783e-03,  5.1813e-03,\n",
       "                      -1.0232e-02, -5.5150e-04, -1.6443e-02, -8.1884e-03, -1.1336e-02,\n",
       "                      -5.2848e-03, -2.8494e-03,  3.3917e-02, -7.4227e-05, -1.3608e-02,\n",
       "                       8.6723e-03,  5.0724e-03, -1.6419e-02, -1.0424e-02,  1.5360e-02,\n",
       "                      -4.8948e-03,  7.8140e-04,  8.5880e-03,  7.5471e-03,  7.6994e-04,\n",
       "                       8.9305e-03,  2.0936e-02, -4.2678e-03,  4.2239e-03, -1.1451e-02,\n",
       "                      -7.6273e-03, -3.0854e-02, -4.0557e-03, -2.3097e-03,  1.3525e-02,\n",
       "                       2.2859e-03,  1.2859e-03, -4.6083e-03,  1.8541e-02,  7.7751e-03,\n",
       "                      -8.7910e-03, -4.6839e-03,  9.7749e-03, -8.0356e-03,  1.9365e-03,\n",
       "                       2.3502e-04, -9.7612e-03,  2.4900e-03,  3.5228e-03,  8.5783e-03,\n",
       "                       3.4645e-04, -1.0397e-02, -2.5846e-02, -3.7340e-03, -2.5938e-02,\n",
       "                      -2.3788e-03,  8.5710e-04, -2.4198e-02,  1.1372e-02,  5.2114e-03,\n",
       "                       5.7973e-03,  6.5263e-03,  1.2418e-02, -4.2803e-03, -1.0122e-02,\n",
       "                      -2.6720e-02,  5.3608e-03,  5.9978e-03,  1.2761e-02, -1.6293e-02,\n",
       "                      -1.1774e-02, -2.3836e-03,  2.1893e-02,  1.2570e-03,  7.9989e-03,\n",
       "                       6.3423e-05,  2.1940e-03, -4.1634e-03,  9.3565e-03,  6.1761e-03,\n",
       "                      -1.1558e-02,  7.2451e-03, -9.5687e-03, -1.1429e-02,  5.0493e-03,\n",
       "                       2.3978e-03,  5.5680e-03, -1.7155e-03, -2.2896e-04,  7.2798e-03,\n",
       "                      -4.0567e-03, -1.4665e-02, -1.0326e-02,  1.2000e-02, -5.0945e-03,\n",
       "                      -2.2800e-03,  2.8951e-03,  1.7097e-03,  7.0968e-03, -1.8668e-03,\n",
       "                      -4.2406e-03,  5.1662e-04, -6.4947e-03, -1.8857e-03,  1.4477e-02,\n",
       "                       5.3758e-03,  2.3617e-03, -1.1846e-03,  7.5850e-03, -2.1564e-03,\n",
       "                       9.7060e-03,  5.7135e-03, -2.4751e-03,  1.7983e-02, -5.3772e-03,\n",
       "                       1.1754e-02, -1.2265e-03,  8.3312e-03,  3.6167e-04,  1.9429e-02,\n",
       "                       9.4623e-03,  4.4531e-03,  3.7757e-03, -3.2398e-05, -5.2422e-03,\n",
       "                      -8.9171e-03,  2.3793e-03,  6.2448e-03, -3.8593e-03, -1.7993e-02,\n",
       "                      -5.2048e-04,  4.5258e-03, -1.2018e-03, -3.6516e-03,  1.1232e-02,\n",
       "                      -2.0399e-02,  2.4905e-03, -8.2052e-03, -4.5544e-03, -2.2436e-03,\n",
       "                       2.8659e-03, -1.7330e-03, -3.4758e-03, -2.8854e-03, -6.6969e-04,\n",
       "                       1.2423e-02, -4.5864e-03,  6.5710e-03, -3.2388e-02,  1.3326e-02,\n",
       "                      -1.6216e-03, -5.7685e-03,  1.3280e-02, -9.4432e-03,  9.4602e-03,\n",
       "                       5.7608e-03, -1.3857e-03, -1.7804e-02,  1.3874e-02, -3.7671e-03,\n",
       "                      -6.1804e-05, -3.1876e-03, -7.0112e-03, -8.0319e-03,  7.0845e-03,\n",
       "                       5.9270e-03, -1.2776e-02, -2.1394e-03,  1.4416e-02, -1.3498e-03,\n",
       "                      -1.0017e-02,  1.0307e-03, -1.0885e-03,  7.8511e-03,  5.3049e-04,\n",
       "                       3.6742e-03,  2.0254e-03,  2.0717e-02, -2.6833e-03, -1.0892e-02,\n",
       "                       4.0455e-03, -2.9172e-03, -1.9571e-03,  2.1350e-03, -1.8224e-03,\n",
       "                      -2.7380e-02, -6.3520e-03,  2.2348e-03, -3.1006e-03, -1.6869e-03,\n",
       "                       3.6246e-03, -3.2954e-03, -3.2526e-03, -2.2529e-02,  2.1795e-02,\n",
       "                      -1.0235e-02, -1.2136e-02,  1.0350e-02,  2.0115e-04,  8.0591e-03,\n",
       "                       8.0230e-04,  1.1932e-02, -1.2044e-02, -1.6401e-03, -2.4957e-03,\n",
       "                      -2.1413e-04,  1.5957e-02, -6.7509e-03, -2.0702e-03, -3.3037e-03,\n",
       "                       1.1827e-02,  5.5247e-03, -2.0207e-03,  1.0404e-02, -1.1634e-02,\n",
       "                      -1.2548e-03, -4.7536e-03, -1.7280e-03, -5.2691e-03,  6.6351e-03,\n",
       "                       6.1850e-03, -2.7053e-03,  1.4171e-02,  8.7151e-03, -3.0289e-03,\n",
       "                      -2.2098e-02,  5.7417e-03, -2.1077e-02,  1.1114e-02,  3.9851e-03,\n",
       "                       9.5074e-03,  3.3342e-03, -3.6235e-02, -8.6872e-03, -5.2666e-03,\n",
       "                      -2.2509e-03, -1.2267e-02,  5.6131e-03, -6.1300e-03,  1.0756e-02,\n",
       "                       4.6498e-03, -1.1734e-03,  1.7412e-02, -8.0559e-03,  4.8985e-03,\n",
       "                       6.1690e-03,  2.9692e-03, -1.4621e-02,  2.0148e-02,  1.1518e-02,\n",
       "                      -1.0554e-02,  2.4590e-02,  4.3544e-03, -5.6489e-03, -5.8286e-03,\n",
       "                       1.5480e-02,  2.8622e-04, -1.4345e-03, -1.5207e-02,  6.6931e-03,\n",
       "                       1.3029e-02, -3.0002e-03,  8.0644e-03, -1.1135e-02,  6.8025e-03,\n",
       "                      -8.9118e-03, -7.9515e-03,  9.0997e-03, -2.2045e-03,  7.8632e-03,\n",
       "                       6.9726e-03,  3.4410e-03,  3.2116e-03, -2.1506e-04,  9.8524e-03,\n",
       "                       9.3408e-03, -2.3467e-02,  4.5129e-03,  2.0470e-02,  8.4386e-04,\n",
       "                       6.4570e-04, -1.1052e-03,  1.5298e-02, -1.5169e-03, -8.4333e-04,\n",
       "                      -9.4145e-03, -8.6479e-03,  1.7911e-02, -2.7929e-04, -5.6491e-03,\n",
       "                      -2.3164e-03,  6.0998e-02, -2.4631e-03,  6.4594e-03,  8.1984e-04,\n",
       "                       9.4027e-03, -9.7750e-03,  5.0344e-03,  1.3393e-02, -2.6869e-02,\n",
       "                       4.4761e-03,  2.0471e-02, -4.5745e-03,  1.9817e-03,  3.6055e-03,\n",
       "                       2.1061e-03, -2.0315e-02, -1.1221e-02,  7.0317e-03, -1.1570e-03,\n",
       "                      -2.1313e-02, -3.8629e-03, -2.5857e-03,  7.3220e-03], device='cuda:0')),\n",
       "             ('blocks.0.ln2.weight',\n",
       "              tensor([1.0199, 1.0343, 1.0346, 1.0720, 1.0283, 1.0704, 1.0264, 1.0447, 1.0113,\n",
       "                      1.0478, 1.0185, 1.0303, 1.0160, 1.0226, 1.0736, 1.0147, 0.9893, 1.0581,\n",
       "                      0.9643, 1.0111, 1.0243, 1.0149, 1.0341, 1.0547, 1.0304, 0.9871, 1.0053,\n",
       "                      1.0652, 1.0675, 1.0650, 1.0563, 1.0515, 1.0690, 1.0776, 1.0140, 1.0554,\n",
       "                      0.9967, 1.0243, 1.0299, 1.0667, 0.9931, 0.9970, 1.0288, 1.0382, 1.0419,\n",
       "                      1.0064, 1.0563, 1.0179, 1.0184, 1.0432, 1.0668, 1.0270, 0.9752, 1.0030,\n",
       "                      0.9863, 1.0049, 0.9927, 0.9304, 0.9527, 1.0084, 0.9817, 0.9989, 1.0591,\n",
       "                      1.0502, 1.0179, 1.0156, 1.0394, 1.0465, 1.0575, 1.0669, 1.0677, 1.0061,\n",
       "                      1.0088, 1.0413, 1.0062, 1.0820, 1.0210, 1.0477, 0.9977, 1.0259, 1.0374,\n",
       "                      1.0283, 1.1003, 1.0027, 1.0436, 0.9945, 1.0201, 1.0021, 1.0053, 1.0677,\n",
       "                      1.0097, 1.0265, 1.0244, 1.0577, 0.9910, 1.0207, 1.0185, 1.0233, 1.0507,\n",
       "                      1.0064, 1.0570, 0.9976, 1.0198, 1.0282, 1.0058, 1.0134, 1.0202, 1.0435,\n",
       "                      1.0146, 1.0450, 1.0355, 1.0082, 0.9912, 1.0230, 1.0324, 1.0172, 1.0574,\n",
       "                      1.0390, 1.0258, 1.0492, 0.9835, 1.0319, 0.9378, 1.0832, 1.0056, 1.0461,\n",
       "                      0.9835, 1.0246, 1.0746, 1.0398, 1.0526, 1.0335, 1.0754, 0.9961, 1.0126,\n",
       "                      1.0461, 1.0113, 1.1093, 1.0261, 0.9812, 0.9949, 1.0650, 1.0334, 1.0541,\n",
       "                      1.0052, 1.0244, 1.0324, 1.0272, 1.0069, 1.0357, 1.0448, 0.9995, 1.0236,\n",
       "                      1.0393, 1.0418, 1.0465, 0.9848, 1.0012, 1.0046, 1.0365, 1.0321, 1.0561,\n",
       "                      0.9951, 1.0467, 1.0418, 1.0172, 0.9989, 1.0402, 0.9991, 1.0058, 1.0430,\n",
       "                      1.0902, 1.0221, 1.0050, 1.0282, 1.0301, 1.0006, 1.0076, 1.0874, 1.0037,\n",
       "                      1.0674, 1.0126, 1.0170, 1.0072, 1.0410, 0.9900, 1.0156, 1.0367, 1.0381,\n",
       "                      1.0043, 1.0538, 1.0298, 1.0213, 1.0008, 1.0508, 0.9999, 0.9592, 0.9742,\n",
       "                      1.0604, 1.0570, 1.0410, 1.0467, 1.0312, 1.0186, 1.0277, 0.9830, 1.0477,\n",
       "                      1.0339, 1.0188, 1.0198, 1.0050, 1.0292, 1.0246, 1.0566, 0.9733, 0.9811,\n",
       "                      1.0583, 0.9997, 1.0440, 1.0448, 0.9890, 1.0195, 0.9732, 1.0540, 1.0658,\n",
       "                      0.9831, 1.0718, 1.0133, 1.0408, 1.0693, 1.0248, 1.0818, 0.9557, 1.0222,\n",
       "                      1.0396, 1.0314, 0.9949, 1.0363, 0.9659, 1.0901, 1.0129, 1.0340, 1.0486,\n",
       "                      1.0603, 1.0408, 1.0348, 1.0339, 1.0155, 1.0842, 0.9830, 1.0237, 1.0435,\n",
       "                      1.0724, 1.0372, 1.0346, 1.0098, 1.0030, 1.0262, 1.0288, 1.0081, 1.0031,\n",
       "                      1.0356, 1.0442, 1.0394, 0.9758, 1.0548, 1.0754, 1.0559, 1.0194, 1.0406,\n",
       "                      1.0623, 1.0573, 0.9763, 1.0270, 1.0353, 1.0762, 1.0003, 1.0112, 1.0327,\n",
       "                      1.0601, 1.0434, 1.0481, 0.9831, 0.9388, 1.0286, 1.0492, 1.0359, 1.0535,\n",
       "                      1.0480, 0.9881, 1.0377, 1.0465, 1.0179, 1.0331, 1.0065, 1.0752, 1.0005,\n",
       "                      1.0256, 0.9486, 1.0395, 1.0279, 1.0266, 0.9354, 1.0482, 1.0351, 1.0563,\n",
       "                      1.0564, 1.0358, 1.0535, 1.0019, 1.0457, 1.0070, 1.0604, 1.0326, 1.0182,\n",
       "                      1.0187, 0.9939, 1.0024, 1.0058, 1.0058, 1.0787, 1.1155, 1.0756, 1.0479,\n",
       "                      1.0358, 1.0378, 1.0195, 1.0440, 1.0123, 0.9855, 1.0714, 1.0364, 1.0267,\n",
       "                      1.0430, 0.9976, 1.0235, 1.0120, 1.0968, 1.0237, 1.0780, 1.0105, 0.9920,\n",
       "                      1.0514, 1.0139, 1.0311, 1.0517, 1.0185, 1.0294, 1.0941, 1.0006, 1.0152,\n",
       "                      1.0508, 1.0226, 1.0196, 1.0146, 1.0129, 1.0100, 0.9864, 1.0215, 1.0776,\n",
       "                      1.0462, 1.0835, 1.0392, 1.0288, 1.0705, 1.1040, 1.0218, 1.0310, 1.0276,\n",
       "                      1.0499, 1.1013, 1.0627, 1.0183, 1.0120, 1.0211, 1.0132, 0.9964, 1.0020,\n",
       "                      0.9526, 1.0321, 1.0238, 1.0339, 1.0282, 1.0140], device='cuda:0')),\n",
       "             ('blocks.0.ln2.bias',\n",
       "              tensor([ 3.2359e-02,  7.7740e-03,  3.3663e-03,  2.1300e-02,  1.0721e-02,\n",
       "                      -8.7333e-02,  1.2209e-02, -3.3546e-03,  1.1178e-02, -8.7481e-03,\n",
       "                      -7.6226e-02, -9.5303e-02, -4.8299e-02, -2.5546e-02,  2.0899e-02,\n",
       "                      -3.8496e-02,  1.2043e-02, -3.5153e-02,  2.2126e-02, -3.0363e-02,\n",
       "                      -2.0979e-02, -3.0000e-02,  6.0769e-02,  4.4376e-02, -1.5691e-02,\n",
       "                      -1.2801e-02, -2.0758e-02, -1.5251e-02,  7.9755e-02,  2.9000e-02,\n",
       "                      -5.4017e-03,  2.7637e-02,  2.5042e-03, -8.8997e-02, -1.6922e-02,\n",
       "                      -8.9438e-03,  3.9655e-02,  2.3373e-02,  2.3515e-02,  7.0129e-02,\n",
       "                      -7.6398e-03,  1.8693e-03, -4.4054e-02, -2.4681e-02,  3.9579e-02,\n",
       "                      -2.3802e-02, -2.6210e-02, -4.9375e-02, -2.7145e-02,  6.5850e-02,\n",
       "                       1.3792e-02,  7.4763e-03, -9.9530e-03, -1.5790e-02, -1.8848e-03,\n",
       "                      -1.1078e-02,  1.6467e-02,  4.1130e-02,  2.0575e-03,  4.1803e-02,\n",
       "                       5.0919e-02,  3.9329e-03, -9.8362e-02,  3.6091e-02, -4.8116e-03,\n",
       "                       5.0130e-02,  4.8734e-02, -2.9112e-02, -8.8592e-03,  9.2642e-03,\n",
       "                      -6.7344e-02, -4.8119e-02,  1.5644e-02, -3.2953e-02, -3.3830e-02,\n",
       "                       4.7339e-02,  1.8761e-02,  2.5186e-03, -5.7335e-03, -3.0729e-02,\n",
       "                      -1.3454e-02, -7.4286e-02,  8.7849e-02, -1.0428e-02, -1.1716e-02,\n",
       "                       1.3268e-03,  3.2814e-02,  7.4326e-02,  6.8029e-02,  5.6462e-03,\n",
       "                       1.3323e-02,  1.7258e-02,  1.5691e-03,  9.3003e-02,  3.8759e-02,\n",
       "                      -3.4512e-02,  4.6427e-02, -2.1778e-02,  4.6839e-02,  2.0558e-02,\n",
       "                       5.8007e-02, -2.6694e-02,  6.4172e-02,  4.5305e-02, -5.1639e-02,\n",
       "                      -4.0240e-02, -3.6368e-02,  6.1191e-02, -3.1071e-02,  3.5959e-02,\n",
       "                      -5.1558e-02, -5.1093e-02, -2.4373e-02,  1.3277e-02, -5.7384e-02,\n",
       "                       2.3659e-02,  3.2800e-02, -1.2943e-02,  4.3882e-02, -3.6117e-02,\n",
       "                       4.6131e-02,  6.4727e-03, -2.2438e-02,  3.0725e-02, -2.8303e-02,\n",
       "                      -1.8508e-02,  3.1453e-02, -2.9565e-02,  7.3318e-02,  4.7788e-02,\n",
       "                       8.4515e-02, -9.8305e-03,  2.4606e-02,  2.0251e-02,  1.8672e-02,\n",
       "                       7.8294e-02, -3.4572e-03, -8.8323e-02,  7.6132e-03,  4.8038e-02,\n",
       "                       9.3021e-03,  6.8066e-02,  4.9579e-02,  2.9110e-02,  3.3098e-02,\n",
       "                       4.6118e-02, -7.8060e-02,  3.0534e-02, -4.3089e-02, -4.3513e-02,\n",
       "                      -9.4911e-03,  3.0739e-02,  1.7947e-02, -1.3994e-02, -4.3334e-02,\n",
       "                       3.1221e-02,  9.4405e-03,  3.1302e-03, -3.6445e-02, -3.7427e-03,\n",
       "                       9.8672e-03,  4.1897e-02,  5.5962e-03, -3.8542e-02,  5.0040e-02,\n",
       "                       7.4968e-03,  8.0457e-04,  2.5850e-05, -2.4529e-02,  1.4676e-02,\n",
       "                      -3.2903e-03, -1.1261e-02,  5.1707e-02, -3.9488e-02, -9.5965e-03,\n",
       "                       3.1090e-02,  3.6920e-02, -3.4370e-02,  5.1498e-02, -1.4275e-02,\n",
       "                      -9.9641e-02, -1.0708e-03,  6.0710e-02, -4.3828e-03, -8.9773e-03,\n",
       "                      -7.3781e-02, -6.5442e-02, -1.6029e-03,  6.6296e-03,  4.5563e-02,\n",
       "                       2.3043e-02, -2.8907e-02, -1.6942e-03, -1.3654e-02,  2.0425e-03,\n",
       "                       8.2948e-02,  1.0445e-02, -2.6825e-02,  3.0053e-02,  6.3260e-02,\n",
       "                      -3.5481e-02, -1.7870e-02,  4.6602e-02,  1.6856e-02, -3.8742e-02,\n",
       "                       2.4897e-02, -5.8903e-02,  2.3672e-02,  5.3728e-02, -8.2555e-02,\n",
       "                       1.4239e-02, -3.3023e-02,  2.0566e-02, -5.1662e-02, -2.4162e-02,\n",
       "                      -3.0116e-03, -1.4293e-02, -1.5851e-03, -4.9379e-02, -7.6835e-02,\n",
       "                      -2.0146e-02,  7.3127e-02, -4.2085e-02, -2.8284e-02, -2.1573e-02,\n",
       "                      -2.7965e-02, -8.1045e-02,  9.0895e-03,  2.0280e-02,  6.7543e-02,\n",
       "                       3.3268e-02,  1.2424e-01,  3.1045e-02,  2.0147e-02,  1.2437e-03,\n",
       "                       2.7521e-02,  1.8160e-02,  6.2414e-02, -1.8933e-02, -1.4844e-02,\n",
       "                       5.9150e-02, -2.7019e-02,  2.9563e-02,  3.1352e-03, -5.2758e-02,\n",
       "                      -5.7422e-03, -6.2303e-02, -1.0191e-02, -2.9944e-02,  6.1586e-02,\n",
       "                      -5.0497e-02,  3.2713e-02,  1.4169e-02,  6.0316e-02,  4.3150e-02,\n",
       "                       2.5230e-02,  3.0786e-02, -3.4219e-02,  6.5600e-03,  3.0268e-03,\n",
       "                       5.9536e-02, -1.3470e-02, -9.0337e-04,  2.9284e-03,  3.2084e-02,\n",
       "                       3.6532e-02, -6.9665e-02,  1.5741e-03,  2.2692e-02, -4.0206e-02,\n",
       "                      -2.7755e-02, -9.7759e-02, -3.0955e-02, -8.2629e-03,  5.7025e-02,\n",
       "                       9.9613e-03, -2.9628e-02,  6.0256e-03, -4.6023e-03, -4.2191e-02,\n",
       "                      -2.8379e-02, -2.5834e-02,  1.0157e-02,  5.4169e-02, -2.6114e-02,\n",
       "                      -1.2269e-02,  2.3060e-03,  4.3268e-02,  2.3996e-02, -3.2702e-02,\n",
       "                      -5.3945e-02,  1.1533e-02,  5.0900e-02,  1.1607e-02, -3.3244e-02,\n",
       "                       2.0005e-02, -2.1864e-02,  9.0753e-02, -2.4780e-02, -5.7138e-03,\n",
       "                       1.2749e-03,  1.8965e-02, -4.2056e-02, -9.3120e-03,  6.6140e-02,\n",
       "                       5.2038e-02, -1.7916e-02, -2.9828e-02,  2.0641e-02, -4.2828e-02,\n",
       "                      -8.2301e-02,  2.7156e-03, -1.0607e-02,  2.2123e-02,  2.8976e-02,\n",
       "                      -2.7313e-02,  1.3708e-02,  4.0504e-03, -2.0116e-02,  2.2391e-02,\n",
       "                       4.6220e-02,  6.1932e-02, -2.1514e-02, -2.7804e-02,  3.5531e-02,\n",
       "                      -1.0516e-01,  6.1710e-02, -2.5982e-02,  2.9156e-02, -4.3491e-03,\n",
       "                      -4.9823e-02,  3.5583e-02, -3.1442e-02,  5.2078e-02, -5.3213e-02,\n",
       "                      -4.2800e-02, -5.6258e-03,  7.2003e-02,  1.2570e-02, -9.1124e-02,\n",
       "                       2.8601e-02,  8.4633e-04, -1.7435e-02, -2.2519e-02, -4.4394e-03,\n",
       "                      -4.6207e-02, -5.9681e-03,  3.4697e-02, -3.8064e-02,  4.9543e-02,\n",
       "                      -7.9445e-02, -1.0432e-02,  1.6701e-02,  2.8536e-02,  1.9380e-02,\n",
       "                      -4.7365e-03, -1.1586e-01, -2.9047e-02, -5.7991e-02, -3.4585e-02,\n",
       "                       6.1209e-02,  7.7088e-02,  2.0741e-02, -5.9177e-02, -1.4725e-02,\n",
       "                      -7.7471e-02,  5.8150e-02, -3.4603e-02, -2.2053e-02,  3.1308e-02,\n",
       "                      -1.4242e-01,  4.8127e-02, -1.0373e-01, -2.5158e-02,  1.5597e-02,\n",
       "                       3.2702e-02,  3.7695e-02,  3.4968e-02,  1.0512e-02,  1.2353e-02,\n",
       "                       4.4872e-03, -8.4858e-02,  3.3071e-02, -3.5703e-02], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.0.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.0.key.weight',\n",
       "              tensor([[ 3.1057e-02, -4.7482e-03,  1.3310e-02,  ..., -1.9209e-02,\n",
       "                       -1.3086e-02,  2.3450e-02],\n",
       "                      [-1.1651e-02, -3.2105e-03,  2.0045e-03,  ...,  1.9621e-02,\n",
       "                        3.3410e-02, -7.6179e-02],\n",
       "                      [-5.4897e-02, -2.3265e-02, -1.9048e-02,  ..., -8.2504e-02,\n",
       "                        2.4692e-02, -2.6223e-03],\n",
       "                      ...,\n",
       "                      [ 1.1621e-02,  9.0931e-02,  4.5107e-03,  ..., -1.4594e-02,\n",
       "                       -4.3952e-02, -4.2619e-02],\n",
       "                      [ 1.0162e-02,  1.3001e-02, -3.3745e-02,  ..., -2.0503e-02,\n",
       "                       -3.3442e-05,  5.5280e-03],\n",
       "                      [ 4.8654e-02, -1.8394e-02, -6.4446e-02,  ..., -3.2082e-02,\n",
       "                       -8.9592e-02,  1.7481e-02]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.0.query.weight',\n",
       "              tensor([[-0.0097,  0.0148,  0.0040,  ..., -0.0218, -0.0098,  0.0381],\n",
       "                      [ 0.0194,  0.0222,  0.0143,  ...,  0.0578,  0.0260,  0.0501],\n",
       "                      [-0.0221,  0.0011, -0.0476,  ..., -0.0370,  0.0266,  0.0154],\n",
       "                      ...,\n",
       "                      [ 0.0199,  0.0120, -0.0063,  ...,  0.0554,  0.0040, -0.0084],\n",
       "                      [-0.0132,  0.0406, -0.0258,  ..., -0.0142,  0.0116, -0.0204],\n",
       "                      [ 0.0541, -0.0043, -0.0008,  ..., -0.0567,  0.0036, -0.0085]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.0.value.weight',\n",
       "              tensor([[-1.8181e-03,  1.4655e-03,  2.2067e-04,  ...,  3.1014e-02,\n",
       "                       -1.1542e-02, -1.5725e-02],\n",
       "                      [ 2.9632e-02,  4.7012e-03, -1.8679e-02,  ..., -1.3436e-02,\n",
       "                        5.3733e-03, -2.7065e-02],\n",
       "                      [ 5.5814e-04,  1.7426e-02,  5.8238e-02,  ..., -1.5774e-02,\n",
       "                       -2.4459e-02, -2.0981e-02],\n",
       "                      ...,\n",
       "                      [ 2.5042e-03,  2.9803e-04,  5.5945e-02,  ...,  2.0664e-02,\n",
       "                       -2.0153e-02,  2.4871e-02],\n",
       "                      [ 3.4309e-04, -3.2559e-02, -2.7635e-02,  ...,  3.3680e-04,\n",
       "                       -9.1858e-03,  2.1809e-03],\n",
       "                      [ 1.3980e-02, -5.3594e-03,  7.8494e-03,  ...,  1.1279e-05,\n",
       "                        1.0649e-02, -1.7697e-02]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.1.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.1.key.weight',\n",
       "              tensor([[ 0.0123,  0.0232, -0.0226,  ..., -0.0118,  0.0326,  0.0194],\n",
       "                      [-0.0524, -0.0774, -0.0280,  ..., -0.0552,  0.0127,  0.0027],\n",
       "                      [ 0.0223,  0.0015, -0.0072,  ..., -0.0204, -0.0803, -0.0351],\n",
       "                      ...,\n",
       "                      [-0.0506, -0.0273,  0.0689,  ...,  0.0110,  0.0724, -0.0181],\n",
       "                      [-0.0191, -0.0317, -0.0578,  ...,  0.0345, -0.0092,  0.0383],\n",
       "                      [ 0.0473, -0.0061,  0.0421,  ..., -0.0113,  0.0221, -0.0129]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.1.query.weight',\n",
       "              tensor([[ 0.0214, -0.0709, -0.0066,  ..., -0.0026,  0.0052, -0.0107],\n",
       "                      [-0.0032,  0.0103, -0.0028,  ...,  0.0240, -0.0292,  0.0117],\n",
       "                      [-0.0052, -0.0251, -0.0268,  ...,  0.0073, -0.0093,  0.0239],\n",
       "                      ...,\n",
       "                      [ 0.0246,  0.0515,  0.0213,  ..., -0.0331, -0.0521,  0.0237],\n",
       "                      [ 0.0038,  0.0367,  0.0038,  ..., -0.0284, -0.0033, -0.0156],\n",
       "                      [-0.0828,  0.0076,  0.0117,  ..., -0.0484,  0.0080,  0.0230]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.1.value.weight',\n",
       "              tensor([[ 0.0004,  0.0063, -0.0090,  ..., -0.0181,  0.0026,  0.0129],\n",
       "                      [-0.0118, -0.0143,  0.0073,  ...,  0.0333,  0.0032, -0.0131],\n",
       "                      [ 0.0128,  0.0113,  0.0079,  ..., -0.0197, -0.0132, -0.0049],\n",
       "                      ...,\n",
       "                      [ 0.0378,  0.0066,  0.0200,  ...,  0.0264,  0.0102, -0.0292],\n",
       "                      [-0.0059,  0.0021, -0.0034,  ...,  0.0008,  0.0038,  0.0276],\n",
       "                      [ 0.0403,  0.0078,  0.0047,  ...,  0.0128, -0.0254,  0.0321]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.2.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.2.key.weight',\n",
       "              tensor([[-0.0324, -0.0041,  0.0400,  ..., -0.0007,  0.0181, -0.0023],\n",
       "                      [ 0.0009,  0.0189,  0.0289,  ..., -0.0086, -0.0116,  0.0011],\n",
       "                      [ 0.0139,  0.0343,  0.0137,  ..., -0.0073, -0.0523, -0.0102],\n",
       "                      ...,\n",
       "                      [-0.0148,  0.0053, -0.0047,  ..., -0.0018,  0.0073, -0.0017],\n",
       "                      [ 0.0044,  0.0288,  0.0284,  ...,  0.0310, -0.0382,  0.0293],\n",
       "                      [ 0.0117,  0.0358, -0.0104,  ..., -0.0036, -0.0288, -0.0751]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.2.query.weight',\n",
       "              tensor([[ 0.0023,  0.0520, -0.0377,  ..., -0.0302, -0.0617,  0.0581],\n",
       "                      [ 0.0339, -0.0643,  0.0607,  ..., -0.0142, -0.0007,  0.0289],\n",
       "                      [ 0.0181,  0.0309, -0.0348,  ...,  0.0896,  0.0201,  0.0044],\n",
       "                      ...,\n",
       "                      [ 0.0513, -0.0618, -0.0330,  ..., -0.0248,  0.0013,  0.0179],\n",
       "                      [ 0.0061,  0.0136, -0.0349,  ..., -0.0135, -0.0666,  0.0126],\n",
       "                      [-0.0168,  0.0286, -0.0270,  ...,  0.0581,  0.0417, -0.0323]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.2.value.weight',\n",
       "              tensor([[-0.0116,  0.0185,  0.0532,  ...,  0.0443, -0.0042,  0.0043],\n",
       "                      [-0.0066, -0.0217, -0.0052,  ...,  0.0080,  0.0064, -0.0234],\n",
       "                      [-0.0339, -0.0014, -0.0170,  ..., -0.0182, -0.0284,  0.0033],\n",
       "                      ...,\n",
       "                      [-0.0081,  0.0517, -0.0430,  ..., -0.0041,  0.0060,  0.0212],\n",
       "                      [ 0.0099,  0.0026,  0.0257,  ...,  0.0287,  0.0162, -0.0042],\n",
       "                      [-0.0226, -0.0013, -0.0111,  ...,  0.0088,  0.0444, -0.0045]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.3.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.3.key.weight',\n",
       "              tensor([[-0.0153,  0.0292, -0.0194,  ..., -0.0081, -0.0248,  0.0052],\n",
       "                      [ 0.0064,  0.0829, -0.0385,  ...,  0.0250, -0.0787, -0.0052],\n",
       "                      [-0.0500, -0.0468, -0.0167,  ...,  0.0495,  0.0286,  0.0329],\n",
       "                      ...,\n",
       "                      [ 0.0120, -0.0049, -0.0288,  ..., -0.0285,  0.0232,  0.0085],\n",
       "                      [ 0.0164, -0.0210,  0.0087,  ..., -0.0080,  0.0405, -0.0083],\n",
       "                      [-0.0055, -0.0698, -0.0073,  ...,  0.0254,  0.0120,  0.0063]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.3.query.weight',\n",
       "              tensor([[-0.0090, -0.0326,  0.0144,  ..., -0.0012,  0.0266,  0.0107],\n",
       "                      [-0.0337,  0.0542, -0.0415,  ..., -0.0141,  0.0216, -0.0458],\n",
       "                      [-0.0082,  0.0104, -0.0200,  ...,  0.0040,  0.0315,  0.0342],\n",
       "                      ...,\n",
       "                      [ 0.0621,  0.0041, -0.0710,  ..., -0.0413, -0.0337,  0.0166],\n",
       "                      [-0.0100,  0.0513, -0.0068,  ..., -0.0200, -0.0114,  0.0038],\n",
       "                      [-0.0113,  0.0595, -0.0693,  ..., -0.0261, -0.0258,  0.0189]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.3.value.weight',\n",
       "              tensor([[-0.0187, -0.0379,  0.0052,  ..., -0.0005, -0.0052,  0.0111],\n",
       "                      [ 0.0032,  0.0187,  0.0154,  ...,  0.0018,  0.0062, -0.0159],\n",
       "                      [-0.0167, -0.0146, -0.0127,  ..., -0.0014,  0.0111,  0.0019],\n",
       "                      ...,\n",
       "                      [-0.0156,  0.0042, -0.0062,  ..., -0.0060,  0.0226,  0.0396],\n",
       "                      [ 0.0057,  0.0014, -0.0123,  ..., -0.0045,  0.0218,  0.0076],\n",
       "                      [-0.0127,  0.0331,  0.0070,  ...,  0.0183,  0.0023,  0.0255]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.4.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.4.key.weight',\n",
       "              tensor([[-0.0018,  0.0528,  0.0463,  ...,  0.0091,  0.0124,  0.0481],\n",
       "                      [ 0.0301, -0.0357, -0.0519,  ..., -0.0258,  0.0007,  0.0087],\n",
       "                      [-0.0061, -0.0008,  0.0392,  ..., -0.0143, -0.0141, -0.0191],\n",
       "                      ...,\n",
       "                      [ 0.0234,  0.0227,  0.0029,  ..., -0.0122, -0.0170, -0.0333],\n",
       "                      [-0.0556, -0.0145,  0.0120,  ..., -0.0284, -0.0288,  0.0332],\n",
       "                      [ 0.0029, -0.0003,  0.0223,  ..., -0.0110,  0.0102, -0.0098]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.4.query.weight',\n",
       "              tensor([[ 0.0061,  0.0372,  0.0051,  ...,  0.0096,  0.0226, -0.0435],\n",
       "                      [-0.0038, -0.0225, -0.0108,  ...,  0.0061,  0.0098,  0.0013],\n",
       "                      [ 0.0284,  0.0377, -0.0215,  ...,  0.0682,  0.0222, -0.0144],\n",
       "                      ...,\n",
       "                      [ 0.0434, -0.0126, -0.0326,  ..., -0.0394,  0.0212,  0.0446],\n",
       "                      [-0.0136, -0.0556,  0.0404,  ..., -0.0194,  0.0146, -0.0380],\n",
       "                      [-0.0110, -0.0199,  0.0198,  ...,  0.0438, -0.0208, -0.0177]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.4.value.weight',\n",
       "              tensor([[ 0.0217,  0.0048, -0.0209,  ..., -0.0031,  0.0134, -0.0242],\n",
       "                      [-0.0303, -0.0183,  0.0068,  ...,  0.0004,  0.0178, -0.0165],\n",
       "                      [-0.0222,  0.0208,  0.0413,  ...,  0.0261,  0.0107, -0.0173],\n",
       "                      ...,\n",
       "                      [ 0.0049, -0.0003, -0.0248,  ..., -0.0138,  0.0043,  0.0191],\n",
       "                      [-0.0138, -0.0176,  0.0124,  ...,  0.0045, -0.0001, -0.0087],\n",
       "                      [-0.0073,  0.0043, -0.0310,  ..., -0.0112, -0.0104,  0.0020]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.5.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.5.key.weight',\n",
       "              tensor([[-0.0533, -0.0362, -0.0365,  ..., -0.0069, -0.0042,  0.0124],\n",
       "                      [ 0.0246,  0.0152,  0.0094,  ...,  0.0030, -0.0510,  0.0079],\n",
       "                      [-0.0266, -0.0125, -0.0099,  ..., -0.0042,  0.0267, -0.0139],\n",
       "                      ...,\n",
       "                      [ 0.0582, -0.0105, -0.0003,  ...,  0.0007,  0.0012, -0.0343],\n",
       "                      [ 0.0134,  0.0315, -0.0061,  ...,  0.0338,  0.0053, -0.0085],\n",
       "                      [ 0.0102, -0.0231,  0.0195,  ...,  0.0209,  0.0209,  0.0071]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.5.query.weight',\n",
       "              tensor([[ 0.0263, -0.0235, -0.0550,  ...,  0.0026,  0.0085,  0.0603],\n",
       "                      [ 0.0175, -0.0227, -0.0234,  ..., -0.0093,  0.0117,  0.0465],\n",
       "                      [-0.0101,  0.0344,  0.0526,  ..., -0.0079, -0.0259, -0.0241],\n",
       "                      ...,\n",
       "                      [ 0.0184,  0.0338, -0.0709,  ..., -0.0159,  0.0438,  0.0119],\n",
       "                      [ 0.0526, -0.0436, -0.0031,  ..., -0.0330,  0.0449, -0.0603],\n",
       "                      [-0.0172, -0.0409, -0.0366,  ..., -0.0641,  0.0377, -0.0367]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.5.value.weight',\n",
       "              tensor([[ 2.7722e-02, -2.9381e-02, -1.6915e-02,  ...,  2.7710e-03,\n",
       "                        2.5968e-02,  1.3197e-02],\n",
       "                      [-8.1235e-05, -1.7897e-03,  1.6902e-02,  ...,  2.9902e-02,\n",
       "                       -2.0799e-02, -1.7532e-02],\n",
       "                      [ 1.9466e-02, -3.8863e-03, -4.4207e-02,  ..., -5.4379e-03,\n",
       "                        1.6472e-02, -4.4937e-02],\n",
       "                      ...,\n",
       "                      [ 4.6553e-03, -1.2781e-02,  4.6187e-04,  ..., -2.8976e-02,\n",
       "                       -4.2382e-03, -4.8900e-03],\n",
       "                      [-2.8395e-02, -1.8999e-02, -1.4140e-02,  ..., -8.0736e-03,\n",
       "                       -1.1771e-02, -1.2757e-02],\n",
       "                      [ 1.4761e-02,  3.4340e-03,  1.9916e-02,  ...,  7.5029e-03,\n",
       "                       -1.3408e-03, -1.9810e-02]], device='cuda:0')),\n",
       "             ('blocks.1.sa.proj.weight',\n",
       "              tensor([[-0.0067,  0.0469, -0.0278,  ..., -0.0139,  0.0179,  0.0055],\n",
       "                      [-0.0190,  0.0017, -0.0048,  ..., -0.0231, -0.0151,  0.0061],\n",
       "                      [ 0.0205,  0.0093,  0.0148,  ...,  0.0060,  0.0234,  0.0401],\n",
       "                      ...,\n",
       "                      [-0.0130,  0.0011, -0.0131,  ...,  0.0181,  0.0224, -0.0158],\n",
       "                      [-0.0554,  0.0130, -0.0262,  ..., -0.0029,  0.0186, -0.0040],\n",
       "                      [-0.0176, -0.0318,  0.0063,  ..., -0.0240, -0.0098,  0.0004]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.proj.bias',\n",
       "              tensor([-4.6680e-03,  5.6588e-03,  1.1635e-02, -4.9886e-03, -1.2140e-02,\n",
       "                       2.6586e-02, -6.5478e-04,  3.3822e-02, -1.0366e-02, -1.4880e-02,\n",
       "                      -1.7754e-02,  2.1010e-02,  1.8920e-02, -1.1076e-02,  1.2541e-02,\n",
       "                       3.7915e-03, -1.6206e-02, -1.4261e-02, -2.0976e-02,  1.5923e-02,\n",
       "                      -1.8285e-02, -1.9164e-02, -2.3797e-02,  1.3326e-02,  8.5407e-03,\n",
       "                       2.6993e-02,  9.4497e-03, -2.4335e-02, -2.2192e-02, -8.3752e-03,\n",
       "                       2.0003e-02, -6.0158e-04,  1.3183e-03,  1.7313e-02,  6.4514e-03,\n",
       "                       5.8178e-03, -2.0630e-02, -1.6720e-02, -1.5229e-02, -3.7071e-03,\n",
       "                      -1.1122e-02,  2.0366e-02, -1.4678e-02, -2.1159e-02,  1.3607e-02,\n",
       "                      -1.9270e-02, -1.8827e-02,  7.8466e-03,  1.4890e-02, -8.6123e-03,\n",
       "                       1.7977e-02,  1.4385e-02,  4.1626e-03, -3.0019e-02, -2.2171e-02,\n",
       "                       1.7792e-02, -1.7455e-02,  9.0731e-03, -1.8894e-02,  1.6394e-02,\n",
       "                       6.2388e-04, -2.2736e-02,  1.1231e-02,  6.2671e-03,  1.7007e-02,\n",
       "                       1.9118e-02, -1.7029e-02,  1.8682e-02,  1.3144e-02,  1.9973e-02,\n",
       "                       1.4413e-02,  2.4193e-03,  2.3059e-02, -1.3648e-02, -9.8700e-03,\n",
       "                       2.4269e-02,  6.1697e-03, -1.7148e-02, -1.5277e-02, -1.0186e-02,\n",
       "                       1.5135e-02,  1.5447e-02, -1.0804e-02, -5.9794e-03, -2.1822e-02,\n",
       "                      -1.5939e-02,  2.8410e-03,  1.7977e-02,  1.2031e-02,  3.7969e-03,\n",
       "                       5.5555e-03,  2.0832e-02,  9.3410e-03, -1.7368e-02,  1.2867e-02,\n",
       "                       4.1674e-03, -1.8503e-02, -1.6464e-02, -2.3588e-02, -1.0768e-02,\n",
       "                      -6.7196e-03,  3.2172e-03,  2.1223e-02,  1.7765e-02,  2.3183e-02,\n",
       "                      -1.4189e-02,  1.1497e-02,  1.3808e-02,  2.0661e-02, -1.5470e-02,\n",
       "                       1.6768e-03,  1.7558e-02, -4.4841e-03,  1.6885e-02, -2.9505e-02,\n",
       "                       1.1981e-02,  1.2746e-02,  9.7793e-03,  6.9769e-03, -8.8408e-03,\n",
       "                       1.3729e-02, -2.5244e-02,  2.3649e-03,  1.3593e-02,  2.0954e-02,\n",
       "                       4.6901e-03,  8.5110e-03,  1.9007e-02, -2.2011e-02, -1.4817e-02,\n",
       "                       7.7562e-03,  1.9916e-02,  1.3342e-02,  1.7073e-02, -1.2628e-02,\n",
       "                       2.0964e-02, -1.4401e-02, -1.6979e-02, -2.0853e-02, -2.8964e-02,\n",
       "                      -1.2672e-02,  2.2692e-02, -1.7214e-02, -8.9388e-03, -1.5970e-02,\n",
       "                       2.1784e-02, -2.8706e-02,  2.0438e-02,  5.4635e-03, -9.9033e-03,\n",
       "                      -1.1680e-02, -1.4730e-02, -1.3227e-03,  2.0471e-02,  1.3616e-02,\n",
       "                      -1.5198e-02,  1.3759e-02, -1.8326e-02, -1.2059e-02, -1.6509e-02,\n",
       "                      -1.2565e-02,  1.4463e-02, -1.3154e-02,  1.6262e-03,  1.8497e-02,\n",
       "                       2.8596e-03, -1.4456e-02,  2.3035e-02, -1.2693e-02,  1.5076e-02,\n",
       "                       1.8032e-02, -1.9799e-02,  6.1260e-03,  2.2957e-02, -2.7507e-03,\n",
       "                      -1.6982e-02,  1.6915e-02,  4.9571e-03,  1.1613e-02, -1.5334e-02,\n",
       "                      -1.4634e-02,  7.8805e-03,  1.1460e-03,  3.4802e-02, -1.2899e-02,\n",
       "                      -3.3993e-03, -1.5490e-02,  4.4756e-03,  1.1455e-02, -1.8968e-02,\n",
       "                      -4.6474e-03, -1.7818e-02, -1.6760e-02,  8.8027e-03, -1.9938e-02,\n",
       "                       9.7613e-03, -1.2432e-02, -1.7008e-02, -5.5416e-03,  1.8187e-02,\n",
       "                       8.5025e-03,  1.7538e-02,  1.0458e-02, -1.3709e-02,  1.0251e-02,\n",
       "                      -1.4519e-02, -6.7522e-03, -1.4100e-02,  1.2478e-02, -1.0384e-02,\n",
       "                       1.7753e-02, -1.8789e-02, -2.9037e-02,  6.2596e-03, -2.3751e-02,\n",
       "                      -4.3182e-05,  9.7112e-03, -7.8235e-03,  1.3074e-02,  9.3292e-03,\n",
       "                       2.2841e-02, -1.4753e-02, -1.6809e-02, -3.0495e-04, -4.1466e-03,\n",
       "                      -1.1036e-02,  1.4423e-02, -2.2419e-02,  9.5205e-03, -8.1009e-03,\n",
       "                      -3.2242e-02,  3.3202e-03, -1.3849e-02,  3.6631e-03,  1.3251e-02,\n",
       "                      -1.6226e-02, -7.4353e-03,  7.6099e-03,  1.1984e-02,  3.1731e-03,\n",
       "                      -2.2247e-02,  1.4553e-02, -2.9599e-02,  1.3989e-02,  2.2729e-02,\n",
       "                       1.0260e-02, -1.8350e-02, -1.5070e-02,  4.1772e-03,  1.5578e-02,\n",
       "                       1.5685e-02,  4.0081e-03,  8.3745e-03, -1.2809e-02, -1.2546e-02,\n",
       "                      -2.2889e-02, -2.0968e-02,  1.3575e-02,  1.1144e-02, -9.3804e-03,\n",
       "                       1.1284e-02,  8.8198e-03, -1.6686e-02, -1.9929e-02,  7.9678e-03,\n",
       "                       1.6929e-02, -2.1841e-02,  1.3897e-02, -2.3923e-02,  1.8222e-02,\n",
       "                       5.1699e-03,  1.9501e-02, -2.3960e-02,  1.1557e-02, -1.1480e-02,\n",
       "                      -8.5608e-03,  1.4889e-02,  8.1435e-03, -2.5101e-02, -1.6921e-02,\n",
       "                      -1.0251e-02, -1.5541e-02, -5.2553e-03, -3.2023e-03,  1.2088e-02,\n",
       "                      -9.8563e-04, -1.6639e-02, -2.2826e-03, -2.3019e-02,  1.1403e-02,\n",
       "                       6.0194e-03,  1.9826e-02,  1.9299e-02,  1.1040e-02, -2.4366e-02,\n",
       "                       9.1914e-03, -1.7453e-02,  3.0222e-02,  2.5152e-02,  8.0323e-03,\n",
       "                      -1.5169e-02, -1.0349e-02, -1.9699e-02,  1.5991e-02,  1.9665e-02,\n",
       "                      -1.8285e-02,  2.4645e-02, -4.8410e-03,  1.0634e-02, -9.1535e-03,\n",
       "                       2.6914e-02, -2.2653e-02,  1.8835e-02,  3.9649e-04,  1.7309e-02,\n",
       "                       1.1140e-02,  3.0748e-02, -2.1985e-02, -1.8668e-02, -2.0036e-02,\n",
       "                       6.5149e-03,  2.2766e-02,  7.2947e-03,  1.0580e-02,  1.3437e-02,\n",
       "                       1.3887e-02, -1.7325e-02,  2.0430e-03, -1.5863e-02,  1.8464e-02,\n",
       "                       1.9794e-02, -5.7120e-03, -1.1293e-02,  1.2292e-02, -6.0309e-03,\n",
       "                      -1.0818e-02, -1.5144e-02,  3.4873e-03, -1.2161e-02,  1.7141e-02,\n",
       "                      -1.6206e-02,  1.3388e-02,  7.0481e-03,  1.5881e-02, -2.0769e-02,\n",
       "                       2.1669e-02, -5.7756e-03, -4.9161e-03, -1.7674e-02, -6.8905e-03,\n",
       "                       5.2383e-03, -4.6999e-03,  2.1140e-02, -1.3420e-02,  7.9652e-03,\n",
       "                       1.2721e-02,  1.6934e-02, -1.6346e-02, -8.2767e-03, -2.4310e-02,\n",
       "                       3.1169e-02,  1.5423e-02,  6.6553e-03,  2.2209e-02,  5.1756e-03,\n",
       "                      -9.2725e-03, -1.9808e-02, -2.1592e-02, -1.5069e-02, -1.3851e-02,\n",
       "                       3.1335e-02, -9.5733e-03,  1.6962e-02,  8.7834e-03, -1.6852e-02,\n",
       "                       1.4809e-02,  2.2041e-03,  2.2579e-02,  1.2924e-02, -1.0256e-02,\n",
       "                       2.9230e-02,  2.3143e-02,  1.1680e-02, -6.8067e-03], device='cuda:0')),\n",
       "             ('blocks.1.ffwd.net.0.weight',\n",
       "              tensor([[ 0.0048, -0.0736, -0.0426,  ...,  0.0141,  0.0208, -0.0490],\n",
       "                      [ 0.0368, -0.0077, -0.0496,  ..., -0.0151, -0.0244,  0.0471],\n",
       "                      [ 0.0381,  0.0255,  0.0376,  ..., -0.0438, -0.0153,  0.0196],\n",
       "                      ...,\n",
       "                      [ 0.0370, -0.0432, -0.0557,  ..., -0.0373,  0.0026,  0.0673],\n",
       "                      [-0.0032,  0.0064, -0.0368,  ...,  0.0338,  0.0465,  0.0309],\n",
       "                      [-0.0214,  0.0193, -0.0084,  ..., -0.0120, -0.0544,  0.0554]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.ffwd.net.0.bias',\n",
       "              tensor([-0.0329, -0.0381, -0.0269,  ..., -0.0350, -0.0232, -0.0251],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.ffwd.net.2.weight',\n",
       "              tensor([[ 0.0531, -0.0128,  0.0035,  ..., -0.0309,  0.0071, -0.0215],\n",
       "                      [-0.0344,  0.0057,  0.0039,  ..., -0.0315,  0.0027, -0.0401],\n",
       "                      [ 0.0489,  0.0121,  0.0229,  ..., -0.0144, -0.0170, -0.0221],\n",
       "                      ...,\n",
       "                      [ 0.0203,  0.0266, -0.0208,  ...,  0.0175, -0.0187, -0.0109],\n",
       "                      [ 0.0078, -0.0201, -0.0266,  ..., -0.0009, -0.0550, -0.0238],\n",
       "                      [ 0.0146, -0.0030,  0.0186,  ...,  0.0285,  0.0285, -0.0093]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.ffwd.net.2.bias',\n",
       "              tensor([ 5.2624e-03,  3.1366e-03, -1.8887e-02,  1.2316e-02,  1.4809e-02,\n",
       "                      -1.6993e-02,  4.7611e-05, -2.6960e-02,  1.1569e-02,  6.8950e-03,\n",
       "                       9.9022e-03, -1.1926e-02, -7.5967e-03,  3.1758e-03, -8.3991e-03,\n",
       "                      -9.3499e-03, -4.1388e-03,  1.9440e-02,  1.2781e-02, -3.8618e-03,\n",
       "                       1.8082e-02,  1.0203e-02,  9.2574e-03, -5.7036e-03, -1.6933e-02,\n",
       "                      -2.4147e-02, -2.3621e-02,  8.9501e-03,  9.7712e-03, -2.8748e-03,\n",
       "                      -9.9100e-03,  8.3725e-03,  5.5438e-03, -8.3295e-03,  8.5876e-03,\n",
       "                       1.1001e-02,  2.6335e-02,  8.5187e-03,  1.2723e-02,  4.0193e-03,\n",
       "                       8.3606e-03, -1.4799e-02, -2.7597e-04,  1.3885e-02, -1.1852e-02,\n",
       "                       1.0213e-02,  2.0742e-02, -2.7660e-03, -1.4567e-03,  3.4533e-03,\n",
       "                      -1.3672e-02, -6.7418e-03, -1.1440e-03,  3.8761e-02,  5.2447e-03,\n",
       "                      -7.7164e-03,  6.0038e-03, -7.3088e-04,  1.1973e-02, -4.7031e-03,\n",
       "                       2.1529e-03,  2.1974e-02, -3.1658e-03,  6.1262e-03,  5.6885e-04,\n",
       "                      -7.8568e-03,  9.3543e-03, -1.8232e-02, -1.3039e-02, -2.8715e-02,\n",
       "                      -2.2561e-02, -1.4829e-02, -1.4102e-02, -4.7189e-03,  1.7314e-02,\n",
       "                      -2.0668e-02, -5.2161e-03,  1.8113e-02,  6.4159e-03,  8.6655e-03,\n",
       "                      -3.3367e-03, -4.0180e-03,  1.1086e-02, -1.0273e-02,  8.7584e-03,\n",
       "                       1.4000e-02,  8.2670e-03,  4.3806e-03, -1.1501e-02,  3.9711e-03,\n",
       "                      -5.0917e-03, -5.1946e-03, -1.0782e-02,  8.5047e-03, -6.8768e-03,\n",
       "                       1.5589e-03,  6.4472e-05, -2.0966e-03,  1.6270e-02,  1.0608e-03,\n",
       "                      -3.2457e-03, -4.4548e-03, -1.8850e-02,  1.1772e-03, -2.1391e-02,\n",
       "                       1.5489e-02,  7.4578e-03, -4.2692e-03, -5.3841e-03,  1.2161e-02,\n",
       "                      -3.3293e-03, -8.3097e-03,  5.5095e-03, -1.2166e-02,  2.2569e-02,\n",
       "                      -3.0134e-03,  1.7833e-04, -3.9566e-03, -5.1227e-03,  4.1424e-03,\n",
       "                      -1.7112e-02,  2.4025e-02, -2.0055e-02, -1.9163e-02, -1.3605e-02,\n",
       "                      -6.7969e-03, -1.6855e-02, -1.2756e-02,  1.1743e-02,  1.3394e-02,\n",
       "                       1.1008e-02, -1.5041e-02, -2.4520e-03, -4.5114e-03,  1.1516e-02,\n",
       "                      -1.7598e-02, -1.2665e-03,  1.5893e-02,  1.6967e-02,  2.2621e-02,\n",
       "                       1.1144e-03, -2.5529e-03,  2.1561e-02,  7.6894e-03,  7.3234e-03,\n",
       "                      -1.2242e-02,  2.5798e-02, -1.4039e-02, -2.2140e-03, -8.1765e-03,\n",
       "                       2.4668e-03,  5.3134e-03,  9.8799e-03, -2.2892e-02, -7.3157e-03,\n",
       "                       6.2658e-03, -8.4231e-03,  1.4333e-02,  8.7519e-03,  1.2850e-02,\n",
       "                       1.5668e-02, -3.8325e-03,  3.5003e-03,  5.4196e-03,  4.2784e-03,\n",
       "                       1.1397e-02,  6.0450e-03, -1.3337e-02,  3.0564e-03, -9.9164e-03,\n",
       "                      -1.4488e-02,  1.6061e-02,  4.0375e-03, -3.5135e-03, -1.1640e-02,\n",
       "                       1.5759e-02, -2.9400e-03, -1.1471e-02, -1.8755e-02,  5.7179e-03,\n",
       "                       1.1156e-02, -4.6815e-03,  1.0582e-03, -1.2690e-02,  1.4070e-02,\n",
       "                       3.4297e-03,  5.9590e-03,  2.1332e-03, -1.1615e-02,  1.1718e-02,\n",
       "                      -2.4380e-03,  8.8030e-03,  4.7994e-03, -1.6808e-02,  9.7373e-03,\n",
       "                      -6.1546e-03,  6.5903e-03,  1.6506e-02, -4.3841e-03, -1.8333e-02,\n",
       "                      -4.1184e-03, -2.2311e-02, -2.4096e-02,  6.4456e-03, -1.6087e-03,\n",
       "                       8.3156e-03,  1.9083e-03,  1.5403e-02, -2.6268e-03, -9.0781e-04,\n",
       "                      -1.1239e-02,  1.1797e-02,  1.8875e-02, -4.7544e-03,  2.0385e-02,\n",
       "                       5.6196e-03, -7.8741e-03,  3.2250e-03, -1.4816e-02,  8.6287e-03,\n",
       "                      -2.1581e-02,  4.8848e-03,  1.4933e-02, -3.5785e-03,  1.4448e-03,\n",
       "                       8.0529e-03, -2.0580e-02,  5.0893e-03,  1.7998e-04,  6.7250e-03,\n",
       "                       3.0353e-02,  6.1016e-03,  2.1250e-02,  1.2227e-02, -1.0650e-02,\n",
       "                       1.1725e-02,  9.7292e-03, -1.5766e-03, -1.3350e-02, -8.3930e-03,\n",
       "                       1.1845e-02, -5.2794e-03,  2.9268e-02, -1.7085e-02, -1.4028e-02,\n",
       "                      -3.4065e-03,  1.2112e-02, -5.5810e-04, -3.1245e-03, -2.1087e-02,\n",
       "                      -1.3153e-02, -1.4957e-03,  4.3943e-03, -5.9947e-04,  7.6773e-03,\n",
       "                       6.1628e-03,  3.6246e-02, -1.6521e-02, -5.8104e-03,  5.4818e-03,\n",
       "                      -2.7106e-03, -5.4302e-03, -1.1805e-03,  2.2207e-02, -1.6198e-03,\n",
       "                      -7.5900e-03,  5.6572e-03, -4.5078e-03,  2.6356e-02, -8.5832e-03,\n",
       "                      -1.1258e-02, -1.3731e-02,  9.8657e-03,  1.6007e-03,  8.4952e-03,\n",
       "                       2.2909e-02, -9.5011e-03,  2.4148e-03,  9.5849e-03,  2.2142e-02,\n",
       "                       2.2260e-02,  1.3980e-02,  3.7218e-03, -7.5285e-03, -9.6253e-03,\n",
       "                      -2.8561e-03,  1.9410e-03,  4.5474e-03,  3.2424e-03, -1.8153e-03,\n",
       "                       1.4877e-03,  4.0588e-03, -5.6084e-03, -8.6012e-03,  1.4646e-02,\n",
       "                      -1.7253e-02,  2.1554e-02, -2.8659e-02, -1.8430e-02,  5.1874e-03,\n",
       "                      -2.3450e-03,  1.0011e-02,  2.5024e-02, -2.3060e-04, -2.0671e-02,\n",
       "                       7.2031e-03, -2.2796e-02, -6.6807e-03, -8.4250e-03,  1.5451e-02,\n",
       "                      -2.3190e-02,  1.1168e-02, -1.4422e-02,  1.5821e-03, -1.2593e-02,\n",
       "                       5.9642e-03, -2.8316e-02,  1.8319e-02,  3.4879e-03,  9.5113e-03,\n",
       "                       8.2834e-03, -2.0871e-02, -9.3371e-03, -1.1166e-02, -1.6990e-02,\n",
       "                      -1.3795e-02,  7.9414e-03, -1.1000e-02,  2.0707e-02, -1.5077e-02,\n",
       "                      -1.4819e-02,  8.9640e-03,  2.9481e-03, -1.3343e-02, -4.4344e-03,\n",
       "                       1.7299e-03,  8.7898e-03, -8.5373e-03,  1.7305e-02, -4.3280e-03,\n",
       "                       9.5979e-03, -1.0330e-02,  4.5595e-03, -1.5108e-02,  1.1349e-02,\n",
       "                      -2.0678e-02,  4.2375e-03,  1.0980e-02,  6.3681e-03,  2.9339e-03,\n",
       "                      -1.3531e-02, -2.2574e-03, -1.9793e-02,  7.9736e-03, -1.8983e-03,\n",
       "                      -4.4157e-03, -3.4230e-03,  4.7123e-03,  9.6476e-03,  2.1719e-02,\n",
       "                      -1.5258e-02, -1.2390e-02, -1.0545e-03, -2.8193e-02, -6.3137e-05,\n",
       "                       1.0628e-02, -2.0790e-03,  1.2836e-02,  1.5953e-02,  1.2016e-02,\n",
       "                      -3.4886e-02,  9.1454e-03, -1.0351e-03, -6.2060e-03,  4.6908e-03,\n",
       "                       8.2353e-05,  9.0163e-03, -1.9057e-02, -1.0761e-02, -1.9849e-03,\n",
       "                      -2.8986e-02, -8.9700e-03, -1.3800e-02,  8.0299e-03], device='cuda:0')),\n",
       "             ('blocks.1.ln1.weight',\n",
       "              tensor([0.8115, 0.8533, 0.8132, 0.8540, 0.8330, 0.8690, 0.8352, 0.8743, 0.8354,\n",
       "                      0.8499, 0.8873, 0.8343, 0.8043, 0.8274, 0.8274, 0.8572, 0.8198, 0.8569,\n",
       "                      0.8680, 0.8427, 0.9015, 0.8637, 0.8890, 0.8608, 0.8099, 0.8425, 0.8352,\n",
       "                      0.8452, 0.8290, 0.8519, 0.8514, 0.7654, 0.8071, 0.8185, 0.8291, 0.8373,\n",
       "                      0.8595, 0.8230, 0.8476, 0.8243, 0.8317, 0.8774, 0.8629, 0.8527, 0.8730,\n",
       "                      0.8163, 0.8394, 0.8333, 0.8367, 0.8793, 0.8303, 0.8511, 0.8000, 0.8922,\n",
       "                      0.8496, 0.8549, 0.8096, 0.7938, 0.9653, 0.8770, 0.8444, 0.8591, 0.8445,\n",
       "                      0.8344, 0.8463, 0.8670, 0.8202, 0.8262, 0.8575, 0.8677, 0.8254, 0.9308,\n",
       "                      0.8476, 0.8403, 0.8672, 0.8548, 0.8501, 0.7970, 0.8308, 0.8398, 0.8487,\n",
       "                      0.8619, 0.8435, 0.8395, 0.8497, 0.8551, 0.7830, 0.8708, 0.8486, 0.8081,\n",
       "                      0.8183, 0.8418, 0.8271, 0.8446, 0.8593, 0.8493, 0.9133, 0.8285, 0.8487,\n",
       "                      0.8459, 0.8308, 0.8798, 0.8566, 0.8015, 0.8538, 0.8274, 0.8025, 0.8138,\n",
       "                      0.8863, 0.8248, 0.8480, 0.8443, 0.8596, 0.8591, 0.8684, 0.8288, 0.7905,\n",
       "                      0.8199, 0.8194, 0.8167, 0.8304, 0.8508, 0.8955, 0.8460, 0.8536, 0.8008,\n",
       "                      0.8310, 0.8580, 0.8656, 0.8210, 0.8460, 0.8648, 0.8485, 0.8276, 0.8788,\n",
       "                      0.8535, 0.8028, 0.8571, 0.8642, 0.8980, 0.8332, 0.8426, 0.8938, 0.8173,\n",
       "                      0.8077, 0.8752, 0.8711, 0.8414, 0.7936, 0.8398, 0.8463, 0.8217, 0.8086,\n",
       "                      0.8606, 0.8117, 0.8438, 0.8630, 0.8344, 0.8380, 0.8389, 0.8184, 0.8339,\n",
       "                      0.8411, 0.8182, 0.8683, 0.8172, 0.8172, 0.8407, 0.8296, 0.8356, 0.8607,\n",
       "                      0.8523, 0.8332, 0.8558, 0.8021, 0.8238, 0.8278, 0.8461, 0.8085, 0.8778,\n",
       "                      0.8264, 0.8410, 0.8161, 0.8646, 0.8085, 0.8136, 0.8675, 0.8669, 0.8170,\n",
       "                      0.8738, 0.8292, 0.8476, 0.8415, 0.8204, 0.8656, 0.8543, 0.8286, 0.8533,\n",
       "                      0.8050, 0.8853, 0.8410, 0.8734, 0.8295, 0.8538, 0.8043, 0.8680, 0.8097,\n",
       "                      0.8451, 0.8460, 0.7974, 0.8216, 0.8565, 0.8603, 0.8023, 0.8373, 0.8012,\n",
       "                      0.8431, 0.8071, 0.8716, 0.8309, 0.8472, 0.8257, 0.8166, 0.8458, 0.8464,\n",
       "                      0.8427, 0.8641, 0.8352, 0.8624, 0.8034, 0.8750, 0.8146, 0.8426, 0.8756,\n",
       "                      0.8461, 0.8695, 0.8527, 0.7974, 0.8466, 0.8273, 0.8843, 0.8442, 0.8726,\n",
       "                      0.8427, 0.8370, 0.8368, 0.8817, 0.8717, 0.7876, 0.8025, 0.8444, 0.8308,\n",
       "                      0.8676, 0.8162, 0.8000, 0.8325, 0.8676, 0.8136, 0.8562, 0.8143, 0.8305,\n",
       "                      0.8071, 0.8376, 0.8386, 0.8291, 0.8549, 0.8545, 0.8321, 0.8581, 0.8362,\n",
       "                      0.8121, 0.8916, 0.8922, 0.8448, 0.8590, 0.8265, 0.8627, 0.8306, 0.8610,\n",
       "                      0.8725, 0.8681, 0.8585, 0.8626, 0.9129, 0.8156, 0.8008, 0.8546, 0.8127,\n",
       "                      0.8776, 0.8301, 0.8331, 0.8660, 0.8571, 0.8281, 0.8722, 0.8153, 0.8390,\n",
       "                      0.8567, 0.9149, 0.8457, 0.8009, 0.8162, 0.8984, 0.8404, 0.8564, 0.8282,\n",
       "                      0.8500, 0.7981, 0.8398, 0.8279, 0.8651, 0.8604, 0.8455, 0.7904, 0.8222,\n",
       "                      0.8211, 0.8779, 0.9168, 0.8291, 0.8448, 0.8177, 0.8807, 0.8308, 0.8099,\n",
       "                      0.8670, 0.8605, 0.8381, 0.8322, 0.8528, 0.8650, 0.8574, 0.8018, 0.8258,\n",
       "                      0.8681, 0.8167, 0.8347, 0.8052, 0.8242, 0.8367, 0.8006, 0.8732, 0.8186,\n",
       "                      0.7980, 0.8523, 0.8441, 0.8030, 0.8829, 0.8833, 0.8062, 0.8303, 0.8227,\n",
       "                      0.8190, 0.8329, 0.8151, 0.8294, 0.8304, 0.8319, 0.8507, 0.8347, 0.8196,\n",
       "                      0.8649, 0.8780, 0.8505, 0.8717, 0.8008, 0.8554, 0.8124, 0.8568, 0.8364,\n",
       "                      0.8879, 0.8777, 0.8445, 0.8735, 0.8426, 0.8300, 0.8696, 0.8483, 0.8678,\n",
       "                      0.8088, 0.8056, 0.8839, 0.8449, 0.8243, 0.8547], device='cuda:0')),\n",
       "             ('blocks.1.ln1.bias',\n",
       "              tensor([-1.9840e-03, -6.0114e-03,  2.0995e-02, -8.8699e-03, -2.5844e-03,\n",
       "                       2.7738e-02, -6.5278e-03,  3.0078e-02, -1.2518e-02,  1.2300e-02,\n",
       "                       6.5237e-03, -3.6889e-03, -4.6090e-02,  3.8754e-02,  2.1386e-02,\n",
       "                       4.6439e-03,  2.9778e-02,  1.9848e-02,  2.3360e-03,  3.1219e-03,\n",
       "                       7.3736e-03, -2.8387e-02, -2.3034e-02,  9.0258e-03,  2.4000e-02,\n",
       "                       1.0310e-02, -5.0907e-03,  1.0549e-03,  1.3591e-02,  3.1620e-02,\n",
       "                       5.3712e-03, -2.2842e-03, -3.5671e-03, -9.7468e-03, -4.3543e-02,\n",
       "                      -3.4638e-02, -1.5467e-02,  3.9521e-02,  3.9180e-03, -5.8837e-03,\n",
       "                      -5.1752e-03,  3.5305e-02, -1.2352e-03, -1.8279e-02, -1.3977e-02,\n",
       "                      -7.0469e-03, -4.4432e-02,  4.3909e-03, -4.9205e-03,  4.1217e-02,\n",
       "                       8.1121e-03, -2.0660e-03,  1.5705e-02, -2.8636e-02,  2.6253e-03,\n",
       "                      -2.4237e-03, -1.7189e-04, -1.0621e-02, -3.0554e-02, -2.0647e-02,\n",
       "                       1.6907e-02, -3.0751e-02,  7.3212e-03,  8.2233e-03, -5.7644e-03,\n",
       "                       2.9591e-02, -5.2694e-03,  4.7667e-03,  9.1125e-03,  8.5174e-03,\n",
       "                       1.4657e-02,  1.8002e-04, -6.6419e-04,  1.4900e-02, -1.0957e-02,\n",
       "                      -3.1334e-03,  5.2132e-03, -1.8880e-02,  4.2513e-03,  2.5877e-02,\n",
       "                       4.2827e-03,  2.3958e-03, -2.0948e-02,  3.3859e-02,  1.4837e-02,\n",
       "                       1.2834e-02,  3.4715e-03, -4.6968e-03, -1.8128e-02, -3.0129e-02,\n",
       "                      -2.2373e-02, -3.5528e-03,  5.8112e-03,  7.3112e-03, -9.8687e-03,\n",
       "                      -2.2658e-02,  1.1158e-02,  2.4284e-02, -2.8894e-02,  2.4637e-02,\n",
       "                      -5.5142e-03, -7.4024e-03,  3.0180e-02, -2.9673e-02,  3.6683e-02,\n",
       "                       5.1718e-03, -4.0710e-02,  1.4868e-02,  7.0459e-03, -1.4238e-02,\n",
       "                       2.5215e-02,  1.9459e-02,  2.8929e-02,  3.5236e-03, -5.1660e-03,\n",
       "                      -6.4415e-03, -1.3423e-02, -2.0336e-02, -8.0704e-03,  4.2678e-02,\n",
       "                       7.7503e-03, -2.2923e-02,  2.9266e-03, -1.3255e-02, -3.4879e-03,\n",
       "                      -1.1319e-02, -2.0925e-02,  9.7520e-03,  2.9553e-03,  9.3810e-03,\n",
       "                      -4.8481e-03,  9.1827e-03, -1.3179e-02, -7.8843e-03,  1.7939e-02,\n",
       "                       3.2417e-02, -2.8538e-03, -2.0734e-02, -1.2678e-02,  1.9160e-03,\n",
       "                      -3.9707e-05, -9.8593e-03, -3.5904e-03,  7.0450e-03, -8.8165e-03,\n",
       "                       1.2416e-02, -4.0246e-02, -9.5184e-03, -2.2175e-03,  2.4015e-02,\n",
       "                       1.8132e-02,  1.0581e-03, -4.9980e-02,  2.8645e-02, -7.5649e-03,\n",
       "                       2.3458e-03,  6.6854e-03, -4.8420e-03, -1.8669e-02, -1.3055e-03,\n",
       "                      -3.7305e-02,  3.0610e-02, -1.6145e-04, -2.7801e-02, -4.4326e-03,\n",
       "                      -1.2449e-02,  1.1481e-03,  2.4429e-02,  1.0264e-02,  2.7816e-02,\n",
       "                       2.5602e-02,  4.6054e-03,  2.9507e-02, -8.6177e-03,  2.0721e-02,\n",
       "                       1.6401e-02, -1.2076e-02,  1.2247e-02,  4.6592e-03,  1.7176e-02,\n",
       "                      -8.1806e-03, -2.0649e-02,  3.9051e-02,  5.0562e-03,  1.3349e-02,\n",
       "                       1.6581e-02,  3.0023e-02, -2.0198e-02, -2.0628e-02, -2.9549e-03,\n",
       "                      -4.2378e-03, -1.2874e-02, -1.4581e-02,  5.7219e-03,  1.7620e-03,\n",
       "                      -2.5965e-02,  8.9339e-03,  2.2276e-03,  1.3087e-02,  4.8552e-03,\n",
       "                      -1.1777e-03, -1.5873e-02,  3.9493e-03, -7.2099e-03, -1.1539e-02,\n",
       "                      -2.6553e-02,  9.4407e-03, -6.2495e-04, -1.2304e-02, -7.0447e-03,\n",
       "                      -5.6483e-03, -4.1059e-03, -1.2388e-02, -2.2722e-02, -5.7265e-05,\n",
       "                       2.4484e-02,  2.1822e-02,  2.9303e-02,  9.8674e-03, -2.9916e-02,\n",
       "                       1.3440e-02,  1.7022e-02,  3.3852e-03,  1.6264e-02, -8.4399e-03,\n",
       "                      -7.4459e-03,  1.3837e-03, -2.8467e-02, -1.0047e-03,  8.2030e-03,\n",
       "                      -2.6849e-03,  6.9674e-03, -4.5423e-03, -1.0036e-02,  5.8649e-03,\n",
       "                       5.7805e-04, -1.4446e-03, -1.2059e-02, -8.4119e-03,  9.0628e-03,\n",
       "                      -1.1787e-02,  3.5157e-02, -2.8266e-02, -1.4448e-03,  2.7037e-02,\n",
       "                      -2.5871e-02, -1.4132e-02,  1.9362e-02,  4.0908e-04,  8.7819e-03,\n",
       "                      -2.0642e-02,  1.1380e-02,  6.8721e-03,  1.8966e-02,  3.4921e-02,\n",
       "                      -2.5609e-02, -1.6387e-02,  1.6368e-02,  1.5315e-02, -1.1977e-03,\n",
       "                      -1.7320e-02, -2.1899e-02,  5.1285e-04, -1.4962e-02, -1.2456e-02,\n",
       "                      -3.8737e-03, -1.0128e-02, -1.0772e-02,  7.4198e-03, -1.9681e-02,\n",
       "                      -1.1241e-02,  2.9241e-03, -1.0786e-02,  1.2134e-02,  2.3439e-02,\n",
       "                      -2.9471e-02,  2.3962e-02, -2.6764e-02,  1.4708e-02, -1.1451e-02,\n",
       "                       2.0929e-02,  2.1478e-02, -2.4171e-04,  4.1619e-02, -2.9736e-02,\n",
       "                      -1.1953e-02, -2.3508e-02, -3.8722e-03,  4.4334e-03, -1.4213e-02,\n",
       "                      -3.9435e-02,  5.0092e-03, -1.8679e-02, -1.7640e-03, -1.9830e-03,\n",
       "                       1.8045e-02,  1.5718e-02,  1.8939e-02,  1.7942e-02, -7.1914e-03,\n",
       "                       2.5278e-02,  8.7867e-03, -3.1299e-02, -1.3398e-02,  2.9867e-02,\n",
       "                       1.4829e-02,  2.2122e-02,  7.4695e-03,  3.8548e-02, -2.1049e-03,\n",
       "                       1.3205e-02, -2.8436e-02,  1.2897e-02, -8.9445e-03, -2.0671e-02,\n",
       "                       4.6489e-03,  3.1352e-02, -3.1648e-02,  7.4664e-03,  1.6862e-02,\n",
       "                       1.4557e-02,  1.9354e-02, -1.7776e-03, -7.3825e-03, -2.0274e-04,\n",
       "                       1.5504e-02,  1.5703e-03, -4.7446e-03, -8.6584e-03,  4.7190e-02,\n",
       "                       2.4625e-03,  6.4905e-03,  5.7219e-03,  1.7538e-02,  2.4984e-02,\n",
       "                       3.6076e-02,  2.2385e-02,  2.4450e-02,  9.1275e-03, -2.4476e-02,\n",
       "                      -7.5469e-03, -6.5920e-04, -1.3055e-02, -1.0885e-02,  3.5652e-03,\n",
       "                      -1.8714e-02,  2.5351e-02,  1.2777e-02, -1.0386e-03,  1.7758e-02,\n",
       "                      -1.0518e-02, -1.4061e-02,  2.2273e-02, -1.5719e-02, -1.8819e-03,\n",
       "                      -2.9526e-02, -3.2525e-04,  1.4973e-02, -7.7860e-03,  1.1655e-02,\n",
       "                       8.0143e-04,  2.1321e-02, -2.8555e-02,  9.7352e-03, -1.4464e-02,\n",
       "                       1.0944e-02,  1.9139e-02,  8.4099e-03, -1.7392e-02, -4.1865e-02,\n",
       "                       4.3541e-02,  1.5703e-02, -9.2602e-03, -2.4277e-02, -1.4056e-02,\n",
       "                       1.0953e-02, -6.3854e-03,  3.4582e-02,  1.4653e-02,  3.6375e-02,\n",
       "                       1.9525e-02, -3.2263e-04,  3.0221e-02,  1.7157e-04], device='cuda:0')),\n",
       "             ('blocks.1.ln2.weight',\n",
       "              tensor([0.8817, 0.9328, 0.8902, 0.8781, 0.9389, 0.9432, 0.9126, 0.9772, 0.9200,\n",
       "                      0.8971, 0.9284, 0.9210, 0.8885, 0.8600, 0.8931, 0.8626, 0.9022, 0.9276,\n",
       "                      0.9330, 0.8979, 0.9241, 0.9230, 0.9062, 0.8830, 0.9032, 0.9085, 0.9323,\n",
       "                      0.9363, 0.9050, 0.9031, 0.9206, 0.8219, 0.8670, 0.9186, 0.8726, 0.9045,\n",
       "                      0.9203, 0.9020, 0.8688, 0.8410, 0.8688, 0.9285, 0.8926, 0.9028, 0.9047,\n",
       "                      0.9048, 0.9298, 0.9074, 0.9029, 0.8642, 0.9337, 0.9021, 0.8775, 0.9627,\n",
       "                      0.8945, 0.8640, 0.8521, 0.8857, 0.9364, 0.9069, 0.8992, 0.9369, 0.9325,\n",
       "                      0.8950, 0.9154, 0.9094, 0.9150, 0.9050, 0.9035, 0.9618, 0.8687, 0.8725,\n",
       "                      0.9151, 0.8717, 0.9309, 0.9396, 0.9165, 0.9247, 0.8943, 0.9524, 0.8967,\n",
       "                      0.9455, 0.9125, 0.8716, 0.9113, 0.9193, 0.8702, 0.9223, 0.8941, 0.8189,\n",
       "                      0.8956, 0.9573, 0.8849, 0.9164, 0.8865, 0.9200, 0.8793, 0.8895, 0.9300,\n",
       "                      0.8757, 0.8580, 0.8843, 0.9274, 0.8620, 0.8995, 0.9076, 0.8960, 0.8790,\n",
       "                      0.9301, 0.8442, 0.8758, 0.8952, 0.9174, 0.8987, 0.9576, 0.9132, 0.8459,\n",
       "                      0.8831, 0.9030, 0.8918, 0.9041, 0.9532, 0.8473, 0.9241, 0.9209, 0.8746,\n",
       "                      0.8902, 0.9026, 0.8931, 0.8460, 0.8850, 0.9260, 0.9172, 0.8967, 0.9208,\n",
       "                      0.9336, 0.8742, 0.9044, 0.9295, 0.9741, 0.9144, 0.8783, 0.9033, 0.8597,\n",
       "                      0.8991, 0.8929, 0.9590, 0.9303, 0.8694, 0.9358, 0.9385, 0.8866, 0.8683,\n",
       "                      0.9240, 0.8550, 0.9122, 0.9281, 0.9029, 0.8941, 0.8841, 0.8840, 0.9293,\n",
       "                      0.8905, 0.8419, 0.8869, 0.8871, 0.8983, 0.9176, 0.8791, 0.9338, 0.9068,\n",
       "                      0.9400, 0.8972, 0.9015, 0.8568, 0.9209, 0.8455, 0.8603, 0.9133, 0.9314,\n",
       "                      0.8885, 0.9216, 0.9023, 0.9498, 0.9059, 0.9096, 0.9239, 0.8868, 0.8860,\n",
       "                      0.9239, 0.8992, 0.9095, 0.9028, 0.9055, 0.9044, 0.8843, 0.9323, 0.9106,\n",
       "                      0.9076, 0.9341, 0.8944, 0.9055, 0.8918, 0.9158, 0.8726, 0.9240, 0.8380,\n",
       "                      0.9068, 0.8348, 0.9167, 0.9094, 0.9023, 0.8957, 0.8978, 0.9295, 0.8137,\n",
       "                      0.8858, 0.8935, 0.9019, 0.8757, 0.9339, 0.9075, 0.9460, 0.8743, 0.9059,\n",
       "                      0.9137, 0.9245, 0.9184, 0.8616, 0.9168, 0.9508, 0.8874, 0.8990, 0.8902,\n",
       "                      0.8892, 0.9233, 0.9148, 0.9073, 0.9169, 0.8780, 0.9340, 0.8958, 0.9451,\n",
       "                      0.9122, 0.9230, 0.8924, 0.9344, 0.9172, 0.8888, 0.8823, 0.8985, 0.9083,\n",
       "                      0.8901, 0.8953, 0.9306, 0.9231, 0.9538, 0.9469, 0.9304, 0.8340, 0.8756,\n",
       "                      0.8508, 0.9190, 0.8978, 0.8834, 0.9181, 0.9371, 0.9332, 0.9136, 0.8943,\n",
       "                      0.8806, 0.9192, 0.9342, 0.8756, 0.8620, 0.8737, 0.9080, 0.9170, 0.9508,\n",
       "                      0.9238, 0.9336, 0.9248, 0.8904, 0.8683, 0.9230, 0.8408, 0.8512, 0.8703,\n",
       "                      0.8962, 0.8608, 0.9163, 0.9267, 0.8967, 0.8770, 0.9234, 0.8352, 0.9382,\n",
       "                      0.9574, 0.9610, 0.8822, 0.9041, 0.8893, 0.9663, 0.8730, 0.9039, 0.8975,\n",
       "                      0.9500, 0.8520, 0.9162, 0.9071, 0.9190, 0.9170, 0.9065, 0.8535, 0.9241,\n",
       "                      0.8296, 0.9669, 0.9119, 0.9014, 0.9243, 0.8980, 0.9151, 0.8879, 0.9199,\n",
       "                      0.9032, 0.9093, 0.8950, 0.9142, 0.8963, 0.9084, 0.8823, 0.8799, 0.8749,\n",
       "                      0.9143, 0.9374, 0.8748, 0.9421, 0.8387, 0.9135, 0.8907, 0.8976, 0.8471,\n",
       "                      0.8786, 0.9185, 0.8700, 0.9290, 0.8759, 0.8952, 0.8956, 0.8700, 0.8572,\n",
       "                      0.9232, 0.9358, 0.8632, 0.9214, 0.8676, 0.8681, 0.8718, 0.9123, 0.9157,\n",
       "                      0.9420, 0.9301, 0.9044, 0.9285, 0.8806, 0.9308, 0.9191, 0.9286, 0.8922,\n",
       "                      0.8710, 0.9616, 0.8855, 0.8784, 0.8651, 0.9111, 0.8828, 0.8683, 0.9115,\n",
       "                      0.8711, 0.8977, 0.9480, 0.8978, 0.8865, 0.8873], device='cuda:0')),\n",
       "             ('blocks.1.ln2.bias',\n",
       "              tensor([-2.8626e-02,  1.8637e-02,  8.9120e-02, -7.5166e-02, -1.2811e-01,\n",
       "                       2.1575e-01, -4.5535e-02,  2.4988e-01, -5.8701e-02, -9.1439e-02,\n",
       "                      -1.2693e-01,  1.4717e-01,  1.6639e-01, -5.2381e-02,  4.5591e-02,\n",
       "                      -2.3674e-02, -1.7244e-01, -1.6238e-01, -1.4984e-01,  1.1792e-01,\n",
       "                      -1.4322e-01, -2.1715e-01, -2.4476e-01,  8.6179e-02,  4.4939e-02,\n",
       "                       1.9961e-01,  1.1518e-01, -2.1759e-01, -1.5124e-01, -8.3927e-02,\n",
       "                       1.0532e-01, -3.1363e-02, -2.6710e-03,  7.3497e-02,  6.2207e-03,\n",
       "                      -3.2257e-02, -2.1866e-01, -9.5597e-02, -1.1608e-01,  7.9032e-02,\n",
       "                      -5.5683e-02,  1.7689e-01, -1.0504e-01, -1.9371e-01,  9.9390e-02,\n",
       "                      -8.1397e-02, -1.9337e-01,  9.8280e-02,  1.2791e-01, -4.3501e-02,\n",
       "                       1.3175e-01,  1.0696e-01,  8.8176e-03, -2.7630e-01, -1.4210e-01,\n",
       "                       1.3511e-01, -1.3664e-01,  5.0413e-02, -1.7174e-01,  1.5476e-01,\n",
       "                       3.0316e-02, -1.8812e-01,  9.5952e-02,  8.0548e-02,  1.8509e-01,\n",
       "                       1.6656e-01, -1.8273e-01,  1.0151e-01,  1.7593e-01,  1.9868e-01,\n",
       "                       1.1621e-01,  3.7293e-02,  2.2246e-01, -5.1277e-02, -1.2197e-01,\n",
       "                       1.5651e-01,  3.8145e-02, -1.4377e-01, -1.5598e-01, -4.0458e-02,\n",
       "                       1.5307e-01,  1.0829e-01, -1.3218e-01,  1.8489e-03, -1.6572e-01,\n",
       "                      -1.5534e-01,  1.7899e-03,  7.1886e-02,  1.1432e-01, -1.9695e-02,\n",
       "                      -2.0158e-02,  1.9777e-01,  1.1035e-01, -1.4060e-01,  1.0339e-01,\n",
       "                       6.7067e-02, -1.2294e-01, -6.5009e-02, -1.9909e-01, -4.3830e-02,\n",
       "                      -7.2293e-02, -3.2412e-02,  2.2935e-01,  1.3276e-01,  1.9203e-01,\n",
       "                      -1.3053e-01,  5.3289e-02,  1.1606e-01,  1.2982e-01, -1.0030e-01,\n",
       "                       9.4416e-02,  1.4759e-01, -9.7037e-02,  7.7387e-02, -2.4634e-01,\n",
       "                       7.7444e-02,  1.0709e-01,  1.5205e-01,  1.3308e-02,  8.5206e-04,\n",
       "                       1.1600e-01, -2.0681e-01,  2.5923e-02,  1.5557e-01,  1.7387e-01,\n",
       "                       2.4670e-02,  9.8726e-02,  1.8159e-01, -1.7717e-01, -4.7520e-02,\n",
       "                       2.8613e-03,  1.2716e-01,  1.2559e-01,  1.4350e-01, -1.4015e-01,\n",
       "                       1.8435e-01, -1.0588e-01, -1.5367e-01, -1.4485e-01, -2.1296e-01,\n",
       "                      -4.8620e-02,  1.3690e-01, -1.3700e-01, -7.0042e-03, -1.7739e-01,\n",
       "                       1.5755e-01, -2.3015e-01,  1.8718e-01,  4.1661e-02, -6.6311e-02,\n",
       "                      -9.7048e-02, -1.3265e-01, -2.6045e-02,  2.0602e-01,  6.9210e-02,\n",
       "                      -2.2445e-01,  1.5086e-01, -1.2663e-01, -8.6768e-02, -1.8938e-01,\n",
       "                      -5.8293e-02,  1.7090e-01, -1.0157e-01,  1.4477e-02,  1.6351e-01,\n",
       "                       7.1206e-02, -1.2009e-01,  1.7261e-01, -1.0093e-01,  1.6363e-01,\n",
       "                       2.0387e-01, -1.6120e-01,  4.0671e-02,  1.0159e-01, -1.2259e-04,\n",
       "                      -1.0817e-01,  1.3515e-01,  4.5520e-02,  1.0803e-01, -1.5501e-01,\n",
       "                      -9.6561e-02,  3.5993e-02,  3.8205e-02,  1.9298e-01, -5.8219e-02,\n",
       "                      -6.4429e-03, -1.8877e-01,  1.4727e-01,  5.1129e-02, -1.3807e-01,\n",
       "                      -4.5173e-02, -1.9446e-01, -1.9289e-01,  5.7563e-02, -2.1401e-01,\n",
       "                       9.0820e-02, -1.5272e-01, -1.9937e-01, -7.7440e-03,  2.3513e-01,\n",
       "                       8.3617e-02,  1.5933e-01,  4.6224e-02, -3.3189e-02,  5.6028e-02,\n",
       "                      -1.6256e-01,  1.6864e-02, -9.8133e-02,  1.5609e-01, -9.6878e-02,\n",
       "                       1.8846e-01, -1.9652e-01, -2.3760e-01,  4.8772e-02, -1.9971e-01,\n",
       "                      -1.4632e-02,  4.0456e-02, -2.0108e-02,  7.3525e-02,  3.6701e-02,\n",
       "                       2.2601e-01, -9.9848e-02, -1.4117e-01,  7.4654e-03, -4.0928e-02,\n",
       "                      -9.2760e-02,  1.3053e-01, -1.7374e-01,  4.5270e-02, -5.8359e-02,\n",
       "                      -2.8664e-01,  1.3350e-03, -8.9715e-02, -1.6985e-02,  1.2813e-01,\n",
       "                      -2.4349e-01, -8.9507e-02,  4.9628e-02,  1.3289e-01,  1.0116e-01,\n",
       "                      -2.1096e-01,  1.3872e-01, -2.5011e-01,  1.0258e-01,  1.6721e-01,\n",
       "                       1.3047e-01, -2.1900e-01, -1.0766e-01, -3.0100e-02,  1.4232e-01,\n",
       "                       1.3376e-01,  7.6268e-02,  6.4236e-02, -5.7578e-02, -1.3322e-01,\n",
       "                      -1.8820e-01, -2.4115e-01,  1.0645e-01,  1.2695e-01, -1.1123e-01,\n",
       "                       1.1486e-01,  3.9906e-02, -1.1002e-01, -1.8698e-01,  4.5649e-02,\n",
       "                       8.8373e-02, -2.1471e-01,  1.6955e-01, -2.2324e-01,  1.7976e-01,\n",
       "                       2.3523e-02,  1.8272e-01, -2.2381e-01,  1.0161e-01, -7.0268e-02,\n",
       "                      -6.5679e-02,  1.9009e-01, -2.1218e-02, -2.0830e-01, -1.6768e-01,\n",
       "                      -1.2479e-01, -1.4298e-01, -5.8269e-02,  2.9970e-02,  9.3893e-02,\n",
       "                      -2.0726e-02, -1.4360e-01, -1.5646e-02, -1.9893e-01,  4.0328e-02,\n",
       "                      -1.8435e-02,  1.4334e-01,  1.3057e-01,  7.0308e-02, -1.6357e-01,\n",
       "                       2.4065e-02, -1.2807e-01,  2.6066e-01,  2.3127e-01,  8.5127e-02,\n",
       "                      -4.1177e-02, -1.1963e-01, -1.8915e-01,  1.5592e-01,  1.7207e-01,\n",
       "                      -1.2371e-01,  1.2927e-01, -2.8185e-03,  9.7632e-02, -9.3285e-02,\n",
       "                       1.9689e-01, -2.0546e-01,  1.8616e-01, -2.8017e-02,  6.5173e-02,\n",
       "                       5.3016e-02,  2.2545e-01, -2.2275e-01, -6.2648e-02, -7.7321e-02,\n",
       "                       1.8507e-02,  2.1137e-01,  2.3271e-02,  7.4062e-02,  9.1369e-02,\n",
       "                       1.2057e-01, -1.3273e-01,  1.0614e-01, -6.6571e-02,  1.7838e-01,\n",
       "                       1.7870e-01, -1.0601e-01, -4.9521e-02,  1.0700e-01,  6.1938e-03,\n",
       "                      -3.5740e-02, -1.2205e-01,  1.0856e-02, -1.0166e-01,  9.7046e-02,\n",
       "                      -1.4717e-01,  5.0146e-02,  1.0858e-01,  1.4038e-01, -1.3986e-01,\n",
       "                       1.3140e-01, -1.0456e-01, -2.2237e-02, -1.5324e-01, -6.2650e-03,\n",
       "                      -9.6500e-02, -1.4987e-01,  2.3194e-01, -6.7219e-02,  3.4794e-02,\n",
       "                       1.2532e-01,  1.1392e-01, -1.5000e-01, -1.1053e-01, -2.3827e-01,\n",
       "                       1.9561e-01,  1.6012e-01,  6.6685e-02,  1.5159e-01,  7.7103e-02,\n",
       "                      -1.1031e-01, -7.5755e-02, -2.3149e-01, -2.0703e-01, -1.3074e-01,\n",
       "                       2.8067e-01, -1.0021e-01,  1.3774e-01,  1.1956e-02, -1.1899e-01,\n",
       "                       5.6655e-02,  7.6342e-02,  1.6661e-01,  8.3560e-02, -7.5871e-02,\n",
       "                       2.4868e-01,  2.3690e-01,  9.7096e-02, -9.8275e-02], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.0.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.0.key.weight',\n",
       "              tensor([[-0.0022,  0.0025, -0.0329,  ...,  0.0190,  0.0066, -0.0078],\n",
       "                      [-0.0267, -0.0018,  0.0088,  ...,  0.0399,  0.0067,  0.0756],\n",
       "                      [-0.0276,  0.0147, -0.0471,  ..., -0.0020, -0.0390, -0.0113],\n",
       "                      ...,\n",
       "                      [ 0.0150, -0.0017, -0.0541,  ..., -0.0117,  0.0258,  0.0265],\n",
       "                      [ 0.0661,  0.0101, -0.0101,  ..., -0.0370, -0.0664,  0.0065],\n",
       "                      [ 0.0224, -0.0154, -0.0020,  ...,  0.0468,  0.0279,  0.0364]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.0.query.weight',\n",
       "              tensor([[-0.0464,  0.0130,  0.0836,  ..., -0.0493,  0.0047,  0.0055],\n",
       "                      [ 0.0210, -0.0239,  0.0017,  ..., -0.0142, -0.0060,  0.0333],\n",
       "                      [-0.0387,  0.0555, -0.0253,  ..., -0.0444, -0.0259, -0.0475],\n",
       "                      ...,\n",
       "                      [-0.0228,  0.0095, -0.0171,  ...,  0.0675, -0.0198,  0.0187],\n",
       "                      [ 0.0354,  0.0601,  0.0216,  ..., -0.0227, -0.0060, -0.0228],\n",
       "                      [-0.0364,  0.0202,  0.0007,  ...,  0.0039, -0.0286, -0.0019]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.0.value.weight',\n",
       "              tensor([[-2.9007e-02,  1.7710e-02, -1.8665e-02,  ...,  3.4192e-03,\n",
       "                       -2.3228e-02, -1.3199e-02],\n",
       "                      [ 2.4895e-03,  3.7461e-02,  1.3416e-04,  ..., -1.6793e-02,\n",
       "                        1.3845e-03, -3.8324e-02],\n",
       "                      [-9.3056e-03,  1.4957e-02, -2.6391e-02,  ..., -3.7773e-05,\n",
       "                        3.4727e-02, -3.2563e-02],\n",
       "                      ...,\n",
       "                      [ 1.2060e-02, -1.6137e-02,  2.2342e-02,  ..., -8.9394e-03,\n",
       "                       -2.1814e-02, -3.2045e-03],\n",
       "                      [-5.0967e-02, -3.7303e-02,  7.9169e-03,  ...,  3.2527e-03,\n",
       "                        8.3725e-03,  2.9375e-02],\n",
       "                      [-2.2549e-03, -5.7941e-02, -2.9735e-02,  ...,  3.2911e-02,\n",
       "                        1.9008e-02, -3.7334e-02]], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.1.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.1.key.weight',\n",
       "              tensor([[ 0.0132, -0.0004, -0.0192,  ..., -0.0313,  0.0114,  0.0066],\n",
       "                      [ 0.0283,  0.0309, -0.0027,  ...,  0.0169, -0.0329, -0.0253],\n",
       "                      [-0.0193,  0.0545,  0.0230,  ...,  0.0875,  0.0217,  0.0794],\n",
       "                      ...,\n",
       "                      [-0.0443, -0.0278, -0.0014,  ...,  0.0238,  0.0113,  0.0592],\n",
       "                      [-0.0582,  0.0114, -0.0296,  ...,  0.0616,  0.0108,  0.0221],\n",
       "                      [ 0.0560, -0.0323, -0.0060,  ...,  0.0058,  0.0345, -0.0615]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.1.query.weight',\n",
       "              tensor([[-0.0412, -0.0466, -0.0382,  ...,  0.0268, -0.0176,  0.0195],\n",
       "                      [-0.0006,  0.0219,  0.0543,  ...,  0.0547,  0.0695, -0.0347],\n",
       "                      [-0.0564,  0.0773, -0.0455,  ..., -0.0017,  0.0204, -0.0124],\n",
       "                      ...,\n",
       "                      [ 0.0246,  0.0460,  0.0182,  ..., -0.0309, -0.0586, -0.0452],\n",
       "                      [ 0.0247, -0.0673,  0.0197,  ..., -0.0533, -0.0574,  0.0142],\n",
       "                      [-0.0309, -0.0261, -0.0064,  ...,  0.0138,  0.0717,  0.0034]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.1.value.weight',\n",
       "              tensor([[ 0.0454,  0.0053,  0.0104,  ..., -0.0146, -0.0089, -0.0282],\n",
       "                      [ 0.0008,  0.0313, -0.0026,  ...,  0.0159, -0.0297,  0.0017],\n",
       "                      [ 0.0282, -0.0035, -0.0149,  ...,  0.0016,  0.0119, -0.0367],\n",
       "                      ...,\n",
       "                      [ 0.0155, -0.0235,  0.0528,  ..., -0.0288, -0.0267,  0.0349],\n",
       "                      [ 0.0319,  0.0239, -0.0005,  ...,  0.0072, -0.0013,  0.0218],\n",
       "                      [ 0.0219,  0.0026, -0.0018,  ..., -0.0088,  0.0130, -0.0310]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.2.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.2.key.weight',\n",
       "              tensor([[ 0.0534,  0.0087, -0.0235,  ...,  0.0118, -0.0142, -0.0066],\n",
       "                      [ 0.0079, -0.0598,  0.0710,  ...,  0.0319,  0.0774,  0.0557],\n",
       "                      [-0.0243, -0.0621, -0.0589,  ..., -0.0189,  0.0320,  0.0168],\n",
       "                      ...,\n",
       "                      [ 0.0016,  0.0543, -0.0535,  ...,  0.0148,  0.0058, -0.0171],\n",
       "                      [-0.0069, -0.0477, -0.0006,  ..., -0.0314, -0.0017,  0.0284],\n",
       "                      [ 0.0214, -0.0511, -0.0383,  ...,  0.0419,  0.0085, -0.0232]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.2.query.weight',\n",
       "              tensor([[-0.0292,  0.0061, -0.0558,  ..., -0.0343,  0.0341,  0.0197],\n",
       "                      [ 0.0202, -0.0158, -0.0191,  ..., -0.0237, -0.0109, -0.0237],\n",
       "                      [ 0.0124,  0.0317, -0.0061,  ..., -0.0463, -0.0172,  0.0664],\n",
       "                      ...,\n",
       "                      [-0.0010,  0.0268, -0.0143,  ..., -0.0116,  0.0215, -0.0112],\n",
       "                      [-0.0097, -0.0295, -0.0200,  ..., -0.0130, -0.0717,  0.0010],\n",
       "                      [-0.0156,  0.0271, -0.0469,  ..., -0.0213, -0.0150,  0.0324]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.2.value.weight',\n",
       "              tensor([[ 0.0312,  0.0269,  0.0435,  ..., -0.0120,  0.0129, -0.0073],\n",
       "                      [ 0.0043,  0.0129,  0.0058,  ..., -0.0111, -0.0270, -0.0196],\n",
       "                      [-0.0360,  0.0054, -0.0146,  ...,  0.0193, -0.0100,  0.0159],\n",
       "                      ...,\n",
       "                      [ 0.0013,  0.0125,  0.0342,  ..., -0.0136, -0.0115,  0.0342],\n",
       "                      [ 0.0100, -0.0343,  0.0263,  ...,  0.0179, -0.0249, -0.0045],\n",
       "                      [ 0.0024,  0.0278, -0.0130,  ...,  0.0126,  0.0073, -0.0106]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.3.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.3.key.weight',\n",
       "              tensor([[ 0.0295,  0.0483, -0.0383,  ..., -0.0691, -0.0244, -0.0834],\n",
       "                      [-0.0385,  0.0184,  0.0291,  ...,  0.0615,  0.0687,  0.0309],\n",
       "                      [ 0.0280,  0.0322, -0.0071,  ...,  0.0051, -0.0186,  0.0036],\n",
       "                      ...,\n",
       "                      [ 0.0049, -0.0709, -0.0060,  ..., -0.0337,  0.0191, -0.0740],\n",
       "                      [ 0.0131, -0.0584,  0.0173,  ..., -0.0014,  0.0174, -0.0269],\n",
       "                      [-0.0120,  0.0534, -0.0409,  ...,  0.0232, -0.0590,  0.0176]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.3.query.weight',\n",
       "              tensor([[ 0.0463, -0.0466,  0.0604,  ..., -0.0217,  0.0065,  0.0374],\n",
       "                      [-0.0150, -0.0433, -0.0220,  ...,  0.0505, -0.0062, -0.0285],\n",
       "                      [-0.0390,  0.0239, -0.0341,  ...,  0.0265, -0.0133, -0.0189],\n",
       "                      ...,\n",
       "                      [-0.0114, -0.0147, -0.0511,  ..., -0.0183, -0.0130,  0.0434],\n",
       "                      [-0.0196, -0.0384, -0.0318,  ...,  0.0403, -0.0200,  0.0069],\n",
       "                      [-0.0287,  0.0538,  0.0622,  ...,  0.0275,  0.0653,  0.0165]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.3.value.weight',\n",
       "              tensor([[ 4.1907e-02,  4.1781e-02, -1.9539e-02,  ...,  4.4119e-03,\n",
       "                       -7.5601e-03, -1.3536e-02],\n",
       "                      [ 2.3209e-02, -9.2784e-05,  2.2281e-02,  ...,  3.5673e-02,\n",
       "                       -1.1776e-02, -2.8673e-02],\n",
       "                      [-3.5307e-03, -1.6777e-04, -7.9552e-03,  ..., -3.4590e-02,\n",
       "                       -2.0485e-02,  3.1117e-02],\n",
       "                      ...,\n",
       "                      [-6.7233e-03,  1.3114e-02,  2.2497e-03,  ...,  2.6380e-02,\n",
       "                        1.6610e-02, -2.8007e-02],\n",
       "                      [ 4.4085e-02,  1.2068e-02,  5.9305e-04,  ...,  1.0329e-02,\n",
       "                       -9.0012e-03, -3.3465e-02],\n",
       "                      [-3.9165e-05, -2.6710e-02, -6.7493e-03,  ...,  1.1657e-03,\n",
       "                       -3.5585e-03,  2.0498e-02]], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.4.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.4.key.weight',\n",
       "              tensor([[-0.0475, -0.0310, -0.0154,  ..., -0.0145,  0.0269,  0.0509],\n",
       "                      [ 0.0018, -0.0051,  0.0235,  ..., -0.0255,  0.0251, -0.0040],\n",
       "                      [-0.0069, -0.0734, -0.0451,  ...,  0.0287,  0.0578,  0.0083],\n",
       "                      ...,\n",
       "                      [-0.0008, -0.0655,  0.0415,  ..., -0.0018, -0.0413, -0.0164],\n",
       "                      [ 0.0275,  0.0205,  0.0568,  ...,  0.0011, -0.0320, -0.0661],\n",
       "                      [-0.0082,  0.0423,  0.0041,  ...,  0.0466,  0.0012,  0.0340]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.4.query.weight',\n",
       "              tensor([[-0.0064, -0.0165, -0.0061,  ..., -0.0063, -0.0283,  0.0003],\n",
       "                      [ 0.0010, -0.0111, -0.0823,  ..., -0.0120, -0.0091, -0.0244],\n",
       "                      [-0.0221,  0.0059,  0.0027,  ..., -0.0704, -0.0523,  0.0420],\n",
       "                      ...,\n",
       "                      [ 0.0255,  0.0343,  0.0127,  ...,  0.0430, -0.0008, -0.0516],\n",
       "                      [-0.0636, -0.0733, -0.0477,  ...,  0.0554,  0.0405, -0.0099],\n",
       "                      [-0.0048, -0.0569,  0.0098,  ...,  0.0120, -0.0668,  0.0311]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.4.value.weight',\n",
       "              tensor([[ 0.0045, -0.0189,  0.0096,  ...,  0.0143, -0.0250,  0.0147],\n",
       "                      [ 0.0346, -0.0057, -0.0029,  ...,  0.0251,  0.0055,  0.0194],\n",
       "                      [ 0.0181,  0.0337,  0.0089,  ...,  0.0126, -0.0259,  0.0206],\n",
       "                      ...,\n",
       "                      [ 0.0137,  0.0064, -0.0466,  ..., -0.0380,  0.0130,  0.0285],\n",
       "                      [-0.0044, -0.0079,  0.0274,  ...,  0.0176, -0.0177, -0.0395],\n",
       "                      [ 0.0100,  0.0202, -0.0309,  ...,  0.0057,  0.0141, -0.0088]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.5.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.5.key.weight',\n",
       "              tensor([[ 0.0128,  0.0099, -0.0460,  ..., -0.0653,  0.0251,  0.0154],\n",
       "                      [ 0.0825,  0.0240,  0.0232,  ...,  0.0177, -0.0198, -0.0144],\n",
       "                      [-0.0270, -0.0469,  0.0016,  ..., -0.0019,  0.0347,  0.0258],\n",
       "                      ...,\n",
       "                      [ 0.0288,  0.0282,  0.0229,  ...,  0.0073,  0.0006, -0.0185],\n",
       "                      [ 0.0158, -0.0472, -0.0365,  ..., -0.0539,  0.0536, -0.0409],\n",
       "                      [ 0.0019,  0.0195, -0.0086,  ..., -0.0347,  0.0511,  0.0441]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.5.query.weight',\n",
       "              tensor([[ 0.0216, -0.0215, -0.0442,  ..., -0.0360,  0.0328, -0.0469],\n",
       "                      [-0.0426,  0.0251,  0.0010,  ...,  0.0047,  0.0331, -0.0007],\n",
       "                      [ 0.0592, -0.0083, -0.0511,  ..., -0.0215, -0.0051, -0.0262],\n",
       "                      ...,\n",
       "                      [-0.0608,  0.0415, -0.0101,  ...,  0.0480,  0.0372, -0.0188],\n",
       "                      [ 0.0025, -0.0105, -0.0687,  ..., -0.0324, -0.0058, -0.0648],\n",
       "                      [ 0.0152,  0.0207, -0.0674,  ...,  0.0259,  0.0206, -0.0008]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.5.value.weight',\n",
       "              tensor([[-0.0450, -0.0183, -0.0085,  ..., -0.0019, -0.0195,  0.0051],\n",
       "                      [ 0.0271,  0.0043, -0.0380,  ...,  0.0034, -0.0135,  0.0013],\n",
       "                      [ 0.0330,  0.0123,  0.0072,  ..., -0.0242,  0.0071, -0.0158],\n",
       "                      ...,\n",
       "                      [ 0.0253,  0.0360, -0.0354,  ...,  0.0222, -0.0075, -0.0076],\n",
       "                      [-0.0147,  0.0232,  0.0089,  ..., -0.0119, -0.0079,  0.0146],\n",
       "                      [-0.0041, -0.0074,  0.0020,  ...,  0.0138,  0.0066,  0.0492]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.proj.weight',\n",
       "              tensor([[-0.0109,  0.0579,  0.0349,  ..., -0.0012, -0.0205, -0.0023],\n",
       "                      [ 0.0030, -0.0091, -0.0217,  ...,  0.0154,  0.0119,  0.0199],\n",
       "                      [-0.0250,  0.0081, -0.0135,  ..., -0.0040,  0.0019,  0.0285],\n",
       "                      ...,\n",
       "                      [ 0.0135,  0.0152, -0.0317,  ...,  0.0277, -0.0199,  0.0198],\n",
       "                      [ 0.0129, -0.0218,  0.0015,  ...,  0.0227, -0.0044,  0.0081],\n",
       "                      [ 0.0103, -0.0109, -0.0369,  ...,  0.0144, -0.0041,  0.0048]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.proj.bias',\n",
       "              tensor([ 1.0845e-03,  1.1160e-02,  1.2399e-02, -5.9338e-03, -8.8516e-03,\n",
       "                       1.5872e-02,  2.6283e-03,  1.7752e-02, -1.4203e-02, -1.1015e-02,\n",
       "                      -8.8869e-03,  1.0153e-02,  1.5737e-02, -1.1131e-02,  1.3342e-02,\n",
       "                       1.4274e-02, -1.0988e-02, -7.8731e-03, -1.3792e-02,  1.3422e-02,\n",
       "                      -1.7704e-02, -1.4286e-02, -1.6304e-02,  1.0046e-02,  8.1108e-03,\n",
       "                       2.1286e-02,  1.6756e-02, -1.3792e-02, -1.4197e-02, -1.0147e-02,\n",
       "                       1.8243e-02,  8.8682e-04, -1.2039e-05,  1.7795e-02,  1.0929e-02,\n",
       "                       1.0575e-02, -9.9302e-03, -1.3514e-02, -1.8277e-02, -4.5858e-03,\n",
       "                      -1.0164e-02,  1.4644e-02, -7.3962e-03, -2.1103e-02,  1.6376e-02,\n",
       "                      -2.0735e-02, -1.1059e-02,  4.3015e-03,  1.6848e-02, -1.2878e-02,\n",
       "                       1.1401e-02,  1.4237e-02,  6.6902e-04, -2.1165e-02, -1.8025e-02,\n",
       "                       1.3770e-02, -1.6300e-02,  1.3867e-02, -1.3780e-02,  1.3427e-02,\n",
       "                      -5.4629e-03, -1.4105e-02,  1.1702e-02,  5.2825e-03,  1.4331e-02,\n",
       "                       1.1467e-02, -1.2641e-02,  1.4681e-02,  1.0136e-02,  1.3044e-02,\n",
       "                       1.4335e-02,  5.3199e-03,  1.9320e-02, -1.5017e-02, -1.1586e-02,\n",
       "                       2.0953e-02,  6.2001e-03, -1.2548e-02, -6.0921e-03, -1.0071e-02,\n",
       "                       1.4002e-02,  1.8705e-02, -1.4619e-02, -8.0784e-03, -1.6613e-02,\n",
       "                      -1.5384e-02,  1.3647e-02,  1.2514e-02,  1.1152e-02,  4.1647e-03,\n",
       "                       1.5661e-02,  1.1699e-02,  9.5742e-03, -3.5506e-03,  7.0995e-03,\n",
       "                       9.9845e-03, -1.5513e-02, -1.6727e-02, -1.8870e-02, -1.0568e-02,\n",
       "                      -1.2988e-02,  5.2825e-03,  1.3666e-02,  1.6381e-02,  1.3528e-02,\n",
       "                      -1.1330e-02,  1.4567e-02,  1.5307e-02,  1.5596e-02, -1.0326e-02,\n",
       "                      -1.4143e-04,  1.5327e-02, -9.6421e-03,  1.5486e-02, -2.3426e-02,\n",
       "                       1.1502e-02,  8.8445e-03,  4.5891e-03,  7.8361e-03, -9.8360e-03,\n",
       "                       9.1391e-03, -2.5301e-02, -3.4786e-04,  1.2510e-02,  1.5422e-02,\n",
       "                       5.1235e-03,  1.1638e-02,  1.5598e-02, -2.2088e-02, -1.7188e-02,\n",
       "                       4.8126e-03,  1.8127e-02,  1.3471e-02,  1.2003e-02, -1.2299e-02,\n",
       "                       1.1891e-02, -5.1830e-03, -1.0440e-02, -2.3761e-02, -2.2231e-02,\n",
       "                      -1.3159e-02,  1.9022e-02, -1.6875e-02, -8.6719e-03, -7.9248e-03,\n",
       "                       1.2206e-02, -2.4944e-02,  2.0275e-02,  4.7236e-03, -1.2428e-02,\n",
       "                      -1.5158e-02, -8.4163e-03, -1.6875e-03,  1.1543e-02,  9.4199e-03,\n",
       "                      -1.0105e-02,  1.3758e-02, -2.2237e-02, -4.5105e-03, -1.4217e-02,\n",
       "                      -9.4663e-03,  1.2942e-02, -1.4761e-02,  4.9245e-04,  1.6373e-02,\n",
       "                      -3.4860e-03, -1.0828e-02,  1.7613e-02, -1.4079e-02,  1.3271e-02,\n",
       "                       8.4458e-03, -1.4345e-02,  4.2487e-03,  1.7611e-02, -6.3744e-03,\n",
       "                      -1.3289e-02,  1.5303e-02,  2.1565e-03,  1.1772e-02, -1.2570e-02,\n",
       "                      -7.2552e-03,  1.1287e-02, -4.0741e-03,  2.9601e-02, -1.3164e-02,\n",
       "                      -5.4010e-03, -1.4459e-02,  1.0615e-02,  8.5231e-03, -1.8175e-02,\n",
       "                      -8.6613e-03, -1.0870e-02, -9.8002e-03,  1.1173e-02, -1.5004e-02,\n",
       "                       5.2384e-03, -1.7171e-02, -6.5597e-03, -1.5861e-03,  1.5510e-02,\n",
       "                       7.9505e-03,  1.0171e-02,  1.0960e-02, -1.5861e-02,  2.3441e-03,\n",
       "                      -1.6561e-02, -6.0074e-03, -1.0906e-02,  3.1748e-03, -8.6219e-03,\n",
       "                       5.3308e-03, -9.8208e-03, -2.3532e-02,  4.9272e-03, -1.2534e-02,\n",
       "                      -3.6098e-03,  1.4834e-02, -8.4190e-03,  1.6268e-02,  8.6420e-03,\n",
       "                       1.4989e-02, -9.4134e-03, -1.4401e-02, -4.6812e-03, -1.0487e-02,\n",
       "                      -1.5522e-02,  9.9565e-03, -1.9082e-02,  1.1247e-02, -5.3588e-03,\n",
       "                      -2.2289e-02,  9.1850e-03, -8.6447e-03,  1.0922e-02,  7.2290e-03,\n",
       "                      -1.4358e-02, -1.3999e-02,  8.8626e-03,  1.1960e-02, -6.9400e-03,\n",
       "                      -2.1697e-02,  9.7619e-03, -1.9186e-02,  2.2300e-02,  1.7233e-02,\n",
       "                       8.3992e-03, -1.5138e-02, -1.6309e-02,  8.9601e-03,  1.1050e-02,\n",
       "                       1.4109e-02, -2.1529e-03,  8.1294e-03, -1.0719e-02, -9.0448e-03,\n",
       "                      -2.2908e-02, -2.0753e-02,  1.4828e-02,  8.5465e-03, -6.6111e-03,\n",
       "                       4.9669e-03,  1.2290e-02, -1.2450e-02, -1.7900e-02,  1.2015e-02,\n",
       "                       2.0359e-02, -1.6284e-02,  1.2089e-02, -1.8649e-02,  1.5440e-02,\n",
       "                       8.9142e-03,  1.6183e-02, -1.4647e-02,  1.3450e-02, -9.2353e-03,\n",
       "                      -9.9485e-03,  5.7130e-03,  6.1623e-03, -1.3812e-02, -1.1490e-02,\n",
       "                      -9.7924e-03, -1.2494e-02, -9.2297e-03, -8.8271e-03,  1.1258e-02,\n",
       "                      -2.2807e-03, -1.1951e-02, -7.7067e-03, -1.8580e-02,  1.3066e-02,\n",
       "                       9.6152e-03,  1.8822e-02,  1.4204e-02,  1.1560e-02, -2.0312e-02,\n",
       "                       6.4873e-03, -2.0597e-02,  1.8984e-02,  1.7399e-02,  8.7567e-03,\n",
       "                      -1.4449e-02, -6.0479e-03, -1.5931e-02,  1.3159e-02,  1.3788e-02,\n",
       "                      -2.1706e-02,  1.6393e-02, -6.6090e-03,  1.2580e-02, -6.2918e-03,\n",
       "                       2.1077e-02, -1.6473e-02,  1.1650e-02,  1.2444e-03,  1.2505e-02,\n",
       "                       6.2985e-03,  2.2151e-02, -1.4579e-02, -1.3471e-02, -2.2004e-02,\n",
       "                       4.1171e-03,  1.1387e-02,  5.7586e-03,  1.5247e-02,  7.2999e-03,\n",
       "                       1.1410e-02, -1.0033e-02,  6.1140e-03, -1.7924e-02,  1.0406e-02,\n",
       "                       2.0613e-02, -1.8513e-03, -1.8803e-02,  8.5632e-03, -9.7433e-03,\n",
       "                      -7.3329e-03, -1.1678e-02, -5.3977e-03, -1.4395e-02,  1.2251e-02,\n",
       "                      -1.5185e-02,  1.1638e-02,  5.0978e-03,  1.3791e-02, -1.9154e-02,\n",
       "                       1.4186e-02, -1.5114e-02, -3.5200e-03, -1.1721e-02, -3.7676e-03,\n",
       "                       1.0146e-02,  4.9290e-03,  1.3007e-02, -9.9453e-03,  4.8123e-03,\n",
       "                       1.0086e-02,  1.3997e-02, -1.6512e-02, -1.0935e-02, -1.3667e-02,\n",
       "                       2.1438e-02,  7.7701e-03,  1.2289e-02,  1.9674e-02,  8.2701e-03,\n",
       "                      -1.2843e-02, -2.0863e-02, -1.3058e-02, -1.3215e-02, -1.1796e-02,\n",
       "                       2.2810e-02, -2.5371e-03,  1.8876e-02,  9.8621e-03, -1.2239e-02,\n",
       "                       1.4286e-02,  3.4973e-03,  1.9121e-02,  8.8819e-03, -3.7391e-03,\n",
       "                       2.0752e-02,  1.3640e-02,  1.3581e-02, -6.8634e-03], device='cuda:0')),\n",
       "             ('blocks.2.ffwd.net.0.weight',\n",
       "              tensor([[ 0.0162, -0.0369, -0.0782,  ..., -0.0189, -0.0368, -0.0154],\n",
       "                      [ 0.0126,  0.0131, -0.0023,  ..., -0.0233,  0.0010,  0.0229],\n",
       "                      [ 0.0074,  0.0190, -0.0002,  ..., -0.0499,  0.0330, -0.0195],\n",
       "                      ...,\n",
       "                      [ 0.0089, -0.0653,  0.0116,  ..., -0.0699, -0.0035, -0.0560],\n",
       "                      [ 0.0228, -0.0771, -0.0240,  ..., -0.0316, -0.0158,  0.0251],\n",
       "                      [-0.0022, -0.0215,  0.0293,  ..., -0.0047, -0.0318, -0.0024]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.ffwd.net.0.bias',\n",
       "              tensor([-0.0173, -0.0191, -0.0262,  ..., -0.0171, -0.0236, -0.0275],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.ffwd.net.2.weight',\n",
       "              tensor([[ 0.0113,  0.0258, -0.0341,  ..., -0.0102,  0.0152, -0.0051],\n",
       "                      [-0.0062,  0.0216, -0.0041,  ...,  0.0080, -0.0331, -0.0099],\n",
       "                      [-0.0200, -0.0561,  0.0441,  ..., -0.0265, -0.0084, -0.0462],\n",
       "                      ...,\n",
       "                      [ 0.0277,  0.0005,  0.0186,  ...,  0.0647,  0.0160, -0.0368],\n",
       "                      [ 0.0005, -0.0143, -0.0414,  ...,  0.0173,  0.0281, -0.0163],\n",
       "                      [-0.0384,  0.0335, -0.0409,  ...,  0.0185, -0.0567,  0.0047]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.ffwd.net.2.bias',\n",
       "              tensor([ 7.2263e-03, -1.2050e-03, -1.4222e-02,  5.8347e-03,  7.6585e-03,\n",
       "                      -1.9068e-02,  1.7212e-03, -1.4132e-02,  7.9382e-03,  6.8122e-03,\n",
       "                       5.7419e-03, -9.6683e-03, -6.5389e-03,  1.2539e-03, -4.7260e-03,\n",
       "                      -5.5487e-03,  5.5774e-03,  2.1296e-02,  4.7755e-03, -1.0666e-02,\n",
       "                       1.4870e-02,  5.2318e-03,  6.1196e-03, -2.8256e-03, -6.8523e-03,\n",
       "                      -1.9187e-02, -1.9870e-02,  1.3947e-02,  1.1145e-02, -7.2018e-03,\n",
       "                      -4.6512e-03,  6.0727e-03,  4.7005e-03, -8.1160e-03, -5.0492e-04,\n",
       "                       2.4894e-03,  1.4605e-02,  3.8557e-03,  1.1192e-02,  5.1432e-03,\n",
       "                       1.1851e-02, -7.1174e-03,  6.8182e-06,  1.6411e-02, -5.2055e-03,\n",
       "                       1.1270e-02,  1.2413e-02,  2.7751e-04, -3.5618e-03, -1.8203e-03,\n",
       "                      -6.7367e-03, -8.4372e-03, -2.2825e-03,  2.1990e-02,  9.6125e-03,\n",
       "                      -6.9758e-03, -7.5215e-04, -3.4460e-03,  1.3315e-02, -2.7553e-03,\n",
       "                       3.9856e-03,  1.4357e-02, -5.7609e-03,  6.2686e-03,  6.3843e-03,\n",
       "                      -7.6893e-03,  3.9062e-03, -1.6050e-02, -7.5732e-03, -2.5702e-02,\n",
       "                      -1.5977e-02, -6.1838e-03, -5.4643e-03, -3.4098e-03,  1.9227e-02,\n",
       "                      -1.7959e-02, -1.3245e-03,  1.0839e-02,  4.6158e-03,  9.3375e-03,\n",
       "                      -1.2448e-03, -7.0323e-03,  1.3816e-02, -8.2238e-03,  8.5560e-03,\n",
       "                       1.2507e-02,  2.6312e-03,  7.8513e-03, -1.0690e-02,  1.2269e-02,\n",
       "                      -4.5651e-03, -6.2273e-03, -1.1705e-02,  1.6460e-03,  3.2056e-04,\n",
       "                       3.9902e-03, -1.7740e-03, -2.0357e-03,  1.2595e-02,  6.2083e-03,\n",
       "                      -6.6278e-03, -1.4841e-03, -2.1425e-02,  4.0854e-03, -1.8244e-02,\n",
       "                       6.7312e-03, -1.2800e-03,  4.1782e-03, -1.8466e-03,  7.6139e-03,\n",
       "                      -9.3119e-03, -3.4731e-03,  1.9052e-03, -3.8338e-03,  1.6470e-02,\n",
       "                       4.6305e-03,  1.2320e-03, -1.8985e-03, -7.4929e-03, -3.7337e-03,\n",
       "                      -9.3925e-03,  2.1446e-02, -1.1916e-02, -1.2899e-02, -1.6227e-02,\n",
       "                      -5.0432e-03, -1.1059e-02, -9.5319e-03,  8.5877e-03,  1.0751e-02,\n",
       "                       7.0798e-03, -2.3278e-02, -4.1706e-03, -7.1421e-04,  9.8103e-03,\n",
       "                      -1.7196e-02, -3.0385e-03,  1.2163e-02,  1.4144e-02,  2.1368e-02,\n",
       "                       8.0707e-03,  5.0157e-03,  6.6462e-03,  6.9237e-03, -9.4794e-04,\n",
       "                      -8.0039e-03,  1.0488e-02, -5.3248e-03, -3.3110e-03, -7.5291e-03,\n",
       "                       3.9731e-03,  5.3051e-03,  8.3625e-03, -2.7394e-02,  3.9412e-05,\n",
       "                       4.6155e-03, -9.4368e-04,  8.9182e-03,  8.1080e-03,  1.2002e-02,\n",
       "                       1.8913e-02,  3.0369e-03,  1.0667e-03,  2.2887e-03, -2.6594e-03,\n",
       "                       1.2795e-02,  6.2330e-03, -5.4966e-03,  2.1046e-03, -1.6050e-02,\n",
       "                      -9.9287e-03,  1.3853e-02, -8.5154e-04, -4.5339e-03, -1.3032e-02,\n",
       "                       1.1186e-02, -1.8160e-03, -9.0122e-03, -9.6562e-03,  1.9373e-03,\n",
       "                       6.2322e-03,  1.6352e-04, -1.1924e-03, -4.4942e-03,  7.6742e-03,\n",
       "                      -2.4885e-03,  5.1438e-03, -4.6671e-03, -1.3182e-02,  6.7093e-03,\n",
       "                      -5.5425e-03,  7.5965e-03,  4.8318e-03, -1.1056e-02,  1.5933e-02,\n",
       "                      -8.4044e-03,  7.9924e-03,  1.2898e-02, -1.5727e-03, -1.3939e-02,\n",
       "                      -5.9770e-03, -1.6199e-02, -1.8245e-02,  5.7216e-03, -7.8864e-04,\n",
       "                       6.3152e-03, -1.2263e-03,  2.0870e-02, -1.0985e-02,  4.3233e-04,\n",
       "                      -7.8729e-03,  2.9919e-03,  1.2931e-02, -3.8054e-03,  1.0837e-02,\n",
       "                      -2.0158e-04, -4.8000e-03,  1.4305e-03, -1.7676e-02,  9.4808e-03,\n",
       "                      -2.2298e-02, -1.4485e-04,  1.2481e-02, -3.4213e-03, -7.1775e-04,\n",
       "                       8.7461e-03, -1.5189e-02,  2.3129e-03, -3.9899e-03, -3.2745e-05,\n",
       "                       1.4940e-02,  2.9683e-03,  7.2073e-03,  7.9460e-03, -6.4976e-04,\n",
       "                       8.5535e-03, -5.4134e-03, -5.3146e-03, -1.7152e-02, -2.5935e-03,\n",
       "                       6.1893e-03, -9.2910e-03,  2.2903e-02, -1.5124e-02, -1.2485e-02,\n",
       "                      -4.5081e-03,  1.1242e-02, -2.1343e-04, -2.9486e-03, -7.7073e-03,\n",
       "                       1.5891e-03,  1.9396e-03,  6.4897e-04,  3.2937e-03, -7.4138e-04,\n",
       "                       1.0760e-02,  3.3265e-02, -1.4032e-02, -6.1705e-03,  8.7239e-03,\n",
       "                       2.8918e-03, -4.0133e-03,  3.8023e-05,  1.4149e-02,  4.4060e-04,\n",
       "                      -2.6086e-03,  4.4722e-03, -3.5207e-03,  1.2297e-02, -7.2483e-03,\n",
       "                      -1.0976e-02, -1.0861e-02,  8.5757e-03,  5.0864e-03,  3.6515e-03,\n",
       "                       1.6649e-02, -1.7766e-04,  9.1506e-03,  5.7176e-03,  1.1984e-02,\n",
       "                       1.7537e-02,  1.1978e-02, -3.2586e-03,  5.2842e-03, -8.9309e-03,\n",
       "                      -3.3228e-03,  1.2943e-02, -4.1465e-03,  4.9072e-03, -3.0120e-03,\n",
       "                       2.6432e-03, -4.0329e-03,  1.4532e-03, -9.4165e-03,  1.3611e-02,\n",
       "                      -1.2433e-02,  9.6957e-03, -2.0917e-02, -1.9381e-02,  1.1002e-03,\n",
       "                      -2.0646e-04, -4.3308e-05,  1.3651e-02, -4.3431e-03, -1.9474e-02,\n",
       "                       1.2277e-02, -2.1282e-02, -7.5256e-04, -4.6771e-03,  1.4497e-02,\n",
       "                      -1.4470e-02,  1.0368e-02, -1.5725e-02,  1.7837e-03, -9.9725e-03,\n",
       "                      -5.3280e-04, -2.7290e-02,  1.6273e-02,  3.7943e-03,  1.4402e-02,\n",
       "                       7.2868e-03, -1.1593e-02, -1.4627e-02, -8.2356e-03, -5.8104e-03,\n",
       "                      -8.9215e-03,  7.7302e-03,  5.2695e-04,  1.8729e-02, -1.7153e-02,\n",
       "                      -3.4283e-03,  8.7882e-03, -2.9319e-03, -1.8063e-02, -9.1933e-03,\n",
       "                       3.1850e-03,  4.9246e-03, -1.2511e-02,  1.6340e-02, -3.4866e-03,\n",
       "                       7.3420e-03, -6.9158e-03,  7.4226e-03, -1.4935e-02,  9.1513e-03,\n",
       "                      -9.4972e-03,  6.0863e-03,  7.6825e-03,  1.5775e-03, -6.4954e-03,\n",
       "                      -1.1274e-02, -4.6313e-03, -1.3155e-02,  4.5808e-03, -1.6771e-03,\n",
       "                       5.2138e-04, -4.4173e-03,  3.9440e-03,  2.0519e-02,  1.1547e-02,\n",
       "                      -1.4002e-02, -5.5152e-03, -1.8633e-03, -2.9173e-02,  4.6758e-03,\n",
       "                       1.2595e-02,  2.8643e-03,  1.4246e-02,  1.1198e-02,  1.2978e-02,\n",
       "                      -2.4513e-02,  2.4091e-03,  3.1829e-04, -3.3837e-03,  1.2228e-02,\n",
       "                      -8.4172e-03,  2.4615e-03, -1.0690e-02, -8.8553e-04, -5.8923e-03,\n",
       "                      -1.6027e-02, -8.6775e-03, -9.8810e-03,  2.3443e-03], device='cuda:0')),\n",
       "             ('blocks.2.ln1.weight',\n",
       "              tensor([0.8504, 0.8338, 0.8619, 0.8592, 0.8508, 0.8933, 0.8177, 0.8871, 0.8578,\n",
       "                      0.8921, 0.8953, 0.8631, 0.8258, 0.8194, 0.8516, 0.8681, 0.8771, 0.8761,\n",
       "                      0.8835, 0.8929, 0.8925, 0.8842, 0.8736, 0.8917, 0.8589, 0.8831, 0.8741,\n",
       "                      0.8595, 0.8472, 0.8507, 0.8855, 0.8238, 0.8416, 0.8375, 0.8446, 0.8899,\n",
       "                      0.8672, 0.8553, 0.8698, 0.8509, 0.8200, 0.9018, 0.8685, 0.8852, 0.8888,\n",
       "                      0.8811, 0.8826, 0.8578, 0.8605, 0.8750, 0.8828, 0.8541, 0.8426, 0.9206,\n",
       "                      0.8454, 0.8045, 0.8505, 0.8095, 0.9478, 0.8901, 0.8425, 0.9026, 0.8724,\n",
       "                      0.8458, 0.8824, 0.8652, 0.8434, 0.8702, 0.8529, 0.8960, 0.8652, 0.9361,\n",
       "                      0.8846, 0.8329, 0.8633, 0.8805, 0.8700, 0.8574, 0.8756, 0.8549, 0.8697,\n",
       "                      0.8543, 0.8899, 0.8460, 0.8637, 0.9011, 0.8443, 0.8722, 0.8750, 0.8373,\n",
       "                      0.8554, 0.8837, 0.8737, 0.8595, 0.8628, 0.8745, 0.9237, 0.8644, 0.8819,\n",
       "                      0.8227, 0.8381, 0.8942, 0.8774, 0.8621, 0.8909, 0.8655, 0.8499, 0.8721,\n",
       "                      0.8856, 0.8591, 0.8592, 0.8488, 0.8756, 0.8418, 0.8883, 0.8511, 0.8335,\n",
       "                      0.8285, 0.8355, 0.8366, 0.8708, 0.8871, 0.8952, 0.9130, 0.8808, 0.8303,\n",
       "                      0.8579, 0.8689, 0.8937, 0.8733, 0.8566, 0.9076, 0.9041, 0.8371, 0.8652,\n",
       "                      0.8712, 0.8316, 0.8619, 0.9051, 0.8985, 0.8710, 0.8342, 0.9370, 0.8259,\n",
       "                      0.8554, 0.8832, 0.9087, 0.9079, 0.8408, 0.8704, 0.8655, 0.8533, 0.8568,\n",
       "                      0.9126, 0.8105, 0.8738, 0.8716, 0.8805, 0.8838, 0.8908, 0.8793, 0.8454,\n",
       "                      0.8537, 0.8688, 0.8490, 0.8671, 0.8598, 0.8728, 0.8271, 0.8384, 0.8912,\n",
       "                      0.8702, 0.8655, 0.8489, 0.8443, 0.8701, 0.8329, 0.8522, 0.8514, 0.9222,\n",
       "                      0.8271, 0.8234, 0.8394, 0.8655, 0.8509, 0.8433, 0.8780, 0.8617, 0.8701,\n",
       "                      0.8746, 0.8930, 0.8633, 0.8466, 0.8418, 0.8693, 0.8678, 0.8757, 0.8374,\n",
       "                      0.8413, 0.8717, 0.8495, 0.9084, 0.8433, 0.8765, 0.8418, 0.8685, 0.8468,\n",
       "                      0.8741, 0.8466, 0.8487, 0.8278, 0.8609, 0.8959, 0.8396, 0.8362, 0.8541,\n",
       "                      0.8518, 0.8410, 0.9097, 0.8378, 0.8653, 0.8447, 0.8890, 0.8980, 0.8563,\n",
       "                      0.9168, 0.8993, 0.8747, 0.8858, 0.8276, 0.9090, 0.8602, 0.8797, 0.8946,\n",
       "                      0.8417, 0.9080, 0.8650, 0.8701, 0.8671, 0.8296, 0.8643, 0.8789, 0.8984,\n",
       "                      0.8513, 0.8588, 0.8631, 0.8950, 0.9018, 0.8186, 0.8836, 0.8719, 0.8452,\n",
       "                      0.8560, 0.8344, 0.8523, 0.8980, 0.9133, 0.8762, 0.8663, 0.8262, 0.8309,\n",
       "                      0.8297, 0.8745, 0.8692, 0.8450, 0.8893, 0.8590, 0.8629, 0.8870, 0.8764,\n",
       "                      0.8432, 0.8741, 0.9020, 0.8581, 0.8690, 0.8282, 0.8734, 0.8461, 0.8931,\n",
       "                      0.8725, 0.8474, 0.8727, 0.8911, 0.9082, 0.8359, 0.8064, 0.9023, 0.8235,\n",
       "                      0.8937, 0.7992, 0.8612, 0.8563, 0.8730, 0.8262, 0.8728, 0.8862, 0.8862,\n",
       "                      0.8836, 0.9162, 0.8382, 0.8400, 0.8478, 0.9237, 0.8615, 0.8857, 0.8728,\n",
       "                      0.8752, 0.8506, 0.8679, 0.8683, 0.8798, 0.8871, 0.8986, 0.8567, 0.8330,\n",
       "                      0.8442, 0.9269, 0.9450, 0.8593, 0.8534, 0.8453, 0.8722, 0.8518, 0.8454,\n",
       "                      0.8922, 0.8929, 0.8572, 0.8567, 0.9006, 0.9046, 0.8683, 0.8698, 0.8621,\n",
       "                      0.8749, 0.8593, 0.8629, 0.8468, 0.8224, 0.8816, 0.8674, 0.9053, 0.8409,\n",
       "                      0.8613, 0.8714, 0.8427, 0.8785, 0.8811, 0.8967, 0.8798, 0.8399, 0.8447,\n",
       "                      0.8520, 0.8685, 0.8384, 0.8350, 0.8610, 0.8539, 0.8671, 0.8700, 0.8572,\n",
       "                      0.8764, 0.9015, 0.8710, 0.8917, 0.8594, 0.8849, 0.8529, 0.8693, 0.8659,\n",
       "                      0.8821, 0.8720, 0.8968, 0.8845, 0.8753, 0.8467, 0.8508, 0.8650, 0.9079,\n",
       "                      0.8384, 0.8188, 0.8811, 0.8655, 0.8762, 0.8398], device='cuda:0')),\n",
       "             ('blocks.2.ln1.bias',\n",
       "              tensor([-0.0108,  0.0084,  0.0072, -0.0037,  0.0085, -0.0143,  0.0023, -0.0021,\n",
       "                       0.0175, -0.0044,  0.0096, -0.0120, -0.0361,  0.0247, -0.0118, -0.0299,\n",
       "                       0.0066, -0.0022,  0.0117, -0.0087,  0.0326,  0.0242,  0.0248, -0.0309,\n",
       "                      -0.0280, -0.0189, -0.0110,  0.0147,  0.0224,  0.0377, -0.0104,  0.0173,\n",
       "                      -0.0042, -0.0198, -0.0161,  0.0171,  0.0184,  0.0186,  0.0182, -0.0052,\n",
       "                       0.0035, -0.0206,  0.0359,  0.0069, -0.0300,  0.0211, -0.0034,  0.0023,\n",
       "                      -0.0153,  0.0428, -0.0235, -0.0208, -0.0202,  0.0065,  0.0425, -0.0469,\n",
       "                       0.0391, -0.0168, -0.0302, -0.0124,  0.0223, -0.0030, -0.0051, -0.0236,\n",
       "                      -0.0332, -0.0139,  0.0253, -0.0158, -0.0182, -0.0153, -0.0265, -0.0297,\n",
       "                      -0.0011,  0.0250,  0.0393, -0.0207, -0.0253,  0.0168, -0.0083,  0.0204,\n",
       "                      -0.0374, -0.0300,  0.0158,  0.0361,  0.0194,  0.0395, -0.0297, -0.0302,\n",
       "                       0.0029,  0.0037, -0.0164, -0.0299, -0.0159,  0.0143, -0.0198, -0.0233,\n",
       "                       0.0322,  0.0445,  0.0295,  0.0203,  0.0105, -0.0238, -0.0122, -0.0215,\n",
       "                      -0.0023,  0.0333, -0.0211, -0.0259, -0.0250,  0.0012,  0.0080, -0.0175,\n",
       "                       0.0176, -0.0289,  0.0149, -0.0143, -0.0272, -0.0104, -0.0078,  0.0315,\n",
       "                      -0.0053,  0.0102, -0.0307, -0.0031, -0.0219, -0.0233, -0.0194, -0.0209,\n",
       "                       0.0548,  0.0050, -0.0147, -0.0084, -0.0197, -0.0057,  0.0238, -0.0051,\n",
       "                       0.0108,  0.0241,  0.0392,  0.0165,  0.0268, -0.0454,  0.0238,  0.0074,\n",
       "                       0.0309, -0.0244,  0.0300, -0.0374, -0.0244,  0.0164,  0.0062,  0.0285,\n",
       "                      -0.0203,  0.0102, -0.0279,  0.0087, -0.0121,  0.0281,  0.0066,  0.0118,\n",
       "                      -0.0012, -0.0251, -0.0055, -0.0122, -0.0189, -0.0174,  0.0015, -0.0189,\n",
       "                       0.0521,  0.0019, -0.0090,  0.0178,  0.0054, -0.0312,  0.0392,  0.0140,\n",
       "                      -0.0318,  0.0113, -0.0158,  0.0197,  0.0231, -0.0249, -0.0115, -0.0554,\n",
       "                       0.0110,  0.0190,  0.0223, -0.0145, -0.0080,  0.0110,  0.0212,  0.0436,\n",
       "                       0.0114, -0.0258,  0.0235, -0.0028,  0.0105, -0.0065,  0.0283, -0.0157,\n",
       "                      -0.0001,  0.0032, -0.0081,  0.0250, -0.0045,  0.0057,  0.0222,  0.0037,\n",
       "                       0.0006,  0.0209, -0.0099,  0.0274,  0.0004, -0.0193,  0.0147,  0.0100,\n",
       "                       0.0036,  0.0082, -0.0513, -0.0297, -0.0110,  0.0314,  0.0159,  0.0039,\n",
       "                       0.0033, -0.0122,  0.0048,  0.0149,  0.0040,  0.0103,  0.0236, -0.0055,\n",
       "                       0.0197, -0.0331, -0.0088,  0.0403,  0.0207, -0.0097, -0.0084, -0.0029,\n",
       "                       0.0358, -0.0227,  0.0200, -0.0304, -0.0325, -0.0152,  0.0139,  0.0356,\n",
       "                      -0.0268,  0.0034, -0.0211, -0.0189, -0.0160,  0.0191,  0.0235,  0.0188,\n",
       "                      -0.0131, -0.0068, -0.0166,  0.0143, -0.0347, -0.0179,  0.0286,  0.0196,\n",
       "                      -0.0142, -0.0176,  0.0230, -0.0133,  0.0024, -0.0064,  0.0076, -0.0124,\n",
       "                       0.0142, -0.0359,  0.0319,  0.0044, -0.0092, -0.0071,  0.0220,  0.0179,\n",
       "                       0.0156,  0.0186,  0.0053,  0.0290, -0.0050,  0.0017, -0.0079,  0.0087,\n",
       "                       0.0042, -0.0256, -0.0090, -0.0244, -0.0070, -0.0206,  0.0252, -0.0150,\n",
       "                       0.0196, -0.0124,  0.0059, -0.0308,  0.0425,  0.0324,  0.0057, -0.0247,\n",
       "                       0.0058,  0.0210, -0.0176, -0.0010, -0.0170,  0.0054, -0.0283,  0.0196,\n",
       "                      -0.0076,  0.0150, -0.0387, -0.0278,  0.0015, -0.0163,  0.0163,  0.0215,\n",
       "                      -0.0180, -0.0132, -0.0033, -0.0083,  0.0024, -0.0167,  0.0216,  0.0020,\n",
       "                      -0.0118, -0.0063, -0.0053,  0.0211,  0.0409, -0.0075,  0.0159,  0.0239,\n",
       "                       0.0207,  0.0225,  0.0190, -0.0254,  0.0298, -0.0243, -0.0182, -0.0244,\n",
       "                       0.0285, -0.0164, -0.0050,  0.0235,  0.0427,  0.0133, -0.0080,  0.0009,\n",
       "                      -0.0073,  0.0198, -0.0281, -0.0154, -0.0271,  0.0161,  0.0195,  0.0247,\n",
       "                      -0.0362, -0.0172, -0.0341, -0.0347, -0.0071,  0.0131,  0.0305,  0.0417,\n",
       "                       0.0133, -0.0077, -0.0057,  0.0242, -0.0407, -0.0101,  0.0034, -0.0091,\n",
       "                      -0.0305, -0.0150, -0.0264,  0.0213, -0.0363, -0.0257, -0.0266,  0.0183],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.ln2.weight',\n",
       "              tensor([0.9185, 0.9609, 0.9620, 0.9621, 0.9572, 1.0073, 0.9637, 1.0320, 0.9711,\n",
       "                      0.9280, 0.9899, 1.0049, 0.9617, 0.9387, 0.9776, 0.9315, 0.9599, 0.9889,\n",
       "                      1.0122, 0.9657, 0.9763, 0.9961, 0.9668, 0.9787, 0.9377, 0.9961, 0.9572,\n",
       "                      0.9999, 0.9768, 0.9605, 0.9983, 0.9317, 0.9372, 0.9924, 0.9836, 0.9419,\n",
       "                      0.9945, 0.9740, 0.9673, 0.9198, 0.9554, 0.9926, 0.9564, 0.9569, 0.9882,\n",
       "                      0.9914, 0.9979, 0.9826, 0.9648, 0.9187, 0.9707, 0.9721, 0.9364, 1.0379,\n",
       "                      0.9286, 0.9909, 0.9463, 0.9594, 0.9693, 0.9624, 0.9562, 1.0123, 0.9858,\n",
       "                      0.9911, 0.9668, 0.9986, 0.9475, 1.0144, 0.9745, 0.9865, 0.9563, 0.9306,\n",
       "                      0.9490, 0.9548, 0.9782, 1.0003, 0.9514, 0.9866, 0.9684, 0.9964, 0.9703,\n",
       "                      1.0033, 0.9779, 0.9257, 0.9980, 0.9423, 0.9460, 0.9861, 0.9574, 0.9275,\n",
       "                      0.9496, 0.9856, 0.9285, 0.9748, 0.9950, 0.9531, 0.9626, 0.9734, 0.9984,\n",
       "                      0.9494, 0.9390, 0.9682, 0.9644, 0.9372, 0.9652, 0.9843, 0.9392, 0.9681,\n",
       "                      0.9601, 0.9492, 0.9390, 0.9804, 0.9843, 0.9689, 1.0435, 1.0182, 0.9381,\n",
       "                      0.9368, 0.9785, 0.9593, 0.9811, 1.0380, 0.9395, 0.9830, 0.9796, 0.9740,\n",
       "                      0.9761, 0.9868, 0.9022, 0.9833, 0.9554, 1.0122, 0.9706, 0.9688, 0.9898,\n",
       "                      0.9622, 0.9285, 0.9649, 0.9993, 1.0502, 0.9434, 0.9934, 0.9652, 0.9455,\n",
       "                      0.9462, 0.9668, 1.0341, 0.9843, 0.9307, 0.9538, 0.9945, 0.9656, 0.9680,\n",
       "                      0.9984, 0.9232, 0.9606, 0.9712, 0.9713, 0.9630, 0.9697, 0.9892, 0.9516,\n",
       "                      0.9691, 0.9022, 0.9682, 0.9538, 0.9626, 1.0239, 0.9396, 0.9968, 0.9541,\n",
       "                      0.9941, 0.9542, 0.9660, 0.9140, 0.9919, 0.9186, 0.9474, 0.9359, 0.9272,\n",
       "                      0.9619, 0.9514, 0.9430, 1.0144, 0.9416, 0.9825, 0.9870, 0.9420, 0.9393,\n",
       "                      1.0039, 0.9509, 0.9571, 0.9686, 0.9496, 0.9946, 0.9222, 0.9682, 0.9663,\n",
       "                      0.9591, 1.0201, 0.9806, 0.9450, 0.9812, 0.9460, 0.9489, 0.9632, 0.9281,\n",
       "                      0.9608, 0.9216, 0.9452, 0.9628, 0.9661, 0.9515, 0.9524, 0.9813, 0.9278,\n",
       "                      0.9630, 0.9534, 0.9855, 0.8978, 0.9890, 0.9393, 1.0056, 0.9220, 0.9748,\n",
       "                      0.9780, 0.9881, 0.9939, 0.9184, 0.9351, 1.0048, 0.9493, 0.9621, 0.9480,\n",
       "                      0.9697, 0.9658, 0.9470, 0.9492, 0.9525, 0.9268, 0.9943, 0.9673, 1.0194,\n",
       "                      0.9769, 0.9912, 0.9665, 0.9886, 0.9516, 0.9412, 0.9833, 0.9532, 0.9397,\n",
       "                      0.9307, 0.9601, 0.9805, 0.9877, 1.0311, 1.0193, 0.9712, 0.8893, 0.9664,\n",
       "                      0.9562, 0.9591, 0.9815, 0.9783, 0.9872, 0.9990, 0.9777, 0.9564, 0.9697,\n",
       "                      0.9652, 1.0146, 0.9536, 0.9623, 0.9410, 0.9412, 0.9310, 0.9680, 1.0149,\n",
       "                      1.0060, 0.9952, 0.9816, 0.9400, 0.9354, 0.9708, 0.8914, 0.9453, 0.9487,\n",
       "                      0.9652, 0.9761, 0.9398, 0.9495, 0.9575, 0.9311, 1.0219, 0.9779, 1.0081,\n",
       "                      0.9894, 1.0073, 0.9415, 0.9309, 0.9547, 1.0048, 0.9665, 0.9926, 0.9550,\n",
       "                      0.9994, 0.9053, 0.9536, 0.9696, 0.9723, 0.9698, 0.9644, 0.9188, 1.0027,\n",
       "                      0.9327, 1.0498, 0.9870, 0.9479, 0.9719, 0.9477, 0.9819, 0.9770, 0.9860,\n",
       "                      0.9328, 0.9644, 0.9166, 0.9518, 0.9321, 0.9776, 0.9571, 0.9422, 0.9525,\n",
       "                      0.9802, 0.9631, 0.9521, 0.9614, 0.9111, 1.0054, 0.9360, 0.9964, 0.9298,\n",
       "                      0.9369, 0.9942, 0.9415, 1.0040, 0.9543, 0.9498, 0.9407, 0.9361, 0.9472,\n",
       "                      0.9751, 1.0141, 0.9293, 0.9705, 0.9175, 0.9686, 0.9916, 0.9645, 0.9810,\n",
       "                      0.9773, 0.9682, 0.9296, 1.0084, 0.9329, 0.9867, 0.9692, 0.9502, 0.9949,\n",
       "                      0.9658, 1.0323, 0.9591, 0.9420, 0.9459, 0.9784, 0.9879, 0.9175, 0.9825,\n",
       "                      0.9284, 0.9241, 0.9713, 0.9632, 0.9727, 0.9348], device='cuda:0')),\n",
       "             ('blocks.2.ln2.bias',\n",
       "              tensor([-0.0402,  0.0270,  0.0930, -0.0625, -0.0636,  0.1460,  0.0046,  0.1972,\n",
       "                      -0.1259, -0.1149, -0.1075,  0.1485,  0.1210, -0.0733,  0.1249,  0.0375,\n",
       "                      -0.0605, -0.0936, -0.1586,  0.1332, -0.1552, -0.1112, -0.1445,  0.1303,\n",
       "                       0.1358,  0.2056,  0.0831, -0.1530, -0.0700, -0.0449,  0.1838,  0.0031,\n",
       "                       0.0050,  0.0986,  0.0595,  0.0102, -0.1252, -0.1277, -0.1137, -0.0571,\n",
       "                      -0.0596,  0.1275, -0.0637, -0.1733,  0.1263, -0.1137, -0.1369,  0.0525,\n",
       "                       0.0847, -0.0527,  0.0633,  0.0912,  0.0201, -0.2369, -0.0913,  0.0947,\n",
       "                      -0.1061,  0.0119, -0.1027,  0.1154,  0.0065, -0.1492,  0.1054, -0.0076,\n",
       "                       0.0792,  0.1245, -0.0947,  0.1374,  0.0666,  0.1886,  0.1205,  0.0463,\n",
       "                       0.1541, -0.0899, -0.0976,  0.1383, -0.0139, -0.1801, -0.0778, -0.0626,\n",
       "                       0.0786,  0.0780, -0.1084, -0.0258, -0.1561, -0.1043,  0.0029,  0.0824,\n",
       "                       0.1011, -0.0423,  0.0350,  0.1122,  0.0994, -0.1106,  0.0725,  0.0168,\n",
       "                      -0.0778, -0.0780, -0.1603, -0.1319, -0.0559,  0.0610,  0.1502,  0.0812,\n",
       "                       0.1610, -0.1189,  0.0759,  0.1311,  0.0677, -0.1036,  0.0213,  0.1451,\n",
       "                      -0.0755,  0.1081, -0.1890,  0.0470,  0.0824,  0.0672,  0.0481, -0.0647,\n",
       "                       0.1369, -0.2111,  0.0512,  0.1552,  0.1201,  0.0195,  0.0839,  0.1260,\n",
       "                      -0.0411, -0.1297, -0.0022,  0.1649,  0.1179,  0.1202, -0.0665,  0.1409,\n",
       "                      -0.0443, -0.1049, -0.1311, -0.1722, -0.0914,  0.0890, -0.1154, -0.0642,\n",
       "                      -0.0553,  0.1236, -0.1778,  0.1076,  0.0526, -0.0422, -0.0988, -0.0964,\n",
       "                      -0.0875,  0.1563,  0.0373, -0.1104,  0.0939, -0.1478, -0.1498, -0.1557,\n",
       "                      -0.1126,  0.1441, -0.1333, -0.0090,  0.0736, -0.0526, -0.1069,  0.1348,\n",
       "                      -0.0855,  0.1233,  0.1110, -0.0994,  0.0040,  0.0818,  0.0068, -0.1500,\n",
       "                       0.1170,  0.0563,  0.0614, -0.0613, -0.0746,  0.0735, -0.0226,  0.1764,\n",
       "                      -0.1175, -0.0412, -0.1339,  0.0676,  0.1008, -0.1736, -0.0448, -0.1199,\n",
       "                      -0.1081,  0.1141, -0.1302,  0.0665, -0.0717, -0.1518,  0.0122,  0.1283,\n",
       "                       0.0765,  0.0920,  0.1330, -0.1337,  0.0209, -0.1218, -0.0240, -0.0816,\n",
       "                       0.0991, -0.0279,  0.0980, -0.0944, -0.2020,  0.0754, -0.1426, -0.0120,\n",
       "                       0.0873, -0.0510,  0.1114, -0.0108,  0.1574, -0.0537, -0.1301,  0.0113,\n",
       "                      -0.0508, -0.1135,  0.1204, -0.1904,  0.0599, -0.0478, -0.1790,  0.0210,\n",
       "                      -0.0986,  0.0030,  0.1039, -0.1352, -0.1230,  0.0027,  0.1133,  0.0160,\n",
       "                      -0.1383,  0.0808, -0.1977,  0.1156,  0.1390,  0.0882, -0.1295, -0.0539,\n",
       "                       0.0864,  0.1572,  0.1069,  0.0146,  0.0204, -0.0456, -0.0775, -0.1541,\n",
       "                      -0.1634,  0.1435,  0.0647, -0.1051,  0.0479,  0.0801, -0.0236, -0.1727,\n",
       "                       0.0683,  0.1090, -0.1149,  0.1198, -0.1525,  0.1432,  0.0482,  0.1526,\n",
       "                      -0.1434,  0.0500, -0.0536, -0.1146,  0.0258,  0.0107, -0.1033, -0.1612,\n",
       "                      -0.1191, -0.0964, -0.0301,  0.0066,  0.1114, -0.0060, -0.0990, -0.0413,\n",
       "                      -0.1576,  0.0523,  0.0432,  0.0982,  0.0764,  0.0411, -0.1606,  0.1129,\n",
       "                      -0.1299,  0.2227,  0.1685,  0.0539, -0.0745, -0.0952, -0.1454,  0.1152,\n",
       "                       0.1694, -0.1271,  0.1469,  0.0059,  0.1356, -0.0849,  0.1642, -0.1811,\n",
       "                       0.1181,  0.0327,  0.1194,  0.0433,  0.1878, -0.1641, -0.0717, -0.1351,\n",
       "                      -0.0034,  0.1156,  0.1056,  0.1150,  0.0608,  0.1198, -0.0928,  0.0601,\n",
       "                      -0.1061,  0.1477,  0.1444, -0.1156, -0.0946,  0.0989, -0.0091, -0.0787,\n",
       "                      -0.1038,  0.0056, -0.1292,  0.1078, -0.1092,  0.0252, -0.0030,  0.0926,\n",
       "                      -0.1203,  0.1335, -0.0729, -0.0370, -0.0637, -0.0050,  0.0632, -0.0107,\n",
       "                       0.1477, -0.0631,  0.0457,  0.0559,  0.1227, -0.1499, -0.0846, -0.1613,\n",
       "                       0.1275,  0.1118,  0.0443,  0.1777,  0.0245, -0.0990, -0.0932, -0.0899,\n",
       "                      -0.1652, -0.1462,  0.2062, -0.0621,  0.0382,  0.0629, -0.1042,  0.0950,\n",
       "                      -0.0389,  0.1313,  0.0527, -0.0226,  0.1051,  0.1283,  0.1251, -0.0778],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.0.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.0.key.weight',\n",
       "              tensor([[-0.0851, -0.0044, -0.0150,  ...,  0.0583,  0.0292,  0.0101],\n",
       "                      [ 0.0461,  0.0223, -0.0302,  ...,  0.0080,  0.0044,  0.0234],\n",
       "                      [ 0.0044,  0.0588, -0.0478,  ..., -0.0430,  0.0330,  0.0565],\n",
       "                      ...,\n",
       "                      [ 0.0278, -0.0053,  0.0521,  ..., -0.0484, -0.0332, -0.0238],\n",
       "                      [ 0.0086, -0.0342, -0.0316,  ...,  0.0068,  0.0404,  0.0372],\n",
       "                      [ 0.0039, -0.0029,  0.0045,  ..., -0.0287,  0.0258,  0.0759]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.0.query.weight',\n",
       "              tensor([[ 0.0345,  0.0125, -0.0071,  ..., -0.0395, -0.0476,  0.0154],\n",
       "                      [ 0.0623,  0.0404, -0.0160,  ...,  0.0115,  0.0177,  0.0502],\n",
       "                      [ 0.1065, -0.0709, -0.0233,  ..., -0.0398,  0.0431, -0.0608],\n",
       "                      ...,\n",
       "                      [ 0.0566, -0.0172, -0.0067,  ..., -0.0190, -0.0571, -0.0440],\n",
       "                      [ 0.0280, -0.0053, -0.0041,  ...,  0.0074,  0.0242, -0.0224],\n",
       "                      [-0.0162,  0.0093, -0.0586,  ..., -0.0069,  0.0014,  0.0326]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.0.value.weight',\n",
       "              tensor([[ 0.0163, -0.0197, -0.0254,  ...,  0.0079, -0.0174,  0.0371],\n",
       "                      [-0.0529,  0.0466, -0.0011,  ...,  0.0143,  0.0147,  0.0308],\n",
       "                      [-0.0368, -0.0015,  0.0016,  ...,  0.0033, -0.0100, -0.0090],\n",
       "                      ...,\n",
       "                      [-0.0179, -0.0367, -0.0052,  ..., -0.0047,  0.0097,  0.0305],\n",
       "                      [-0.0354, -0.0566,  0.0188,  ..., -0.0112,  0.0265, -0.0347],\n",
       "                      [-0.0081, -0.0123, -0.0435,  ...,  0.0074,  0.0044,  0.0255]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.1.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.1.key.weight',\n",
       "              tensor([[-0.0383, -0.0244,  0.0636,  ...,  0.0248,  0.0071,  0.0644],\n",
       "                      [-0.0195, -0.0785, -0.0082,  ..., -0.0064, -0.0198, -0.0180],\n",
       "                      [ 0.0033, -0.0292,  0.0429,  ..., -0.0145, -0.0187,  0.0310],\n",
       "                      ...,\n",
       "                      [ 0.0518, -0.0053,  0.0253,  ..., -0.0308,  0.0259, -0.0176],\n",
       "                      [ 0.0700,  0.0418,  0.0017,  ...,  0.0385, -0.0382, -0.0002],\n",
       "                      [ 0.0226,  0.0237,  0.0318,  ..., -0.0383, -0.0621, -0.0698]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.1.query.weight',\n",
       "              tensor([[ 0.0178,  0.0571, -0.0187,  ...,  0.0062,  0.0339, -0.0390],\n",
       "                      [-0.0397,  0.0152,  0.0182,  ..., -0.0123, -0.0293,  0.0081],\n",
       "                      [ 0.0145,  0.0320, -0.0142,  ...,  0.0190,  0.0404, -0.0312],\n",
       "                      ...,\n",
       "                      [ 0.0238,  0.0166,  0.0339,  ...,  0.0002, -0.0110,  0.0127],\n",
       "                      [-0.0054, -0.0092,  0.0613,  ..., -0.0392,  0.0578,  0.0013],\n",
       "                      [-0.0092, -0.0166, -0.0482,  ...,  0.0710,  0.0300,  0.0199]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.1.value.weight',\n",
       "              tensor([[-0.0175,  0.0159,  0.0155,  ..., -0.0036,  0.0111, -0.0195],\n",
       "                      [-0.0192, -0.0104,  0.0026,  ...,  0.0206,  0.0246, -0.0155],\n",
       "                      [-0.0477, -0.0179,  0.0052,  ...,  0.0122, -0.0122,  0.0424],\n",
       "                      ...,\n",
       "                      [ 0.0523, -0.0336, -0.0318,  ..., -0.0052, -0.0166, -0.0294],\n",
       "                      [ 0.0027, -0.0150, -0.0237,  ..., -0.0244, -0.0336,  0.0271],\n",
       "                      [ 0.0303,  0.0562,  0.0068,  ..., -0.0181, -0.0030,  0.0120]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.2.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.2.key.weight',\n",
       "              tensor([[ 0.0723,  0.0205,  0.0624,  ..., -0.0294, -0.0502,  0.0055],\n",
       "                      [ 0.0010,  0.0833, -0.0292,  ...,  0.0392, -0.0672,  0.0264],\n",
       "                      [ 0.0032,  0.0367,  0.0457,  ..., -0.0485,  0.0511,  0.0411],\n",
       "                      ...,\n",
       "                      [-0.0113, -0.0431, -0.0286,  ...,  0.0078,  0.0276, -0.0079],\n",
       "                      [ 0.0046, -0.0355, -0.0137,  ..., -0.0338, -0.0462,  0.0082],\n",
       "                      [ 0.0127,  0.0030,  0.0228,  ..., -0.0254,  0.0167,  0.0192]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.2.query.weight',\n",
       "              tensor([[ 0.0069,  0.0400,  0.0196,  ...,  0.0123,  0.0074,  0.0140],\n",
       "                      [-0.0376,  0.0117,  0.0120,  ...,  0.0184,  0.0286,  0.0433],\n",
       "                      [-0.0499, -0.0084, -0.0298,  ...,  0.0378, -0.0680,  0.0136],\n",
       "                      ...,\n",
       "                      [ 0.0769,  0.0320, -0.0959,  ...,  0.0137, -0.0344, -0.0515],\n",
       "                      [ 0.0328,  0.0341, -0.0254,  ...,  0.0084,  0.0356,  0.0788],\n",
       "                      [-0.0153,  0.0206,  0.0243,  ...,  0.0883,  0.0988,  0.0088]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.2.value.weight',\n",
       "              tensor([[ 0.0046, -0.0167,  0.0039,  ...,  0.0332,  0.0146,  0.0356],\n",
       "                      [-0.0070,  0.0290, -0.0196,  ...,  0.0002, -0.0037,  0.0151],\n",
       "                      [-0.0073,  0.0140,  0.0194,  ..., -0.0448, -0.0265, -0.0106],\n",
       "                      ...,\n",
       "                      [-0.0102,  0.0006, -0.0095,  ..., -0.0110,  0.0176,  0.0184],\n",
       "                      [-0.0077, -0.0098, -0.0074,  ..., -0.0098,  0.0173,  0.0180],\n",
       "                      [ 0.0071, -0.0146,  0.0105,  ...,  0.0072, -0.0336, -0.0089]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.3.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.3.key.weight',\n",
       "              tensor([[-0.0515, -0.0547, -0.0161,  ..., -0.0231,  0.0415, -0.0294],\n",
       "                      [-0.0765, -0.0304, -0.0782,  ...,  0.0188,  0.0643, -0.0224],\n",
       "                      [-0.0049,  0.0226, -0.0068,  ..., -0.0059, -0.0305, -0.0643],\n",
       "                      ...,\n",
       "                      [-0.0017, -0.0280, -0.0186,  ..., -0.0308,  0.0069,  0.0380],\n",
       "                      [ 0.0356,  0.0509,  0.0034,  ..., -0.0105,  0.0301,  0.0179],\n",
       "                      [-0.0100, -0.0051, -0.0333,  ...,  0.0119, -0.0184, -0.0175]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.3.query.weight',\n",
       "              tensor([[-0.0236,  0.0656,  0.0303,  ...,  0.0737, -0.0100,  0.0084],\n",
       "                      [ 0.0125, -0.0117, -0.0389,  ..., -0.0466, -0.0584,  0.0104],\n",
       "                      [ 0.0143,  0.0279,  0.0493,  ..., -0.0086, -0.0342, -0.0547],\n",
       "                      ...,\n",
       "                      [ 0.0072,  0.0742,  0.0193,  ...,  0.0054,  0.0105, -0.0110],\n",
       "                      [-0.0074, -0.0258,  0.0212,  ..., -0.0048, -0.0724, -0.0117],\n",
       "                      [ 0.0332,  0.0674,  0.0202,  ...,  0.0368,  0.0281, -0.0595]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.3.value.weight',\n",
       "              tensor([[ 0.0035,  0.0004, -0.0402,  ..., -0.0224, -0.0125,  0.0108],\n",
       "                      [ 0.0002, -0.0325, -0.0059,  ...,  0.0130,  0.0037, -0.0015],\n",
       "                      [-0.0155, -0.0146, -0.0063,  ...,  0.0136, -0.0181, -0.0162],\n",
       "                      ...,\n",
       "                      [-0.0472, -0.0037, -0.0015,  ...,  0.0011, -0.0265,  0.0249],\n",
       "                      [ 0.0228, -0.0059, -0.0298,  ..., -0.0029,  0.0181,  0.0082],\n",
       "                      [-0.0002,  0.0220,  0.0142,  ..., -0.0084,  0.0057, -0.0075]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.4.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.4.key.weight',\n",
       "              tensor([[ 0.0122, -0.0053, -0.0476,  ...,  0.0477, -0.0024, -0.0259],\n",
       "                      [-0.0626,  0.0365, -0.0096,  ...,  0.0149,  0.0778,  0.0100],\n",
       "                      [-0.0278,  0.0183,  0.0192,  ..., -0.0210,  0.0110, -0.0065],\n",
       "                      ...,\n",
       "                      [ 0.0254,  0.0392,  0.0702,  ...,  0.0459,  0.0195,  0.0339],\n",
       "                      [ 0.0213,  0.0084, -0.0450,  ..., -0.0587, -0.0040,  0.0071],\n",
       "                      [ 0.0345,  0.0020,  0.0606,  ...,  0.0069, -0.0055, -0.0413]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.4.query.weight',\n",
       "              tensor([[-0.0200, -0.0107, -0.0083,  ..., -0.0614, -0.0038,  0.0546],\n",
       "                      [ 0.0570, -0.0151,  0.0377,  ..., -0.0204,  0.0227, -0.0169],\n",
       "                      [-0.0050,  0.0345, -0.0277,  ...,  0.0091,  0.0305, -0.0321],\n",
       "                      ...,\n",
       "                      [ 0.0195, -0.0146, -0.0241,  ..., -0.0582, -0.0153, -0.0191],\n",
       "                      [ 0.0053,  0.0277,  0.0236,  ...,  0.0078, -0.0012, -0.0538],\n",
       "                      [-0.0384, -0.0012,  0.0292,  ...,  0.0539,  0.0508,  0.0150]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.4.value.weight',\n",
       "              tensor([[-0.0022,  0.0028, -0.0221,  ..., -0.0168,  0.0001, -0.0045],\n",
       "                      [ 0.0232, -0.0465,  0.0179,  ..., -0.0138,  0.0044,  0.0123],\n",
       "                      [ 0.0066,  0.0080,  0.0208,  ...,  0.0111,  0.0304, -0.0106],\n",
       "                      ...,\n",
       "                      [-0.0009, -0.0189,  0.0194,  ..., -0.0011,  0.0177, -0.0322],\n",
       "                      [-0.0040, -0.0164,  0.0025,  ..., -0.0268,  0.0272, -0.0040],\n",
       "                      [ 0.0186, -0.0081,  0.0116,  ..., -0.0035, -0.0029,  0.0068]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.5.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.5.key.weight',\n",
       "              tensor([[-0.0294,  0.0755,  0.0273,  ...,  0.0254,  0.0124,  0.0190],\n",
       "                      [ 0.0297,  0.0188,  0.0687,  ..., -0.0323, -0.0335, -0.0138],\n",
       "                      [ 0.0367,  0.0178,  0.0150,  ..., -0.0053, -0.0712, -0.0381],\n",
       "                      ...,\n",
       "                      [ 0.0211,  0.0025,  0.0194,  ...,  0.0364, -0.0437,  0.0205],\n",
       "                      [ 0.0025,  0.0490, -0.0306,  ..., -0.0201, -0.0697,  0.0126],\n",
       "                      [-0.0317,  0.0146, -0.0221,  ...,  0.0272, -0.0895, -0.0257]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.5.query.weight',\n",
       "              tensor([[ 0.0052, -0.0038, -0.0254,  ..., -0.0363, -0.0532, -0.0773],\n",
       "                      [ 0.0180,  0.0683, -0.0412,  ...,  0.0081, -0.0106,  0.0039],\n",
       "                      [ 0.0406,  0.0310,  0.0527,  ..., -0.0108,  0.0490,  0.0134],\n",
       "                      ...,\n",
       "                      [-0.0156,  0.0052, -0.0306,  ..., -0.0156,  0.0335,  0.0221],\n",
       "                      [ 0.0710,  0.0108,  0.0021,  ..., -0.0123,  0.0053, -0.0197],\n",
       "                      [ 0.0080,  0.0397, -0.0015,  ..., -0.0340,  0.0649, -0.0605]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.5.value.weight',\n",
       "              tensor([[ 1.8062e-02,  1.0608e-02,  1.6826e-02,  ..., -8.5402e-03,\n",
       "                       -2.7698e-02,  1.5177e-02],\n",
       "                      [ 7.2260e-02, -1.8552e-02,  3.6199e-03,  ..., -2.1720e-03,\n",
       "                        4.9512e-03, -1.4809e-02],\n",
       "                      [ 1.7107e-02, -1.0110e-02,  2.9216e-02,  ...,  3.9255e-03,\n",
       "                        2.7816e-02,  6.0504e-03],\n",
       "                      ...,\n",
       "                      [-4.7989e-03, -7.1057e-03, -1.3100e-02,  ...,  7.4441e-05,\n",
       "                        1.3621e-02, -2.0024e-02],\n",
       "                      [ 6.7406e-03,  3.8003e-03,  4.5794e-02,  ...,  2.5751e-03,\n",
       "                       -2.7486e-02,  2.2984e-02],\n",
       "                      [ 1.0631e-02, -2.2692e-02,  1.1057e-02,  ...,  1.5391e-02,\n",
       "                        8.8590e-03,  6.3265e-03]], device='cuda:0')),\n",
       "             ('blocks.3.sa.proj.weight',\n",
       "              tensor([[-0.0023,  0.0210, -0.0016,  ..., -0.0094, -0.0361, -0.0062],\n",
       "                      [-0.0226, -0.0003,  0.0016,  ..., -0.0228,  0.0289, -0.0258],\n",
       "                      [ 0.0136, -0.0113,  0.0171,  ...,  0.0002, -0.0036, -0.0065],\n",
       "                      ...,\n",
       "                      [-0.0234,  0.0103, -0.0183,  ..., -0.0025,  0.0293,  0.0130],\n",
       "                      [-0.0048, -0.0266,  0.0615,  ..., -0.0235, -0.0015, -0.0005],\n",
       "                      [ 0.0078,  0.0125,  0.0141,  ...,  0.0018,  0.0180,  0.0420]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.proj.bias',\n",
       "              tensor([ 0.0045,  0.0122,  0.0081, -0.0064, -0.0055,  0.0120,  0.0089,  0.0086,\n",
       "                      -0.0088, -0.0063, -0.0068,  0.0082,  0.0055, -0.0099,  0.0099,  0.0151,\n",
       "                      -0.0113, -0.0026, -0.0045,  0.0119, -0.0054, -0.0151, -0.0122,  0.0050,\n",
       "                       0.0150,  0.0108,  0.0080, -0.0102, -0.0167, -0.0136,  0.0156, -0.0059,\n",
       "                       0.0047,  0.0161,  0.0088,  0.0055, -0.0091, -0.0148, -0.0106, -0.0025,\n",
       "                      -0.0126,  0.0086, -0.0077, -0.0119,  0.0057, -0.0115, -0.0082,  0.0051,\n",
       "                       0.0088, -0.0147,  0.0113,  0.0126,  0.0090, -0.0101, -0.0135,  0.0123,\n",
       "                      -0.0158,  0.0112, -0.0111,  0.0065, -0.0095, -0.0067,  0.0154,  0.0052,\n",
       "                       0.0122,  0.0074, -0.0063,  0.0073,  0.0030,  0.0055,  0.0095,  0.0070,\n",
       "                       0.0063, -0.0091, -0.0077,  0.0122,  0.0043, -0.0067, -0.0049, -0.0036,\n",
       "                       0.0106,  0.0125, -0.0098, -0.0126, -0.0146, -0.0113,  0.0142,  0.0136,\n",
       "                       0.0036,  0.0085,  0.0130,  0.0071,  0.0085, -0.0063,  0.0066,  0.0147,\n",
       "                      -0.0131, -0.0185, -0.0134, -0.0122, -0.0109,  0.0080,  0.0110,  0.0144,\n",
       "                       0.0084, -0.0095,  0.0119,  0.0082,  0.0111, -0.0092, -0.0120,  0.0157,\n",
       "                      -0.0092,  0.0098, -0.0160,  0.0084,  0.0100,  0.0033,  0.0141, -0.0141,\n",
       "                       0.0079, -0.0141,  0.0036,  0.0065,  0.0095,  0.0054,  0.0065,  0.0127,\n",
       "                      -0.0153, -0.0109,  0.0078,  0.0138,  0.0029,  0.0104, -0.0145,  0.0094,\n",
       "                      -0.0068, -0.0054, -0.0231, -0.0170, -0.0095,  0.0155, -0.0126, -0.0101,\n",
       "                      -0.0064,  0.0141, -0.0198,  0.0140,  0.0054, -0.0074, -0.0094, -0.0126,\n",
       "                       0.0100,  0.0109,  0.0117, -0.0083,  0.0081, -0.0149, -0.0060, -0.0082,\n",
       "                      -0.0075,  0.0067, -0.0101, -0.0010,  0.0189, -0.0015, -0.0047,  0.0123,\n",
       "                      -0.0097,  0.0054,  0.0073, -0.0062,  0.0100,  0.0148, -0.0087, -0.0144,\n",
       "                       0.0124, -0.0034,  0.0075, -0.0129, -0.0064,  0.0122, -0.0023,  0.0213,\n",
       "                      -0.0166, -0.0049, -0.0027,  0.0076,  0.0112, -0.0102, -0.0073, -0.0086,\n",
       "                      -0.0075,  0.0082, -0.0141,  0.0034, -0.0082,  0.0029, -0.0140,  0.0091,\n",
       "                       0.0059,  0.0037,  0.0094, -0.0174,  0.0099, -0.0042, -0.0084, -0.0080,\n",
       "                       0.0015, -0.0142,  0.0029, -0.0043, -0.0089,  0.0102, -0.0170, -0.0070,\n",
       "                       0.0057, -0.0101,  0.0160,  0.0112,  0.0092, -0.0133, -0.0072, -0.0117,\n",
       "                      -0.0043, -0.0028,  0.0081, -0.0111,  0.0074, -0.0096, -0.0174,  0.0044,\n",
       "                      -0.0110,  0.0112,  0.0102, -0.0080, -0.0103,  0.0143, -0.0004, -0.0034,\n",
       "                      -0.0124,  0.0111, -0.0207,  0.0191,  0.0136,  0.0113, -0.0005, -0.0146,\n",
       "                       0.0116,  0.0055,  0.0144,  0.0047,  0.0047, -0.0176, -0.0126, -0.0201,\n",
       "                      -0.0039,  0.0139,  0.0023, -0.0030,  0.0054,  0.0088, -0.0105, -0.0127,\n",
       "                       0.0115,  0.0128, -0.0119,  0.0117, -0.0101,  0.0123,  0.0072,  0.0110,\n",
       "                      -0.0069,  0.0160, -0.0111, -0.0078,  0.0027,  0.0087, -0.0106, -0.0083,\n",
       "                      -0.0109, -0.0097, -0.0065, -0.0135,  0.0100, -0.0026, -0.0120, -0.0117,\n",
       "                      -0.0149,  0.0146,  0.0095,  0.0131,  0.0093,  0.0140, -0.0088,  0.0063,\n",
       "                      -0.0154,  0.0064,  0.0068,  0.0090, -0.0148, -0.0039, -0.0131,  0.0088,\n",
       "                       0.0013, -0.0131,  0.0165, -0.0095,  0.0057, -0.0041,  0.0172, -0.0009,\n",
       "                       0.0022, -0.0035,  0.0098,  0.0144,  0.0122, -0.0068, -0.0152, -0.0126,\n",
       "                       0.0082,  0.0092,  0.0062,  0.0083,  0.0060,  0.0070, -0.0100,  0.0034,\n",
       "                      -0.0148,  0.0040,  0.0067, -0.0051, -0.0173,  0.0145, -0.0141, -0.0101,\n",
       "                      -0.0069, -0.0096, -0.0135,  0.0092, -0.0118,  0.0097,  0.0112,  0.0082,\n",
       "                      -0.0126,  0.0113, -0.0075, -0.0051, -0.0057, -0.0070,  0.0092,  0.0082,\n",
       "                       0.0027, -0.0023,  0.0124,  0.0159,  0.0073, -0.0114, -0.0161, -0.0104,\n",
       "                       0.0211,  0.0053,  0.0098,  0.0090, -0.0010, -0.0142, -0.0251, -0.0127,\n",
       "                      -0.0092, -0.0045,  0.0123, -0.0044,  0.0186,  0.0123, -0.0109,  0.0084,\n",
       "                       0.0031,  0.0134,  0.0080, -0.0061,  0.0193,  0.0106,  0.0104, -0.0067],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.ffwd.net.0.weight',\n",
       "              tensor([[-0.0186, -0.0538,  0.0836,  ...,  0.0124, -0.0427,  0.0312],\n",
       "                      [ 0.0334,  0.0166,  0.0121,  ...,  0.0369, -0.0257, -0.0604],\n",
       "                      [-0.0048, -0.0405, -0.0586,  ..., -0.0396, -0.0435,  0.0201],\n",
       "                      ...,\n",
       "                      [ 0.0119,  0.0254, -0.0165,  ..., -0.0003,  0.0280,  0.0369],\n",
       "                      [-0.0171,  0.0296, -0.0147,  ..., -0.0184, -0.0094,  0.0204],\n",
       "                      [ 0.0123,  0.0202, -0.0655,  ..., -0.0388, -0.0359, -0.0118]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.ffwd.net.0.bias',\n",
       "              tensor([-0.0184, -0.0208, -0.0281,  ..., -0.0297, -0.0207, -0.0311],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.ffwd.net.2.weight',\n",
       "              tensor([[-8.3714e-02,  1.0354e-02,  3.0017e-02,  ..., -1.6088e-02,\n",
       "                        4.1051e-02,  1.1270e-02],\n",
       "                      [-1.9845e-02, -2.5784e-02, -4.0738e-02,  ...,  4.2208e-02,\n",
       "                        2.5075e-02,  3.6847e-02],\n",
       "                      [ 6.9451e-02,  7.7448e-05,  1.3545e-02,  ...,  3.7863e-02,\n",
       "                        4.8132e-02,  9.6030e-03],\n",
       "                      ...,\n",
       "                      [ 1.1256e-02,  4.4260e-02, -6.1882e-04,  ...,  5.8982e-02,\n",
       "                        5.6547e-02,  3.1185e-02],\n",
       "                      [ 2.9887e-04,  7.5880e-03,  2.2151e-02,  ..., -3.5457e-02,\n",
       "                        2.0640e-02,  5.2772e-03],\n",
       "                      [-3.1127e-03, -2.3660e-02,  9.4027e-03,  ...,  4.5212e-02,\n",
       "                        2.4065e-02,  7.3587e-02]], device='cuda:0')),\n",
       "             ('blocks.3.ffwd.net.2.bias',\n",
       "              tensor([ 0.0070, -0.0145, -0.0126,  0.0079,  0.0094, -0.0174, -0.0028, -0.0134,\n",
       "                       0.0063,  0.0016,  0.0044, -0.0142, -0.0071, -0.0008, -0.0050, -0.0114,\n",
       "                       0.0152,  0.0176,  0.0106, -0.0134,  0.0100,  0.0090,  0.0075, -0.0032,\n",
       "                      -0.0075, -0.0130, -0.0157,  0.0202,  0.0041, -0.0004, -0.0130,  0.0092,\n",
       "                       0.0026, -0.0068,  0.0040, -0.0086,  0.0089,  0.0090,  0.0083,  0.0013,\n",
       "                       0.0155, -0.0124,  0.0019,  0.0158, -0.0071,  0.0109,  0.0139, -0.0049,\n",
       "                      -0.0059,  0.0033, -0.0060, -0.0038,  0.0013,  0.0259,  0.0050, -0.0053,\n",
       "                       0.0042, -0.0075,  0.0130, -0.0036,  0.0059,  0.0112, -0.0024,  0.0014,\n",
       "                       0.0057, -0.0045,  0.0113, -0.0116, -0.0073, -0.0239, -0.0095, -0.0070,\n",
       "                      -0.0015, -0.0006,  0.0091, -0.0173,  0.0015,  0.0100,  0.0020,  0.0071,\n",
       "                      -0.0024, -0.0066,  0.0157, -0.0098,  0.0137,  0.0126, -0.0042,  0.0088,\n",
       "                      -0.0079,  0.0092, -0.0081, -0.0021, -0.0146,  0.0023,  0.0013, -0.0033,\n",
       "                       0.0021,  0.0024,  0.0169, -0.0034, -0.0068, -0.0007, -0.0176,  0.0067,\n",
       "                      -0.0141,  0.0055, -0.0002,  0.0036, -0.0029,  0.0149, -0.0037, -0.0057,\n",
       "                       0.0097, -0.0097,  0.0143,  0.0006, -0.0021, -0.0094, -0.0122,  0.0009,\n",
       "                      -0.0101,  0.0179, -0.0178, -0.0093, -0.0117, -0.0133, -0.0116, -0.0191,\n",
       "                      -0.0015,  0.0180,  0.0061, -0.0286, -0.0118, -0.0040,  0.0117, -0.0150,\n",
       "                       0.0038,  0.0113,  0.0134,  0.0209,  0.0057, -0.0004,  0.0096,  0.0072,\n",
       "                       0.0024, -0.0143,  0.0219, -0.0068,  0.0002, -0.0039,  0.0080,  0.0070,\n",
       "                       0.0079, -0.0209, -0.0007, -0.0011, -0.0065,  0.0100,  0.0171,  0.0114,\n",
       "                       0.0172, -0.0048,  0.0059, -0.0007, -0.0040,  0.0065,  0.0116, -0.0073,\n",
       "                       0.0009, -0.0047, -0.0107,  0.0163, -0.0014, -0.0068, -0.0126,  0.0062,\n",
       "                      -0.0035, -0.0059, -0.0115,  0.0027,  0.0010, -0.0019, -0.0015, -0.0086,\n",
       "                       0.0112,  0.0061,  0.0021, -0.0062, -0.0057,  0.0041,  0.0026,  0.0014,\n",
       "                       0.0027, -0.0145,  0.0127, -0.0069,  0.0038,  0.0111,  0.0010, -0.0099,\n",
       "                      -0.0091, -0.0079, -0.0116,  0.0088,  0.0038,  0.0163,  0.0056,  0.0184,\n",
       "                      -0.0123,  0.0046, -0.0070,  0.0006,  0.0095, -0.0155,  0.0129, -0.0039,\n",
       "                      -0.0048,  0.0103, -0.0193,  0.0097, -0.0139,  0.0036,  0.0115,  0.0075,\n",
       "                       0.0078,  0.0123, -0.0187, -0.0029, -0.0013,  0.0001,  0.0155, -0.0001,\n",
       "                       0.0086,  0.0036, -0.0002,  0.0091,  0.0073, -0.0142, -0.0135, -0.0051,\n",
       "                       0.0028, -0.0064,  0.0193, -0.0254, -0.0102, -0.0107,  0.0141,  0.0070,\n",
       "                      -0.0074, -0.0090,  0.0015,  0.0027,  0.0072,  0.0049,  0.0123,  0.0175,\n",
       "                       0.0284, -0.0164, -0.0046,  0.0112,  0.0004, -0.0046,  0.0042,  0.0100,\n",
       "                       0.0003,  0.0013,  0.0055, -0.0025,  0.0141, -0.0136, -0.0119, -0.0113,\n",
       "                       0.0142,  0.0035,  0.0074,  0.0123, -0.0034,  0.0059,  0.0103,  0.0114,\n",
       "                       0.0188,  0.0048,  0.0065,  0.0044, -0.0101, -0.0030,  0.0154,  0.0031,\n",
       "                       0.0078,  0.0004, -0.0006, -0.0110, -0.0011, -0.0096,  0.0138, -0.0104,\n",
       "                       0.0048, -0.0188, -0.0176, -0.0011,  0.0019,  0.0020,  0.0125, -0.0062,\n",
       "                      -0.0168,  0.0160, -0.0169,  0.0045, -0.0054,  0.0108, -0.0117,  0.0041,\n",
       "                      -0.0227, -0.0022, -0.0082,  0.0009, -0.0249,  0.0138,  0.0118,  0.0165,\n",
       "                       0.0036, -0.0175, -0.0215, -0.0177, -0.0067, -0.0073,  0.0099, -0.0029,\n",
       "                       0.0224, -0.0159,  0.0018,  0.0047,  0.0015, -0.0194, -0.0002, -0.0021,\n",
       "                       0.0050, -0.0071,  0.0082, -0.0072,  0.0059, -0.0115,  0.0042, -0.0087,\n",
       "                       0.0120, -0.0058,  0.0139,  0.0166, -0.0008,  0.0029, -0.0172, -0.0052,\n",
       "                      -0.0113,  0.0068, -0.0075, -0.0006, -0.0104,  0.0065,  0.0194,  0.0111,\n",
       "                      -0.0150, -0.0098, -0.0008, -0.0294,  0.0053,  0.0141,  0.0089,  0.0110,\n",
       "                       0.0139,  0.0152, -0.0199, -0.0041,  0.0037, -0.0065,  0.0168, -0.0133,\n",
       "                       0.0046, -0.0047, -0.0011, -0.0026, -0.0098, -0.0059, -0.0073,  0.0115],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.ln1.weight',\n",
       "              tensor([0.8447, 0.8767, 0.8726, 0.8909, 0.8755, 0.8649, 0.8672, 0.8996, 0.8782,\n",
       "                      0.8923, 0.8843, 0.9033, 0.8856, 0.8616, 0.8801, 0.8793, 0.8597, 0.9203,\n",
       "                      0.8791, 0.8954, 0.9054, 0.8879, 0.8783, 0.8698, 0.8879, 0.8845, 0.8741,\n",
       "                      0.8860, 0.8775, 0.8510, 0.8927, 0.8705, 0.8639, 0.8570, 0.8570, 0.8740,\n",
       "                      0.8969, 0.9043, 0.8945, 0.8486, 0.8814, 0.8754, 0.8766, 0.8814, 0.8929,\n",
       "                      0.8897, 0.9000, 0.8565, 0.8865, 0.9087, 0.8588, 0.8900, 0.8809, 0.9410,\n",
       "                      0.8746, 0.8421, 0.8436, 0.8469, 0.9414, 0.9026, 0.8427, 0.9036, 0.8695,\n",
       "                      0.8598, 0.8791, 0.8562, 0.8699, 0.9146, 0.8534, 0.9113, 0.8851, 0.9366,\n",
       "                      0.9078, 0.8345, 0.8878, 0.9143, 0.8720, 0.8617, 0.8510, 0.8801, 0.8443,\n",
       "                      0.8629, 0.9108, 0.8487, 0.8742, 0.8872, 0.8512, 0.8936, 0.9084, 0.8955,\n",
       "                      0.8796, 0.8868, 0.8700, 0.8694, 0.8648, 0.8701, 0.9259, 0.9041, 0.8738,\n",
       "                      0.8902, 0.8823, 0.9256, 0.9026, 0.8378, 0.8959, 0.8865, 0.8563, 0.9110,\n",
       "                      0.9022, 0.8878, 0.8688, 0.8614, 0.8952, 0.8757, 0.9127, 0.8354, 0.8559,\n",
       "                      0.8593, 0.8704, 0.8705, 0.8776, 0.8957, 0.8861, 0.9196, 0.8930, 0.8566,\n",
       "                      0.8681, 0.9035, 0.8928, 0.8989, 0.8751, 0.9034, 0.8917, 0.8588, 0.8502,\n",
       "                      0.8813, 0.8636, 0.9077, 0.9053, 0.8832, 0.8741, 0.8606, 0.9253, 0.8391,\n",
       "                      0.8825, 0.8930, 0.8880, 0.9095, 0.8268, 0.8614, 0.8794, 0.8688, 0.8668,\n",
       "                      0.9019, 0.8597, 0.9019, 0.8770, 0.8767, 0.8982, 0.8928, 0.8773, 0.8782,\n",
       "                      0.8315, 0.8775, 0.8515, 0.8802, 0.8810, 0.8776, 0.8448, 0.9075, 0.8835,\n",
       "                      0.9165, 0.8649, 0.8868, 0.8776, 0.8859, 0.8670, 0.8487, 0.8605, 0.8966,\n",
       "                      0.8634, 0.8689, 0.8729, 0.8547, 0.8728, 0.8854, 0.8836, 0.8688, 0.9011,\n",
       "                      0.8917, 0.9230, 0.8947, 0.8848, 0.8448, 0.8891, 0.8878, 0.8496, 0.8553,\n",
       "                      0.8637, 0.8615, 0.8657, 0.9175, 0.8633, 0.9153, 0.8601, 0.8791, 0.8613,\n",
       "                      0.8935, 0.8882, 0.8509, 0.8632, 0.8883, 0.9388, 0.8328, 0.8700, 0.8565,\n",
       "                      0.8632, 0.8845, 0.9076, 0.8677, 0.8690, 0.8500, 0.8970, 0.8841, 0.8685,\n",
       "                      0.9395, 0.8975, 0.8702, 0.8773, 0.8864, 0.8929, 0.8589, 0.8655, 0.8738,\n",
       "                      0.8836, 0.8923, 0.8897, 0.8740, 0.8903, 0.8574, 0.8540, 0.8997, 0.8938,\n",
       "                      0.8744, 0.8769, 0.8711, 0.8986, 0.8809, 0.8630, 0.8959, 0.8670, 0.8606,\n",
       "                      0.8836, 0.8774, 0.8683, 0.8994, 0.9178, 0.8830, 0.8711, 0.8833, 0.8512,\n",
       "                      0.8978, 0.8724, 0.8659, 0.8528, 0.8856, 0.8503, 0.8847, 0.8925, 0.8939,\n",
       "                      0.8783, 0.8612, 0.9158, 0.8817, 0.8891, 0.8550, 0.8952, 0.8668, 0.8962,\n",
       "                      0.9010, 0.8476, 0.8779, 0.8682, 0.9229, 0.8663, 0.8461, 0.8991, 0.8712,\n",
       "                      0.8997, 0.8352, 0.8812, 0.8992, 0.8762, 0.8570, 0.8425, 0.9010, 0.9022,\n",
       "                      0.9167, 0.9101, 0.8258, 0.8927, 0.8861, 0.9178, 0.8735, 0.8711, 0.8816,\n",
       "                      0.9097, 0.8772, 0.8701, 0.8880, 0.8681, 0.8933, 0.9013, 0.8690, 0.8432,\n",
       "                      0.8757, 0.9118, 0.9483, 0.8755, 0.8669, 0.8795, 0.8843, 0.8703, 0.8792,\n",
       "                      0.9146, 0.8762, 0.8830, 0.8735, 0.9052, 0.9021, 0.8644, 0.8778, 0.8904,\n",
       "                      0.8941, 0.8786, 0.8683, 0.8778, 0.8556, 0.8833, 0.8598, 0.9300, 0.8630,\n",
       "                      0.8493, 0.8953, 0.8695, 0.8580, 0.9004, 0.9360, 0.8684, 0.8637, 0.8792,\n",
       "                      0.8788, 0.8903, 0.8370, 0.8700, 0.8627, 0.8722, 0.8572, 0.9007, 0.8585,\n",
       "                      0.8666, 0.8754, 0.8846, 0.9116, 0.8813, 0.8975, 0.8834, 0.8709, 0.8738,\n",
       "                      0.9045, 0.9123, 0.8675, 0.8781, 0.8730, 0.8760, 0.8582, 0.8576, 0.8781,\n",
       "                      0.8573, 0.8682, 0.8470, 0.8856, 0.9148, 0.8804], device='cuda:0')),\n",
       "             ('blocks.3.ln1.bias',\n",
       "              tensor([-0.0095, -0.0016, -0.0119,  0.0143,  0.0210, -0.0211,  0.0085, -0.0415,\n",
       "                       0.0406,  0.0120,  0.0336, -0.0255, -0.0286,  0.0368, -0.0248, -0.0359,\n",
       "                       0.0162,  0.0032,  0.0412, -0.0110,  0.0405,  0.0389,  0.0372, -0.0413,\n",
       "                      -0.0407, -0.0481, -0.0137,  0.0307,  0.0422,  0.0409, -0.0504,  0.0196,\n",
       "                      -0.0087, -0.0342, -0.0015,  0.0121,  0.0286,  0.0258,  0.0332,  0.0227,\n",
       "                       0.0071, -0.0411,  0.0443,  0.0280, -0.0244,  0.0283,  0.0312,  0.0043,\n",
       "                      -0.0243,  0.0408, -0.0284, -0.0304, -0.0287,  0.0141,  0.0512, -0.0561,\n",
       "                       0.0553, -0.0323, -0.0068,  0.0009,  0.0292,  0.0148, -0.0231, -0.0065,\n",
       "                      -0.0346, -0.0422,  0.0240, -0.0253, -0.0222, -0.0324, -0.0406, -0.0394,\n",
       "                      -0.0224,  0.0143,  0.0373, -0.0457, -0.0148,  0.0360,  0.0090,  0.0293,\n",
       "                      -0.0296, -0.0295,  0.0304,  0.0271,  0.0439,  0.0470, -0.0278, -0.0365,\n",
       "                       0.0050,  0.0033, -0.0142, -0.0326, -0.0299,  0.0193, -0.0258, -0.0227,\n",
       "                       0.0496,  0.0371,  0.0447,  0.0452,  0.0168, -0.0340, -0.0336, -0.0290,\n",
       "                      -0.0305,  0.0431, -0.0321, -0.0476, -0.0311,  0.0121,  0.0077, -0.0395,\n",
       "                       0.0084, -0.0451,  0.0300, -0.0396, -0.0326, -0.0198, -0.0268,  0.0481,\n",
       "                      -0.0213,  0.0355, -0.0324, -0.0125, -0.0424, -0.0132, -0.0135, -0.0313,\n",
       "                       0.0584,  0.0119, -0.0095, -0.0313, -0.0183, -0.0087,  0.0446, -0.0307,\n",
       "                       0.0227,  0.0327,  0.0551,  0.0369,  0.0251, -0.0503,  0.0405,  0.0047,\n",
       "                       0.0319, -0.0238,  0.0554, -0.0455, -0.0197,  0.0348,  0.0253,  0.0346,\n",
       "                      -0.0060, -0.0200, -0.0328,  0.0134, -0.0189,  0.0612,  0.0244,  0.0327,\n",
       "                       0.0295, -0.0299,  0.0017, -0.0181, -0.0272, -0.0030,  0.0302, -0.0462,\n",
       "                       0.0553, -0.0159, -0.0374,  0.0250,  0.0182, -0.0351,  0.0261,  0.0332,\n",
       "                      -0.0271,  0.0002, -0.0327,  0.0348,  0.0200, -0.0321,  0.0080, -0.0769,\n",
       "                       0.0292,  0.0245,  0.0253, -0.0182, -0.0176,  0.0273,  0.0269,  0.0453,\n",
       "                       0.0337, -0.0448,  0.0313,  0.0065,  0.0110,  0.0099,  0.0230, -0.0259,\n",
       "                      -0.0119, -0.0179, -0.0290,  0.0432, -0.0220,  0.0327,  0.0263,  0.0078,\n",
       "                      -0.0093,  0.0338, -0.0193,  0.0373,  0.0238, -0.0251,  0.0404,  0.0272,\n",
       "                      -0.0277,  0.0212, -0.0644, -0.0276, -0.0334,  0.0475,  0.0230, -0.0020,\n",
       "                       0.0104,  0.0063, -0.0249,  0.0370,  0.0152,  0.0230,  0.0662, -0.0062,\n",
       "                       0.0490, -0.0286, -0.0150,  0.0482,  0.0429, -0.0174, -0.0228, -0.0072,\n",
       "                       0.0440, -0.0236,  0.0523, -0.0427, -0.0377, -0.0242,  0.0225,  0.0482,\n",
       "                      -0.0427, -0.0292, -0.0351, -0.0103, -0.0168,  0.0306,  0.0375,  0.0442,\n",
       "                       0.0129, -0.0377, -0.0230,  0.0156, -0.0394, -0.0185,  0.0478,  0.0397,\n",
       "                      -0.0183, -0.0339,  0.0386, -0.0105,  0.0394, -0.0348, -0.0189, -0.0325,\n",
       "                       0.0281, -0.0258,  0.0360,  0.0221, -0.0075,  0.0031,  0.0140,  0.0149,\n",
       "                       0.0375,  0.0314, -0.0011,  0.0246, -0.0192, -0.0031,  0.0044,  0.0201,\n",
       "                       0.0219, -0.0438, -0.0238, -0.0190, -0.0191, -0.0266,  0.0491, -0.0264,\n",
       "                       0.0281, -0.0337, -0.0138, -0.0350,  0.0417,  0.0359,  0.0243, -0.0256,\n",
       "                      -0.0162,  0.0421, -0.0386,  0.0169, -0.0393,  0.0133, -0.0500,  0.0356,\n",
       "                      -0.0114,  0.0013, -0.0390, -0.0269, -0.0210,  0.0073,  0.0259,  0.0277,\n",
       "                      -0.0177, -0.0393, -0.0113, -0.0131, -0.0071, -0.0363,  0.0237, -0.0059,\n",
       "                       0.0154, -0.0060, -0.0179,  0.0228,  0.0390, -0.0312,  0.0246,  0.0299,\n",
       "                       0.0217,  0.0223,  0.0244, -0.0294,  0.0324, -0.0334, -0.0202, -0.0353,\n",
       "                       0.0374, -0.0573,  0.0039,  0.0417,  0.0540,  0.0136, -0.0194, -0.0236,\n",
       "                      -0.0165,  0.0248, -0.0376, -0.0403, -0.0256,  0.0149,  0.0262,  0.0319,\n",
       "                      -0.0657, -0.0170, -0.0397, -0.0419, -0.0090,  0.0325,  0.0479,  0.0509,\n",
       "                       0.0223,  0.0112, -0.0188,  0.0354, -0.0648, -0.0147,  0.0175,  0.0042,\n",
       "                      -0.0156, -0.0602, -0.0321,  0.0194, -0.0611, -0.0309, -0.0477,  0.0236],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.ln2.weight',\n",
       "              tensor([0.9959, 1.0342, 1.0273, 1.0112, 1.0263, 1.0422, 0.9676, 1.0519, 1.0115,\n",
       "                      1.0182, 1.0036, 1.0168, 1.0027, 0.9774, 1.0591, 1.0177, 0.9942, 1.0412,\n",
       "                      1.0155, 1.0193, 0.9943, 1.0186, 1.0179, 0.9940, 0.9855, 1.0184, 1.0459,\n",
       "                      1.0140, 0.9957, 1.0170, 1.0384, 0.9616, 0.9954, 1.0437, 1.0014, 0.9824,\n",
       "                      1.0301, 0.9971, 0.9894, 0.9857, 1.0254, 1.0596, 1.0025, 1.0221, 0.9865,\n",
       "                      1.0483, 1.0166, 1.0142, 0.9760, 0.9347, 1.0230, 1.0030, 1.0093, 1.0414,\n",
       "                      0.9965, 0.9856, 0.9864, 1.0274, 0.9856, 0.9808, 1.0485, 1.0344, 1.0308,\n",
       "                      1.0078, 0.9970, 1.0086, 1.0117, 1.0278, 0.9917, 1.0327, 1.0020, 0.9875,\n",
       "                      1.0167, 1.0154, 1.0411, 1.0243, 1.0115, 1.0130, 1.0056, 1.0121, 1.0029,\n",
       "                      1.0080, 1.0222, 0.9810, 1.0238, 1.0178, 0.9727, 1.0073, 1.0052, 0.9515,\n",
       "                      1.0380, 1.0351, 0.9714, 1.0105, 1.0139, 0.9573, 1.0008, 1.0044, 1.0209,\n",
       "                      1.0098, 0.9711, 0.9949, 1.0299, 0.9981, 1.0209, 1.0272, 0.9765, 0.9827,\n",
       "                      1.0067, 1.0127, 1.0068, 1.0689, 0.9918, 0.9831, 1.0460, 1.0549, 0.9816,\n",
       "                      0.9751, 0.9766, 1.0196, 1.0124, 1.0386, 1.0208, 0.9964, 1.0258, 0.9984,\n",
       "                      0.9694, 1.0093, 0.9335, 1.0319, 0.9969, 1.0364, 0.9744, 1.0025, 1.0148,\n",
       "                      1.0144, 0.9917, 1.0089, 1.0785, 1.0882, 1.0112, 1.0207, 1.0055, 1.0055,\n",
       "                      0.9839, 1.0396, 1.0509, 1.0276, 0.9786, 1.0152, 0.9876, 1.0125, 0.9977,\n",
       "                      1.0370, 1.0051, 0.9474, 1.0093, 1.0276, 1.0176, 1.0272, 1.0211, 0.9902,\n",
       "                      1.0041, 0.9573, 1.0255, 0.9920, 1.0153, 1.0223, 0.9666, 1.0256, 1.0203,\n",
       "                      1.0043, 0.9596, 1.0127, 0.9775, 0.9961, 0.9721, 0.9943, 1.0032, 0.9909,\n",
       "                      1.0085, 1.0532, 1.0096, 0.9783, 0.9919, 1.0295, 1.0076, 0.9909, 0.9926,\n",
       "                      1.0472, 0.9846, 0.9981, 1.0096, 0.9705, 1.0200, 0.9693, 1.0150, 0.9670,\n",
       "                      1.0290, 1.0631, 0.9930, 1.0140, 1.0130, 1.0127, 0.9862, 0.9855, 0.9819,\n",
       "                      1.0310, 1.0068, 0.9959, 1.0065, 0.9791, 1.0188, 1.0291, 1.0078, 0.9721,\n",
       "                      1.0091, 0.9970, 1.0339, 0.9877, 1.0451, 0.9763, 1.0128, 0.9969, 0.9974,\n",
       "                      1.0088, 1.0290, 1.0477, 0.9764, 0.9834, 1.0181, 1.0142, 0.9764, 0.9809,\n",
       "                      0.9910, 1.0086, 0.9760, 1.0230, 0.9993, 0.9516, 1.0517, 1.0120, 1.0542,\n",
       "                      1.0439, 1.0598, 1.0170, 1.0097, 1.0032, 0.9938, 1.0229, 1.0237, 0.9446,\n",
       "                      0.9963, 1.0094, 1.0135, 1.0470, 1.0925, 1.0254, 1.0133, 0.9704, 1.0011,\n",
       "                      0.9818, 1.0019, 1.0025, 1.0077, 0.9906, 1.0018, 0.9894, 1.0347, 1.0033,\n",
       "                      1.0115, 1.0414, 1.0188, 0.9809, 0.9779, 1.0062, 0.9772, 0.9998, 1.0374,\n",
       "                      0.9908, 1.0269, 1.0342, 0.9978, 0.9822, 1.0016, 0.9856, 1.0336, 0.9727,\n",
       "                      0.9969, 1.0266, 1.0150, 1.0101, 0.9981, 1.0143, 1.0472, 1.0124, 1.0187,\n",
       "                      1.0157, 1.0262, 1.0111, 1.0100, 1.0055, 1.0766, 1.0158, 1.0130, 1.0298,\n",
       "                      1.0676, 0.9840, 1.0045, 0.9719, 1.0322, 0.9889, 0.9922, 0.9722, 1.0391,\n",
       "                      0.9858, 1.0717, 1.0066, 1.0217, 1.0115, 0.9994, 1.0278, 1.0055, 1.0151,\n",
       "                      0.9774, 1.0074, 0.9985, 0.9976, 1.0364, 1.0479, 1.0177, 1.0006, 1.0060,\n",
       "                      1.0307, 0.9962, 0.9861, 0.9807, 0.9627, 1.0123, 1.0062, 1.0232, 1.0007,\n",
       "                      0.9515, 1.0242, 1.0143, 1.0317, 1.0052, 1.0123, 0.9864, 0.9532, 1.0135,\n",
       "                      1.0054, 1.0230, 0.9864, 0.9878, 0.9872, 1.0143, 1.0049, 1.0316, 1.0412,\n",
       "                      1.0102, 0.9783, 0.9789, 1.0349, 0.9611, 1.0299, 1.0077, 0.9638, 1.0431,\n",
       "                      1.0159, 1.0727, 0.9637, 0.9926, 1.0049, 1.0213, 1.0072, 0.9627, 1.0249,\n",
       "                      0.9622, 0.9847, 1.0169, 1.0166, 1.0232, 1.0063], device='cuda:0')),\n",
       "             ('blocks.3.ln2.bias',\n",
       "              tensor([-0.0477,  0.0971,  0.0879, -0.0278, -0.0767,  0.1332, -0.0234,  0.1687,\n",
       "                      -0.1243, -0.1072, -0.0697,  0.0810,  0.0700, -0.0573,  0.1732,  0.0987,\n",
       "                      -0.1046, -0.0708, -0.0966,  0.1111, -0.0896, -0.0995, -0.1062,  0.0763,\n",
       "                       0.1224,  0.1498,  0.1106, -0.1141, -0.0647, -0.0519,  0.1600, -0.0164,\n",
       "                       0.0166,  0.1176,  0.0600,  0.0414, -0.0860, -0.1160, -0.1019, -0.0997,\n",
       "                      -0.1217,  0.1205, -0.0341, -0.1153,  0.0807, -0.1236, -0.1114,  0.0503,\n",
       "                       0.0979, -0.0285,  0.0930,  0.0766,  0.0750, -0.1460, -0.0939,  0.0767,\n",
       "                      -0.0835,  0.1084, -0.0931,  0.0571, -0.0849, -0.1211,  0.1017,  0.0325,\n",
       "                       0.1050,  0.0989, -0.0813,  0.1075,  0.0607,  0.1607,  0.1319,  0.0381,\n",
       "                       0.0887, -0.0746, -0.1013,  0.1192,  0.0257, -0.1011, -0.0457, -0.0969,\n",
       "                       0.0741,  0.0913, -0.1187, -0.0635, -0.1562, -0.0998,  0.0458,  0.0362,\n",
       "                       0.0657, -0.0530,  0.0912,  0.0972,  0.0724, -0.0809,  0.0714,  0.0256,\n",
       "                      -0.0835, -0.1102, -0.1226, -0.1228, -0.0083,  0.0100,  0.1852,  0.0937,\n",
       "                       0.1372, -0.0734,  0.0817,  0.0986,  0.0520, -0.1220, -0.0226,  0.1579,\n",
       "                      -0.0728,  0.0691, -0.1315,  0.0557,  0.0316,  0.0460,  0.0474, -0.1013,\n",
       "                       0.1235, -0.1811,  0.0707,  0.0909,  0.1278,  0.0546,  0.0743,  0.0813,\n",
       "                      -0.0530, -0.1574,  0.0379,  0.1787,  0.0608,  0.0929, -0.0631,  0.1207,\n",
       "                      -0.0178, -0.1168, -0.1572, -0.1413, -0.0747,  0.0778, -0.0780, -0.0623,\n",
       "                      -0.0518,  0.1425, -0.1922,  0.0581,  0.0231, -0.0072, -0.0652, -0.0808,\n",
       "                       0.0010,  0.1796,  0.0605, -0.0497,  0.0604, -0.1029, -0.1047, -0.1338,\n",
       "                      -0.1436,  0.0409, -0.1157, -0.0135,  0.1058, -0.0771, -0.0891,  0.1216,\n",
       "                      -0.0118,  0.1247,  0.1308, -0.0879,  0.0497,  0.0708,  0.0393, -0.1121,\n",
       "                       0.0895,  0.0477,  0.1120, -0.0385, -0.0523,  0.0455, -0.0091,  0.1111,\n",
       "                      -0.1218, -0.0477, -0.1223,  0.0585,  0.1025, -0.1141, -0.0518, -0.1050,\n",
       "                      -0.1040,  0.0630, -0.1275, -0.0247, -0.0674, -0.1042, -0.0509,  0.0980,\n",
       "                       0.0750,  0.0669,  0.0892, -0.1259,  0.0480, -0.0992, -0.0565, -0.0745,\n",
       "                       0.0942, -0.0805,  0.0573, -0.0340, -0.1751,  0.1007, -0.1211, -0.0206,\n",
       "                       0.0934, -0.0651,  0.1136, -0.0149,  0.1678, -0.0813, -0.1035,  0.0057,\n",
       "                      -0.0510, -0.1285,  0.0799, -0.1211,  0.0302, -0.0646, -0.1307,  0.0641,\n",
       "                      -0.1057,  0.0248,  0.0685, -0.0905, -0.0628,  0.0828,  0.0579,  0.0119,\n",
       "                      -0.1068,  0.0960, -0.1599,  0.1469,  0.1638,  0.0640, -0.1165, -0.0468,\n",
       "                       0.1062,  0.1014,  0.0691, -0.0174,  0.0345, -0.0747, -0.0777, -0.1404,\n",
       "                      -0.1684,  0.0923,  0.0781, -0.0775,  0.0522,  0.0354, -0.0550, -0.1398,\n",
       "                       0.0600,  0.0739, -0.0863,  0.0834, -0.1540,  0.1008,  0.0605,  0.0980,\n",
       "                      -0.1306,  0.0457, -0.0425, -0.1215,  0.0226, -0.0159, -0.0921, -0.1144,\n",
       "                      -0.0839, -0.1294, -0.0358, -0.0555,  0.1188,  0.0088, -0.1312, -0.0397,\n",
       "                      -0.0877,  0.0937,  0.0448,  0.0855,  0.0541,  0.0831, -0.1296,  0.0911,\n",
       "                      -0.0948,  0.0899,  0.1782,  0.0468, -0.0835, -0.0323, -0.1236,  0.0724,\n",
       "                       0.1233, -0.1337,  0.1714, -0.0444,  0.0515, -0.0617,  0.0731, -0.0722,\n",
       "                       0.0910,  0.0045,  0.0925,  0.0569,  0.1768, -0.1284, -0.0929, -0.1086,\n",
       "                       0.0632,  0.1279,  0.1213,  0.0963,  0.0288,  0.1129, -0.0693,  0.0707,\n",
       "                      -0.1147,  0.1490,  0.0939, -0.0826, -0.1270,  0.1569, -0.0151, -0.0213,\n",
       "                      -0.0697,  0.0471, -0.1020,  0.0900, -0.1073,  0.0412,  0.0389,  0.0951,\n",
       "                      -0.1427,  0.1077, -0.0789, -0.0554, -0.0836, -0.0477,  0.0641,  0.0067,\n",
       "                       0.1309, -0.0770,  0.0311,  0.0509,  0.0781, -0.0916, -0.1307, -0.1144,\n",
       "                       0.0928,  0.0781,  0.0492,  0.1429, -0.0256, -0.0821, -0.1141, -0.0410,\n",
       "                      -0.1380, -0.1014,  0.2125, -0.0577,  0.0558,  0.1044, -0.1095,  0.0582,\n",
       "                      -0.0306,  0.1008,  0.0428,  0.0068,  0.1100,  0.0866,  0.1233, -0.0432],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.0.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.0.key.weight',\n",
       "              tensor([[-0.0051, -0.0051,  0.0489,  ...,  0.0059, -0.0012, -0.0658],\n",
       "                      [ 0.0208,  0.0348,  0.0312,  ..., -0.0015, -0.0987,  0.0486],\n",
       "                      [ 0.0493,  0.0276, -0.0782,  ...,  0.0431,  0.0204,  0.0072],\n",
       "                      ...,\n",
       "                      [-0.0130, -0.0179, -0.0004,  ..., -0.0126,  0.0308, -0.0163],\n",
       "                      [ 0.0012,  0.0017, -0.0020,  ..., -0.0322, -0.0949,  0.0332],\n",
       "                      [ 0.0382,  0.0361, -0.0033,  ...,  0.0454, -0.0935, -0.0414]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.0.query.weight',\n",
       "              tensor([[-0.0064,  0.0243,  0.0900,  ...,  0.0061, -0.0291, -0.0273],\n",
       "                      [ 0.0187,  0.0725, -0.0175,  ...,  0.0283,  0.0591,  0.0405],\n",
       "                      [-0.0343,  0.0270, -0.0092,  ...,  0.0243,  0.0284, -0.0377],\n",
       "                      ...,\n",
       "                      [-0.0397, -0.0200, -0.0105,  ...,  0.0219, -0.0158,  0.0037],\n",
       "                      [-0.0029,  0.0401,  0.0494,  ...,  0.0026, -0.0024, -0.0932],\n",
       "                      [-0.0255,  0.0212,  0.0009,  ...,  0.0324,  0.0058,  0.0098]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.0.value.weight',\n",
       "              tensor([[ 0.0004,  0.0027, -0.0108,  ...,  0.0133, -0.0093,  0.0126],\n",
       "                      [ 0.0491,  0.0394, -0.0056,  ..., -0.0212,  0.0130,  0.0015],\n",
       "                      [-0.0048,  0.0334,  0.0250,  ..., -0.0286, -0.0009, -0.0434],\n",
       "                      ...,\n",
       "                      [-0.0040, -0.0136, -0.0427,  ...,  0.0098, -0.0102,  0.0101],\n",
       "                      [-0.0115,  0.0092, -0.0118,  ..., -0.0271, -0.0008,  0.0208],\n",
       "                      [ 0.0138,  0.0140, -0.0158,  ..., -0.0369,  0.0231, -0.0005]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.1.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.1.key.weight',\n",
       "              tensor([[-0.0337,  0.0259,  0.0272,  ..., -0.0326, -0.0440,  0.0091],\n",
       "                      [-0.0121,  0.0244, -0.0063,  ..., -0.0106,  0.0061,  0.0286],\n",
       "                      [ 0.0523,  0.0511, -0.0076,  ..., -0.0290,  0.0252, -0.0271],\n",
       "                      ...,\n",
       "                      [ 0.0462,  0.0210,  0.0092,  ..., -0.0010, -0.0225,  0.0609],\n",
       "                      [-0.0292, -0.0392,  0.0013,  ...,  0.0270,  0.0722,  0.0074],\n",
       "                      [ 0.0017, -0.0290, -0.0758,  ...,  0.0255,  0.0245, -0.0272]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.1.query.weight',\n",
       "              tensor([[ 0.0530,  0.0101, -0.0343,  ..., -0.0570, -0.0011,  0.0023],\n",
       "                      [-0.0083,  0.0749,  0.0381,  ...,  0.0264, -0.0035, -0.0404],\n",
       "                      [-0.0171, -0.0833, -0.0386,  ..., -0.0010,  0.0058,  0.0392],\n",
       "                      ...,\n",
       "                      [ 0.0594,  0.0345,  0.0100,  ..., -0.0011,  0.0140,  0.0344],\n",
       "                      [ 0.0286,  0.0069, -0.0076,  ...,  0.0046, -0.0196, -0.0637],\n",
       "                      [-0.0270, -0.0434,  0.0066,  ..., -0.0271,  0.0113,  0.0304]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.1.value.weight',\n",
       "              tensor([[-0.0158,  0.0054,  0.0190,  ..., -0.0269, -0.0323, -0.0143],\n",
       "                      [-0.0250, -0.0345,  0.0116,  ..., -0.0188,  0.0101, -0.0126],\n",
       "                      [ 0.0435,  0.0137, -0.0098,  ..., -0.0188,  0.0251,  0.0669],\n",
       "                      ...,\n",
       "                      [-0.0023,  0.0030, -0.0382,  ..., -0.0417, -0.0103, -0.0193],\n",
       "                      [ 0.0058, -0.0119, -0.0234,  ...,  0.0049, -0.0124, -0.0228],\n",
       "                      [-0.0203,  0.0171,  0.0346,  ...,  0.0243,  0.0097, -0.0237]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.2.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.2.key.weight',\n",
       "              tensor([[ 0.0080,  0.0457, -0.0332,  ...,  0.0507, -0.0397,  0.0491],\n",
       "                      [ 0.0102, -0.0072, -0.0225,  ..., -0.0429,  0.0069,  0.0342],\n",
       "                      [-0.0080, -0.0615, -0.1002,  ...,  0.0479, -0.0209, -0.0397],\n",
       "                      ...,\n",
       "                      [-0.0275,  0.0286, -0.0096,  ...,  0.0254, -0.0285, -0.0537],\n",
       "                      [ 0.0320, -0.0157,  0.0226,  ...,  0.0340,  0.0329,  0.0385],\n",
       "                      [-0.0236, -0.0236, -0.0411,  ...,  0.0147, -0.0222,  0.0038]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.2.query.weight',\n",
       "              tensor([[-0.0111,  0.0071,  0.0171,  ..., -0.0125, -0.0071, -0.0426],\n",
       "                      [ 0.0102,  0.0087,  0.0185,  ..., -0.0222,  0.0159,  0.0215],\n",
       "                      [ 0.0483,  0.0286,  0.0222,  ..., -0.0117, -0.0437, -0.0445],\n",
       "                      ...,\n",
       "                      [-0.0139, -0.0588, -0.0178,  ...,  0.0301, -0.0127, -0.0010],\n",
       "                      [ 0.0094, -0.0315, -0.0292,  ...,  0.0169, -0.0397,  0.0518],\n",
       "                      [-0.0635, -0.0095,  0.0291,  ...,  0.0105,  0.0167,  0.0567]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.2.value.weight',\n",
       "              tensor([[-0.0222,  0.0176,  0.0292,  ...,  0.0167,  0.0038,  0.0100],\n",
       "                      [ 0.0008,  0.0322, -0.0185,  ..., -0.0263,  0.0177,  0.0243],\n",
       "                      [-0.0174, -0.0081, -0.0098,  ..., -0.0447,  0.0346, -0.0282],\n",
       "                      ...,\n",
       "                      [ 0.0076, -0.0224, -0.0192,  ..., -0.0124,  0.0293, -0.0169],\n",
       "                      [-0.0009, -0.0261,  0.0123,  ..., -0.0105,  0.0166, -0.0038],\n",
       "                      [-0.0146,  0.0443, -0.0315,  ...,  0.0122, -0.0029, -0.0134]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.3.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.3.key.weight',\n",
       "              tensor([[ 0.0524,  0.0020,  0.0213,  ..., -0.0164,  0.0213, -0.0095],\n",
       "                      [ 0.0219,  0.0279,  0.0003,  ...,  0.0015, -0.0574, -0.0032],\n",
       "                      [ 0.0014, -0.0147,  0.0057,  ...,  0.0656, -0.0344,  0.0053],\n",
       "                      ...,\n",
       "                      [-0.0105, -0.0089, -0.0206,  ..., -0.0343, -0.0196, -0.0353],\n",
       "                      [ 0.0455,  0.0701,  0.0198,  ..., -0.0424,  0.0025, -0.0208],\n",
       "                      [-0.0016, -0.0343,  0.0160,  ..., -0.0148,  0.0010, -0.0027]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.3.query.weight',\n",
       "              tensor([[-4.5103e-02, -5.6616e-05, -1.6641e-02,  ...,  2.8330e-03,\n",
       "                        1.8055e-02,  2.9264e-02],\n",
       "                      [-2.9199e-02,  1.0047e-02,  3.5922e-03,  ..., -4.7706e-02,\n",
       "                        1.9756e-02, -1.3793e-02],\n",
       "                      [ 3.5995e-02,  2.2113e-02, -1.5311e-02,  ...,  3.0590e-03,\n",
       "                        1.3936e-02,  8.7458e-04],\n",
       "                      ...,\n",
       "                      [ 6.0235e-02, -2.7102e-02,  2.4865e-02,  ...,  8.0548e-03,\n",
       "                        5.1940e-02,  1.3440e-02],\n",
       "                      [ 1.3387e-02,  5.9259e-03,  8.9034e-02,  ...,  9.8453e-02,\n",
       "                        3.2343e-02, -1.1433e-02],\n",
       "                      [-5.9752e-02,  4.0223e-04,  1.4637e-02,  ...,  6.6933e-02,\n",
       "                        5.4558e-02, -2.3488e-02]], device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.3.value.weight',\n",
       "              tensor([[-0.0193, -0.0094,  0.0105,  ...,  0.0083,  0.0034, -0.0308],\n",
       "                      [-0.0056, -0.0111,  0.0220,  ...,  0.0039, -0.0166, -0.0067],\n",
       "                      [ 0.0012, -0.0438, -0.0195,  ...,  0.0696, -0.0237,  0.0127],\n",
       "                      ...,\n",
       "                      [ 0.0001, -0.0148, -0.0126,  ..., -0.0009,  0.0475, -0.0289],\n",
       "                      [-0.0037, -0.0271,  0.0371,  ..., -0.0070,  0.0011, -0.0233],\n",
       "                      [-0.0323, -0.0067,  0.0208,  ...,  0.0182,  0.0126,  0.0209]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.4.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.4.key.weight',\n",
       "              tensor([[-0.0087,  0.0144, -0.0049,  ..., -0.0088, -0.0877, -0.0110],\n",
       "                      [ 0.0166, -0.1088,  0.0690,  ..., -0.0227,  0.0668, -0.0025],\n",
       "                      [ 0.0167,  0.0214, -0.0684,  ...,  0.0938, -0.0347,  0.0365],\n",
       "                      ...,\n",
       "                      [ 0.0277,  0.0905, -0.0448,  ..., -0.0768, -0.0029,  0.0768],\n",
       "                      [-0.0200, -0.0181, -0.0314,  ..., -0.0146, -0.0748,  0.0565],\n",
       "                      [-0.0073, -0.0510,  0.0134,  ..., -0.0329,  0.0170,  0.0627]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.4.query.weight',\n",
       "              tensor([[ 0.0549, -0.0041,  0.0115,  ..., -0.0394, -0.0375,  0.0016],\n",
       "                      [ 0.0613,  0.0173,  0.0738,  ..., -0.0102, -0.0081, -0.0250],\n",
       "                      [ 0.0428,  0.0270,  0.1024,  ..., -0.0839,  0.0199,  0.0370],\n",
       "                      ...,\n",
       "                      [-0.0023, -0.0234, -0.0477,  ..., -0.0232,  0.0133,  0.0371],\n",
       "                      [ 0.0181,  0.0104,  0.0440,  ...,  0.0021, -0.0227,  0.0627],\n",
       "                      [-0.0042, -0.0759,  0.0068,  ..., -0.0587, -0.0148,  0.0329]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.4.value.weight',\n",
       "              tensor([[ 3.1025e-03, -2.2475e-02,  6.0789e-05,  ..., -4.4781e-02,\n",
       "                       -2.5763e-02, -1.7491e-02],\n",
       "                      [ 1.8218e-02, -3.0512e-04, -1.3302e-02,  ...,  2.2503e-02,\n",
       "                       -6.1160e-03, -9.9692e-04],\n",
       "                      [ 9.1265e-03,  1.1553e-02,  2.4044e-02,  ..., -5.5077e-03,\n",
       "                       -5.7886e-03,  3.9907e-02],\n",
       "                      ...,\n",
       "                      [-1.1867e-02,  1.0394e-03, -2.0405e-02,  ..., -1.3834e-02,\n",
       "                        3.0066e-02, -2.0069e-02],\n",
       "                      [-5.6314e-03, -5.4829e-02,  2.3930e-03,  ..., -3.6681e-03,\n",
       "                       -2.7969e-02, -4.0023e-02],\n",
       "                      [-2.1037e-02,  1.3571e-02, -3.8863e-02,  ...,  4.6578e-02,\n",
       "                        1.1116e-02,  9.5338e-03]], device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.5.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.5.key.weight',\n",
       "              tensor([[-0.0173, -0.0092,  0.0298,  ..., -0.0088, -0.0325,  0.0098],\n",
       "                      [-0.0244, -0.0197,  0.0132,  ...,  0.0015,  0.0191,  0.0100],\n",
       "                      [ 0.0132,  0.0063,  0.0224,  ...,  0.0250, -0.0030, -0.0134],\n",
       "                      ...,\n",
       "                      [-0.0166, -0.0650, -0.0344,  ...,  0.0103,  0.0380,  0.0241],\n",
       "                      [-0.0387,  0.0581, -0.0228,  ...,  0.0220, -0.0074,  0.0084],\n",
       "                      [ 0.0262, -0.0307,  0.0048,  ..., -0.0269, -0.0359, -0.0392]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.5.query.weight',\n",
       "              tensor([[ 0.0545, -0.0415, -0.0234,  ..., -0.0564, -0.0262, -0.0295],\n",
       "                      [ 0.0338,  0.0139, -0.0395,  ...,  0.0663,  0.0497,  0.0698],\n",
       "                      [-0.0572,  0.0404, -0.0340,  ...,  0.0150,  0.0282,  0.0155],\n",
       "                      ...,\n",
       "                      [-0.0274, -0.0113, -0.0600,  ...,  0.1215,  0.0108,  0.0424],\n",
       "                      [ 0.0129, -0.0120,  0.0075,  ..., -0.0071, -0.0638, -0.0386],\n",
       "                      [ 0.0460, -0.0339, -0.0522,  ...,  0.0142, -0.0565,  0.0177]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.heads.5.value.weight',\n",
       "              tensor([[ 0.0137,  0.0108,  0.0133,  ..., -0.0090, -0.0019,  0.0199],\n",
       "                      [-0.0098,  0.0035, -0.0289,  ...,  0.0439, -0.0245,  0.0099],\n",
       "                      [ 0.0212, -0.0113,  0.0239,  ...,  0.0052,  0.0077,  0.0079],\n",
       "                      ...,\n",
       "                      [-0.0245, -0.0273, -0.0145,  ...,  0.0051,  0.0095,  0.0120],\n",
       "                      [ 0.0043, -0.0146,  0.0003,  ..., -0.0156, -0.0093,  0.0172],\n",
       "                      [-0.0286,  0.0042,  0.0135,  ...,  0.0167, -0.0393,  0.0160]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.proj.weight',\n",
       "              tensor([[ 0.0101,  0.0185, -0.0007,  ...,  0.0055, -0.0074,  0.0099],\n",
       "                      [-0.0059,  0.0316, -0.0234,  ..., -0.0156, -0.0118,  0.0118],\n",
       "                      [-0.0040, -0.0051, -0.0156,  ...,  0.0081,  0.0085, -0.0127],\n",
       "                      ...,\n",
       "                      [ 0.0046, -0.0221,  0.0051,  ..., -0.0054,  0.0047,  0.0104],\n",
       "                      [ 0.0007,  0.0245,  0.0232,  ..., -0.0185, -0.0345,  0.0329],\n",
       "                      [-0.0153, -0.0035, -0.0004,  ...,  0.0081, -0.0021, -0.0185]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.sa.proj.bias',\n",
       "              tensor([ 7.7771e-03,  4.6406e-03, -2.3466e-03, -7.5256e-03, -1.6174e-03,\n",
       "                      -3.5028e-03,  8.8676e-03,  7.6790e-03, -5.3139e-03,  2.3616e-03,\n",
       "                      -5.8304e-03,  2.4319e-03,  2.4877e-03, -1.7817e-02, -2.1173e-04,\n",
       "                       8.9298e-03, -4.9263e-03,  6.8521e-04, -5.3930e-03,  8.7777e-04,\n",
       "                      -4.8009e-03, -3.8387e-03, -4.5114e-03,  5.6554e-03,  1.2336e-02,\n",
       "                       4.2913e-03,  4.5447e-03, -1.3650e-03, -1.0575e-02, -9.3623e-03,\n",
       "                       4.6873e-03, -2.8149e-03,  7.5923e-03,  8.9943e-03,  5.4947e-03,\n",
       "                       1.1190e-02, -6.0015e-03, -8.4851e-03, -8.1651e-03, -3.1862e-03,\n",
       "                      -8.8849e-03,  9.0335e-03, -9.4200e-03, -2.3171e-03,  7.6252e-03,\n",
       "                      -6.2428e-03, -7.1856e-03,  6.9538e-03,  5.3665e-03, -1.5705e-02,\n",
       "                       6.9742e-03,  1.0691e-02,  1.2052e-02,  3.7551e-03, -9.2376e-03,\n",
       "                       9.8294e-03, -1.1665e-02,  3.4513e-03, -4.7202e-03,  2.0903e-03,\n",
       "                      -1.1463e-02,  6.3758e-03,  5.9162e-03,  9.9767e-03,  6.5584e-03,\n",
       "                       1.8025e-03, -6.1153e-03,  2.3117e-03,  6.0779e-03, -1.5220e-03,\n",
       "                       1.6619e-03,  1.0710e-03,  7.4534e-03, -5.3059e-03, -4.7710e-03,\n",
       "                       4.5419e-03,  2.4199e-05, -5.6905e-03, -6.2561e-03, -3.9622e-03,\n",
       "                       7.0904e-03,  7.7715e-03,  1.4822e-03, -8.6832e-03, -8.9169e-03,\n",
       "                      -1.0858e-03,  1.5722e-02,  1.2718e-02,  4.5654e-03,  1.5216e-02,\n",
       "                       7.0969e-03, -2.4540e-03,  4.5077e-03,  8.1142e-04,  6.2841e-03,\n",
       "                       8.6334e-03, -1.4023e-02, -1.4509e-02, -6.5796e-03, -7.9274e-03,\n",
       "                      -1.4180e-02,  5.2179e-03, -2.4767e-04,  8.1507e-03,  8.8818e-04,\n",
       "                      -6.5436e-03,  1.0887e-02,  8.1022e-03,  8.9591e-03, -5.2332e-03,\n",
       "                      -1.3654e-02,  9.7220e-03, -1.0644e-02,  3.7615e-03, -1.2743e-03,\n",
       "                       5.4574e-03,  8.4243e-03,  6.2199e-03,  9.7462e-03, -1.1219e-02,\n",
       "                       3.6897e-03, -2.3531e-03,  2.0498e-03,  6.4500e-03,  6.4433e-03,\n",
       "                       6.5768e-03,  4.5568e-03,  3.5325e-03, -2.8101e-02, -4.6345e-03,\n",
       "                       8.5148e-03,  1.6887e-03,  2.9164e-03, -1.7179e-03, -9.4273e-03,\n",
       "                       2.3333e-03, -6.7335e-03, -2.2519e-03, -1.0147e-02, -1.3047e-02,\n",
       "                      -9.5351e-03,  1.4959e-02, -3.2936e-03, -2.2613e-04, -7.2097e-03,\n",
       "                       4.7591e-03, -2.5534e-03,  9.4793e-03,  4.3658e-03, -1.0879e-02,\n",
       "                      -4.1864e-03, -6.5020e-03,  8.4465e-03, -6.3079e-03,  1.3514e-02,\n",
       "                      -5.5576e-03,  3.9553e-03, -8.1818e-03,  3.5089e-04, -9.3902e-03,\n",
       "                      -3.3218e-03,  4.6068e-03,  1.4975e-03, -7.3148e-04,  9.7582e-03,\n",
       "                       3.4039e-03, -7.0557e-03,  3.7211e-03, -1.6659e-02,  7.4835e-04,\n",
       "                       1.3746e-03, -6.1136e-03,  2.2798e-03,  2.4337e-03, -1.0781e-02,\n",
       "                      -5.8187e-03,  9.3433e-03, -9.1586e-03,  1.0049e-02, -1.5754e-02,\n",
       "                      -3.6940e-03,  4.4174e-03, -4.9982e-03,  2.1618e-02, -5.7037e-03,\n",
       "                      -6.3630e-03,  4.1667e-03,  5.9094e-03,  5.2743e-03, -9.6064e-03,\n",
       "                      -1.1625e-02, -8.0291e-03, -5.3026e-03,  2.7544e-03, -2.4729e-03,\n",
       "                      -1.7605e-03,  8.8976e-04,  2.4173e-03, -9.8166e-03,  3.3542e-03,\n",
       "                       1.0121e-02,  5.2094e-04,  4.4637e-03, -1.4079e-02,  6.7942e-03,\n",
       "                      -1.5682e-03, -1.8219e-02, -3.6343e-03,  6.6429e-03, -1.0386e-02,\n",
       "                       2.1495e-04, -7.8049e-03,  3.6923e-03,  6.9164e-03, -3.0964e-03,\n",
       "                      -1.1834e-02,  7.4457e-03, -9.6301e-03,  1.1899e-02,  2.0543e-02,\n",
       "                      -1.5427e-03, -5.4890e-03, -4.0277e-03, -1.0842e-02, -9.3699e-03,\n",
       "                      -4.1687e-03,  7.4049e-03, -4.5288e-03,  8.4879e-04, -1.4616e-02,\n",
       "                      -6.3318e-03,  5.0224e-03, -8.9972e-03,  1.4254e-02,  6.2943e-03,\n",
       "                      -6.2218e-03, -8.1211e-03,  7.7780e-03, -6.1584e-04, -1.3599e-02,\n",
       "                      -1.0281e-03,  3.5316e-03, -9.1122e-03,  7.4517e-03,  1.2471e-02,\n",
       "                       5.0240e-03, -3.5668e-03, -1.8109e-02,  7.9422e-03,  3.7634e-05,\n",
       "                       3.1427e-03,  3.7386e-04,  1.0050e-02, -4.6190e-03, -7.0825e-03,\n",
       "                      -1.1378e-02,  7.8420e-03,  8.6676e-03,  2.9900e-03, -1.9495e-03,\n",
       "                       8.1859e-03,  6.5302e-03, -1.3218e-02, -1.8391e-03,  5.5173e-03,\n",
       "                       8.7560e-03, -8.3813e-03,  1.8971e-04,  2.6856e-03,  1.5359e-02,\n",
       "                       6.1823e-03,  4.3659e-03,  2.4833e-03,  9.2892e-03, -7.7926e-03,\n",
       "                      -2.3546e-03, -2.0828e-05,  7.2765e-03, -2.3698e-03, -6.8000e-03,\n",
       "                      -5.4742e-03, -4.3382e-03, -5.9877e-03, -8.7912e-03,  9.9296e-04,\n",
       "                      -1.5027e-03,  1.2954e-03, -9.7437e-03, -6.1218e-03,  1.3037e-02,\n",
       "                       8.7868e-03,  7.8353e-03,  7.0804e-03,  9.1861e-03, -5.2794e-03,\n",
       "                       3.2762e-04, -8.5483e-03,  5.4353e-03, -1.7841e-03,  9.4107e-03,\n",
       "                      -7.6397e-03, -7.1315e-03, -1.2921e-03,  6.4899e-03, -1.8034e-03,\n",
       "                      -1.2696e-02,  8.2816e-03, -8.0657e-03,  1.4649e-02, -4.9402e-03,\n",
       "                       1.0914e-02, -4.6621e-04, -2.7934e-03, -1.8631e-03,  4.8549e-03,\n",
       "                       1.4209e-02,  3.6997e-04, -2.0979e-03, -9.1146e-03, -8.9772e-03,\n",
       "                       3.5213e-03,  7.9235e-03, -1.3786e-03,  3.3285e-03, -2.3455e-03,\n",
       "                       5.3612e-03, -5.9734e-03, -2.7246e-03, -6.7918e-03,  3.6917e-04,\n",
       "                       5.4647e-03, -2.4443e-03, -1.5979e-02,  4.8606e-03, -1.6420e-02,\n",
       "                      -6.7071e-03, -4.2983e-03, -1.3301e-02, -1.1553e-02,  1.5583e-03,\n",
       "                      -5.6888e-03,  1.4323e-02,  6.8335e-03,  8.7545e-03, -7.5170e-03,\n",
       "                       8.7703e-03, -5.7165e-03, -1.0095e-02, -9.7444e-04, -8.1540e-03,\n",
       "                       1.5176e-02,  1.0211e-02, -3.6811e-03, -7.7688e-03,  1.2617e-02,\n",
       "                       7.3023e-03,  7.7600e-03, -2.8957e-03, -4.1965e-03, -3.2365e-03,\n",
       "                       2.4194e-02, -1.3087e-03,  1.1419e-02,  9.7520e-03,  3.8437e-03,\n",
       "                      -7.2557e-03, -1.6641e-02, -1.3305e-02, -2.0512e-03, -3.1073e-03,\n",
       "                      -1.3836e-03, -4.3471e-03,  1.6452e-02,  4.8907e-03, -3.2205e-03,\n",
       "                       9.2302e-04,  7.1487e-03,  5.8959e-03,  9.2607e-03, -9.9204e-03,\n",
       "                       1.8959e-02,  4.2209e-03,  1.0035e-02, -5.9244e-03], device='cuda:0')),\n",
       "             ('blocks.4.ffwd.net.0.weight',\n",
       "              tensor([[-0.0036, -0.0216, -0.0035,  ..., -0.0202, -0.0084, -0.0053],\n",
       "                      [ 0.0150, -0.0177, -0.0981,  ...,  0.0291, -0.0384, -0.0068],\n",
       "                      [-0.0291,  0.0037,  0.0485,  ...,  0.0269,  0.0107, -0.0234],\n",
       "                      ...,\n",
       "                      [ 0.0085, -0.0240,  0.0184,  ...,  0.0479, -0.0201,  0.0397],\n",
       "                      [-0.0261,  0.0190,  0.0182,  ...,  0.0110,  0.0087,  0.0211],\n",
       "                      [ 0.0360, -0.0068,  0.0411,  ...,  0.0153, -0.0326,  0.0363]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.ffwd.net.0.bias',\n",
       "              tensor([-0.0187, -0.0283, -0.0158,  ..., -0.0140, -0.0297, -0.0132],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.ffwd.net.2.weight',\n",
       "              tensor([[ 0.0512,  0.0240, -0.0515,  ...,  0.0475,  0.0996,  0.0044],\n",
       "                      [ 0.0486, -0.0167,  0.0208,  ...,  0.0265, -0.0045, -0.0182],\n",
       "                      [ 0.0025,  0.0384,  0.0007,  ...,  0.0101,  0.0122,  0.0691],\n",
       "                      ...,\n",
       "                      [ 0.0069,  0.0102,  0.0240,  ..., -0.0627,  0.0149, -0.0180],\n",
       "                      [-0.0077,  0.0356, -0.0002,  ...,  0.0199,  0.0105,  0.0422],\n",
       "                      [-0.0028,  0.0279, -0.0194,  ..., -0.0153,  0.0200,  0.0378]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.4.ffwd.net.2.bias',\n",
       "              tensor([ 8.7289e-04, -2.1164e-02, -1.2987e-02,  3.2396e-03,  8.8425e-03,\n",
       "                      -8.2665e-03,  1.7045e-03, -1.0965e-02,  1.0189e-02,  5.8191e-03,\n",
       "                       1.2278e-02, -1.4638e-02, -2.4438e-03, -1.6203e-03, -1.0197e-02,\n",
       "                      -1.5354e-02,  7.5086e-03,  1.2090e-02,  1.2072e-02, -9.5033e-03,\n",
       "                       1.1039e-02,  1.1326e-02,  1.0719e-02, -8.5335e-03, -1.0900e-02,\n",
       "                      -6.5373e-03, -1.9421e-02,  1.8312e-02,  8.1966e-03, -6.3149e-04,\n",
       "                      -2.0279e-02,  9.1853e-03, -8.1159e-03, -1.0981e-02, -1.1642e-02,\n",
       "                      -1.8266e-02,  1.5405e-02,  9.5559e-03,  5.8156e-03, -7.8380e-04,\n",
       "                       9.9511e-03, -1.8653e-02, -3.6682e-04,  1.9738e-02, -5.2208e-03,\n",
       "                       8.1198e-03,  1.4417e-02, -2.8530e-03, -8.3370e-03,  1.7252e-03,\n",
       "                      -1.1568e-02, -6.8070e-03, -6.7567e-04,  1.4330e-02,  7.4504e-03,\n",
       "                      -5.7386e-03,  1.3825e-03, -4.9680e-03,  1.7197e-02, -9.3936e-03,\n",
       "                       3.5889e-03,  1.8742e-02, -3.3478e-03,  1.1840e-04,  4.4593e-04,\n",
       "                      -5.2842e-03,  2.2147e-03, -4.1052e-03, -8.1184e-03, -1.4497e-02,\n",
       "                      -1.4518e-02, -1.0910e-02, -8.5595e-03,  8.6016e-03,  4.8427e-03,\n",
       "                      -1.4911e-02, -2.6942e-03,  9.3871e-03,  6.0588e-03,  1.4880e-02,\n",
       "                      -6.4035e-03, -8.5397e-03,  1.7804e-02,  6.0512e-04,  1.3931e-02,\n",
       "                       2.2889e-02, -4.0717e-03,  1.7642e-03, -1.1178e-02,  4.7411e-03,\n",
       "                      -7.4072e-03, -4.8031e-03, -1.6880e-02,  4.2039e-03, -2.0535e-03,\n",
       "                      -8.0167e-04,  4.7090e-03,  7.7519e-03,  1.7114e-02,  3.9213e-03,\n",
       "                      -8.2085e-03, -7.0808e-03, -1.7319e-02, -4.6716e-04, -4.5888e-03,\n",
       "                       7.1229e-03, -3.1015e-03,  2.6743e-04, -3.7539e-03,  4.2337e-03,\n",
       "                      -1.3150e-03, -2.7789e-03,  8.3352e-03, -9.2699e-03,  1.0047e-02,\n",
       "                       2.8875e-03, -4.0859e-03, -8.0966e-03, -8.5310e-03, -4.1363e-03,\n",
       "                      -8.3940e-03,  2.1182e-02, -1.4543e-02, -1.0989e-02, -7.4933e-03,\n",
       "                      -9.6163e-03, -1.2480e-02, -1.2629e-02, -2.3939e-03,  1.6130e-02,\n",
       "                      -5.6607e-03, -2.9724e-02, -1.2217e-02, -5.2696e-03,  7.8425e-03,\n",
       "                      -2.1232e-02, -3.3029e-03,  1.6563e-02,  1.1345e-02,  1.4863e-02,\n",
       "                       1.4864e-02, -1.1598e-02,  3.4822e-03,  1.1966e-02,  1.0701e-02,\n",
       "                      -1.9167e-02,  1.6445e-02, -1.1923e-02,  2.1057e-03,  3.5881e-03,\n",
       "                       1.3384e-03,  2.5513e-03,  2.3284e-03, -2.3025e-02, -9.5258e-03,\n",
       "                       3.4812e-03, -2.1198e-03,  8.4707e-03,  1.8774e-02,  8.3245e-03,\n",
       "                       1.9497e-02, -5.8451e-05,  6.2274e-03,  1.2465e-02, -4.2620e-03,\n",
       "                       8.5619e-03,  7.6129e-03, -1.4478e-02, -4.5249e-03, -8.8673e-03,\n",
       "                      -1.0013e-02,  1.5696e-02, -9.1296e-04, -1.1681e-02, -1.3905e-02,\n",
       "                       5.2057e-03, -1.1966e-02, -5.5275e-03, -8.9374e-03,  6.4264e-03,\n",
       "                       3.5171e-03, -1.2211e-02,  6.3087e-03,  1.3493e-03,  7.1167e-03,\n",
       "                       8.8522e-03,  6.1848e-03, -7.9941e-03, -6.0519e-03,  6.0041e-03,\n",
       "                       9.9399e-03, -1.3615e-03,  3.8854e-03, -1.0720e-02,  1.4935e-02,\n",
       "                      -4.9804e-04,  1.0018e-02,  1.1581e-02,  3.1384e-03, -1.4261e-02,\n",
       "                      -1.0241e-02, -1.0786e-02, -8.3370e-03,  1.3486e-02, -1.8017e-03,\n",
       "                       1.3335e-02,  2.7052e-03,  1.2896e-02, -1.5988e-03,  4.0373e-03,\n",
       "                      -3.0344e-04,  8.9265e-04,  1.5428e-02, -1.4236e-02,  9.1323e-03,\n",
       "                       2.2795e-03, -6.7186e-03,  3.1135e-03, -1.8910e-02,  1.0279e-02,\n",
       "                      -1.0440e-02,  4.9655e-03,  1.4264e-02,  1.3911e-02,  3.5871e-03,\n",
       "                       1.8186e-02, -1.2085e-02,  1.0851e-02, -3.4333e-04,  8.9616e-03,\n",
       "                       1.2913e-02, -1.1982e-03,  3.2022e-03, -5.0684e-03,  1.1608e-03,\n",
       "                       8.9562e-03, -2.9978e-03, -1.2978e-02, -1.3132e-02,  1.1928e-03,\n",
       "                       1.3748e-02, -6.4161e-03,  1.3765e-02, -1.4547e-02, -1.2432e-02,\n",
       "                      -9.4769e-03,  1.4583e-02,  1.2975e-02, -8.2482e-03, -1.0587e-02,\n",
       "                       4.9658e-03, -2.1017e-03, -8.0754e-03,  4.1874e-03,  1.2744e-02,\n",
       "                       1.6221e-02,  3.0338e-02, -1.0384e-02, -1.2622e-02,  8.6934e-03,\n",
       "                      -1.6162e-02, -5.9229e-03,  4.1065e-03,  1.6926e-02,  2.4236e-03,\n",
       "                       3.7309e-04,  1.2119e-02, -3.0895e-03,  9.3520e-03, -1.2206e-02,\n",
       "                      -1.7202e-02, -6.8482e-03,  1.5960e-02,  1.0009e-03,  7.9185e-03,\n",
       "                       1.0393e-02, -1.0504e-03,  9.9032e-03,  1.5183e-02,  1.3764e-02,\n",
       "                       1.9027e-02,  1.6475e-03,  7.4672e-03, -4.6246e-04, -7.6018e-03,\n",
       "                      -2.3231e-03,  2.4156e-02, -5.0993e-04,  1.0005e-02, -2.4257e-03,\n",
       "                      -4.2032e-03, -1.6401e-02, -3.0497e-03, -9.2585e-03,  1.3579e-02,\n",
       "                      -1.2097e-02,  1.1019e-02, -1.8997e-02, -1.7370e-02, -2.6358e-03,\n",
       "                       2.0576e-03,  4.2143e-03,  9.7667e-03, -1.1099e-02, -1.3409e-02,\n",
       "                       1.6946e-02, -1.4848e-02,  1.0500e-02, -1.2809e-02,  1.6271e-02,\n",
       "                      -1.2450e-02,  1.2614e-02, -2.3102e-02, -5.5418e-03, -2.6752e-03,\n",
       "                      -3.4316e-03, -2.4222e-02,  1.8163e-02,  9.5569e-03,  1.6030e-02,\n",
       "                      -4.9696e-03, -1.4248e-02, -1.1755e-02, -1.4967e-02, -6.2749e-03,\n",
       "                      -3.8670e-03,  1.0841e-02, -7.2053e-03,  3.0521e-02, -1.4083e-02,\n",
       "                      -4.7315e-03,  6.3846e-03,  3.2644e-03, -1.5589e-02, -2.4244e-04,\n",
       "                       4.1643e-03,  1.6786e-03, -5.8270e-03,  8.3510e-03, -9.5983e-03,\n",
       "                       1.5883e-02, -9.2956e-03,  8.5678e-03, -9.7751e-03,  1.1975e-02,\n",
       "                      -1.0401e-02,  2.2726e-02,  9.1696e-03, -5.6501e-03, -9.5976e-04,\n",
       "                      -1.1298e-02, -5.6984e-03, -1.4686e-02,  1.1579e-02, -1.0212e-02,\n",
       "                       8.8782e-04, -2.0746e-02,  1.0122e-02,  1.2805e-02,  1.5901e-02,\n",
       "                      -1.0051e-02, -1.1253e-02, -7.7864e-03, -2.3544e-02, -9.4250e-05,\n",
       "                       1.9486e-02,  1.0626e-02,  1.2366e-02,  1.1282e-02,  1.6652e-02,\n",
       "                      -1.7539e-02,  2.2098e-03, -6.9391e-04, -1.3413e-02,  1.4660e-02,\n",
       "                      -1.1870e-02,  7.5797e-03, -1.4600e-02,  1.7189e-03,  7.7151e-03,\n",
       "                      -6.7815e-03, -1.2636e-02, -7.7335e-03,  9.0112e-03], device='cuda:0')),\n",
       "             ('blocks.4.ln1.weight',\n",
       "              tensor([0.8804, 0.8959, 0.8959, 0.9130, 0.9060, 0.8904, 0.9058, 0.9301, 0.9103,\n",
       "                      0.9297, 0.8872, 0.9116, 0.8864, 0.8725, 0.8779, 0.8931, 0.8625, 0.9274,\n",
       "                      0.8697, 0.9173, 0.9128, 0.8917, 0.8893, 0.9291, 0.8973, 0.9131, 0.9039,\n",
       "                      0.9044, 0.8878, 0.8821, 0.9180, 0.8857, 0.8904, 0.8575, 0.8821, 0.9140,\n",
       "                      0.9289, 0.8937, 0.8610, 0.8651, 0.9068, 0.8995, 0.9054, 0.9149, 0.9292,\n",
       "                      0.9119, 0.9034, 0.8739, 0.9169, 0.9146, 0.9214, 0.9117, 0.8813, 0.9594,\n",
       "                      0.8798, 0.8821, 0.8615, 0.8790, 0.9327, 0.8852, 0.8707, 0.9169, 0.8779,\n",
       "                      0.8732, 0.9125, 0.8835, 0.9075, 0.9522, 0.9025, 0.9127, 0.8856, 0.9597,\n",
       "                      0.9088, 0.8562, 0.8971, 0.9056, 0.8988, 0.9094, 0.8572, 0.8969, 0.8866,\n",
       "                      0.8876, 0.9088, 0.8623, 0.8877, 0.9207, 0.9170, 0.8807, 0.9239, 0.9027,\n",
       "                      0.8981, 0.9020, 0.9167, 0.9084, 0.9015, 0.8870, 0.9283, 0.8747, 0.8859,\n",
       "                      0.9112, 0.8748, 0.9610, 0.9124, 0.9057, 0.9206, 0.9111, 0.9098, 0.9057,\n",
       "                      0.9302, 0.8745, 0.8774, 0.9022, 0.9110, 0.8547, 0.9271, 0.8635, 0.8781,\n",
       "                      0.8919, 0.9176, 0.9028, 0.8999, 0.9363, 0.8940, 0.9144, 0.8950, 0.9101,\n",
       "                      0.8749, 0.8802, 0.8513, 0.9211, 0.9111, 0.8996, 0.9148, 0.8723, 0.8654,\n",
       "                      0.9104, 0.8834, 0.9194, 0.9321, 0.8901, 0.8876, 0.8901, 0.9120, 0.8739,\n",
       "                      0.9010, 0.9268, 0.9156, 0.9031, 0.8488, 0.8890, 0.8863, 0.9059, 0.9002,\n",
       "                      0.9075, 0.8673, 0.9049, 0.9243, 0.8865, 0.9329, 0.9149, 0.9038, 0.8854,\n",
       "                      0.8503, 0.9099, 0.8687, 0.8834, 0.9176, 0.9193, 0.8538, 0.9047, 0.8993,\n",
       "                      0.9118, 0.8944, 0.8813, 0.9047, 0.8939, 0.8978, 0.8848, 0.8964, 0.8955,\n",
       "                      0.8913, 0.9320, 0.9109, 0.8546, 0.8757, 0.8729, 0.9170, 0.9186, 0.9122,\n",
       "                      0.9052, 0.9473, 0.8858, 0.8973, 0.9009, 0.8910, 0.8911, 0.8920, 0.8893,\n",
       "                      0.8779, 0.8833, 0.8761, 0.9456, 0.9103, 0.9079, 0.8819, 0.8977, 0.8930,\n",
       "                      0.9015, 0.9202, 0.8737, 0.9081, 0.8815, 0.9490, 0.8909, 0.9068, 0.8839,\n",
       "                      0.9016, 0.9086, 0.9041, 0.9217, 0.9010, 0.8869, 0.9067, 0.9117, 0.8956,\n",
       "                      0.9222, 0.9260, 0.9108, 0.9017, 0.8957, 0.9055, 0.8948, 0.8846, 0.8976,\n",
       "                      0.8665, 0.9192, 0.9132, 0.9070, 0.9094, 0.8656, 0.8698, 0.9071, 0.8941,\n",
       "                      0.9043, 0.9102, 0.8754, 0.9327, 0.8586, 0.8972, 0.9166, 0.8802, 0.8740,\n",
       "                      0.9203, 0.9075, 0.8789, 0.9025, 0.9368, 0.8953, 0.9141, 0.8963, 0.8362,\n",
       "                      0.8885, 0.9056, 0.8855, 0.8595, 0.8950, 0.8496, 0.8865, 0.9186, 0.9023,\n",
       "                      0.9212, 0.8829, 0.9373, 0.8852, 0.8556, 0.9039, 0.9059, 0.8955, 0.8817,\n",
       "                      0.9067, 0.9134, 0.9072, 0.9103, 0.9279, 0.9382, 0.8879, 0.8885, 0.9017,\n",
       "                      0.9080, 0.8632, 0.9080, 0.8765, 0.8802, 0.8767, 0.8804, 0.8807, 0.9126,\n",
       "                      0.9128, 0.9014, 0.8433, 0.9092, 0.8812, 0.8936, 0.8756, 0.9078, 0.8879,\n",
       "                      0.8812, 0.9010, 0.8929, 0.9123, 0.8808, 0.9060, 0.9255, 0.9225, 0.8924,\n",
       "                      0.8539, 0.9313, 0.9494, 0.9181, 0.9059, 0.9138, 0.8918, 0.9038, 0.8545,\n",
       "                      0.8905, 0.8892, 0.9055, 0.8919, 0.9471, 0.9360, 0.8785, 0.8997, 0.9212,\n",
       "                      0.9341, 0.9006, 0.8812, 0.8923, 0.8742, 0.9001, 0.8735, 0.9347, 0.9135,\n",
       "                      0.8855, 0.9161, 0.8985, 0.8608, 0.9376, 0.9374, 0.9175, 0.8846, 0.9117,\n",
       "                      0.8900, 0.9192, 0.8810, 0.8935, 0.8889, 0.9112, 0.8884, 0.9033, 0.8782,\n",
       "                      0.8560, 0.8842, 0.8903, 0.8941, 0.8944, 0.8865, 0.8900, 0.8460, 0.8873,\n",
       "                      0.9354, 0.9245, 0.9084, 0.8909, 0.9113, 0.9091, 0.8841, 0.8979, 0.9066,\n",
       "                      0.8956, 0.8819, 0.8833, 0.8971, 0.9153, 0.8764], device='cuda:0')),\n",
       "             ('blocks.4.ln1.bias',\n",
       "              tensor([-1.5689e-02, -8.2368e-03,  3.1848e-03,  5.2095e-03,  9.5438e-03,\n",
       "                      -4.0595e-02,  1.3659e-02, -3.0337e-02,  4.1266e-02,  5.7668e-03,\n",
       "                       3.6427e-02, -1.7295e-02, -3.5587e-02,  4.0246e-02, -3.4055e-02,\n",
       "                      -4.9091e-02,  3.1848e-03,  1.2781e-02,  3.5641e-02, -1.2211e-02,\n",
       "                       5.7727e-02,  4.3850e-02,  4.2801e-02, -4.1961e-02, -5.5598e-02,\n",
       "                      -4.0835e-02, -1.8982e-02,  2.6224e-02,  4.3399e-02,  4.8288e-02,\n",
       "                      -4.7597e-02,  9.8467e-03,  1.9301e-03, -2.5946e-02, -1.4705e-02,\n",
       "                       1.3344e-02,  3.3195e-02,  3.1975e-02,  5.1394e-02,  3.1431e-02,\n",
       "                       1.4218e-02, -4.7312e-02,  4.4502e-02,  2.0749e-02, -3.0505e-02,\n",
       "                       2.6305e-02,  2.4524e-02,  3.5702e-03, -4.4004e-02,  4.5166e-02,\n",
       "                      -2.5354e-02, -5.2815e-02, -4.7494e-02,  1.9779e-02,  5.3289e-02,\n",
       "                      -5.2981e-02,  5.1282e-02, -3.2374e-02, -5.9825e-03, -1.8563e-03,\n",
       "                       4.8661e-02,  2.2372e-02, -1.2196e-02,  1.6183e-03, -5.2980e-02,\n",
       "                      -3.8735e-02,  2.1119e-02, -3.4665e-02, -2.8353e-02, -3.2834e-02,\n",
       "                      -3.5353e-02, -1.1710e-02, -3.6329e-02,  9.0348e-03,  4.3326e-02,\n",
       "                      -5.9137e-02, -1.2359e-02,  4.4222e-02,  1.8107e-02,  2.8149e-02,\n",
       "                      -2.3604e-02, -4.3071e-02,  3.7562e-02,  3.3939e-02,  3.6200e-02,\n",
       "                       4.4210e-02, -3.5900e-02, -4.4600e-02, -3.9302e-03, -1.3234e-02,\n",
       "                      -4.6617e-03, -2.5357e-02, -3.0528e-02,  1.4718e-02, -2.0869e-02,\n",
       "                      -2.0690e-02,  6.3451e-02,  6.1032e-02,  5.0024e-02,  4.4984e-02,\n",
       "                       2.3975e-02, -4.5019e-02, -3.8768e-02, -3.4782e-02, -3.8961e-02,\n",
       "                       4.9310e-02, -4.4052e-02, -4.3796e-02, -3.4329e-02,  2.3857e-02,\n",
       "                       2.0416e-02, -3.6844e-02,  8.5187e-03, -4.7644e-02,  3.5356e-02,\n",
       "                      -1.9500e-02, -3.7783e-02, -2.4217e-02, -2.1503e-02,  4.5498e-02,\n",
       "                      -2.0169e-02,  2.9741e-02, -2.5571e-02, -3.3052e-02, -4.3141e-02,\n",
       "                      -2.9131e-02, -2.8712e-02, -4.2305e-02,  7.5615e-02,  1.9813e-02,\n",
       "                      -1.5224e-02, -3.8712e-02, -1.0590e-02,  1.4839e-03,  6.2967e-02,\n",
       "                      -3.7415e-02,  8.6192e-03,  2.4947e-02,  5.3722e-02,  4.2468e-02,\n",
       "                       4.4750e-02, -6.1100e-02,  3.5844e-02,  3.7751e-03,  3.6156e-02,\n",
       "                      -3.5095e-02,  5.4525e-02, -4.9971e-02, -1.2884e-02,  4.7533e-02,\n",
       "                       3.3356e-02,  3.7375e-02, -9.9363e-03, -1.7647e-02, -4.4598e-02,\n",
       "                       1.1962e-02, -2.4560e-02,  5.5965e-02,  2.1258e-02,  3.8602e-02,\n",
       "                       2.9622e-02, -1.9217e-02, -6.4399e-03,  1.2404e-02, -3.7104e-02,\n",
       "                       1.4886e-02,  2.4849e-02, -4.8576e-02,  6.5775e-02, -1.4606e-02,\n",
       "                      -3.8915e-02,  2.3815e-02,  1.2840e-02, -3.7081e-02,  3.1232e-02,\n",
       "                       4.0152e-02, -3.9311e-02,  2.2314e-03, -3.1327e-02,  3.8638e-02,\n",
       "                       2.1039e-02, -2.4138e-02,  1.4786e-02, -8.6754e-02,  4.3163e-02,\n",
       "                       1.6487e-02,  2.2151e-02, -2.4979e-02, -1.5306e-02,  3.7341e-02,\n",
       "                       2.7240e-02,  5.8187e-02,  2.8553e-02, -4.5122e-02,  3.9217e-02,\n",
       "                       1.3797e-02,  2.2648e-02,  6.9129e-03,  3.7823e-02, -4.8449e-02,\n",
       "                      -1.2543e-02, -2.7797e-02, -2.7243e-02,  4.8337e-02, -2.1790e-02,\n",
       "                       4.1290e-02,  4.6506e-02,  1.6654e-02,  3.8393e-04,  2.8927e-02,\n",
       "                      -2.0773e-02,  4.1074e-02,  3.6395e-02, -2.4031e-02,  5.1459e-02,\n",
       "                       2.0951e-02, -4.7723e-02,  1.7078e-02, -6.4437e-02, -3.0775e-02,\n",
       "                      -3.9623e-02,  4.0562e-02,  2.0892e-02, -3.2022e-03, -1.0810e-02,\n",
       "                      -5.7206e-03, -2.6249e-02,  3.8678e-02,  7.0389e-03,  2.5464e-02,\n",
       "                       6.8033e-02, -1.3583e-02,  5.1881e-02, -4.4947e-02, -1.4014e-02,\n",
       "                       5.3950e-02,  3.2605e-02, -2.8586e-02, -1.2980e-02,  1.2564e-02,\n",
       "                       6.6458e-02, -3.6815e-02,  5.4258e-02, -5.2151e-02, -4.5392e-02,\n",
       "                      -2.3691e-02,  2.6412e-02,  5.4601e-02, -3.8933e-02, -3.0012e-02,\n",
       "                      -4.2133e-02,  1.6678e-05, -1.9747e-02,  3.1817e-02,  5.2544e-02,\n",
       "                       5.2583e-02,  1.0157e-02, -4.5481e-02, -3.4168e-02,  2.8887e-02,\n",
       "                      -5.1616e-02, -3.3308e-02,  3.6763e-02,  4.4013e-02, -5.9272e-03,\n",
       "                      -5.6995e-02,  4.3922e-02, -1.5482e-02,  4.2246e-02, -3.8243e-02,\n",
       "                      -8.2365e-03, -2.1193e-02,  1.6210e-02, -3.4432e-02,  2.7674e-02,\n",
       "                       1.7764e-02, -8.4436e-03,  1.5019e-02,  2.3993e-02,  2.9342e-02,\n",
       "                       2.5367e-02,  3.8652e-02, -9.4268e-03,  2.9562e-02, -2.3096e-02,\n",
       "                       6.1140e-03,  5.2943e-03,  1.0392e-02,  3.8208e-02, -5.1346e-02,\n",
       "                      -2.9884e-02, -2.1996e-02, -1.7261e-02, -3.7046e-02,  3.9571e-02,\n",
       "                      -2.7967e-02,  4.4293e-02, -4.8196e-02, -3.9517e-02, -3.3273e-02,\n",
       "                       4.2697e-02,  4.3361e-02,  3.6609e-02, -3.4905e-02, -7.8242e-04,\n",
       "                       4.9541e-02, -5.2698e-02,  1.8787e-02, -4.1333e-02,  1.3998e-02,\n",
       "                      -5.4417e-02,  3.5376e-02, -1.3572e-02,  6.2780e-03, -3.7341e-02,\n",
       "                      -3.8049e-02, -2.9785e-02,  2.2879e-03,  2.3451e-02,  2.8372e-02,\n",
       "                      -2.3213e-02, -5.1700e-02, -1.8817e-02, -3.4201e-02,  1.1562e-03,\n",
       "                      -4.2725e-02,  4.1038e-02, -7.1506e-03,  1.4794e-02, -1.7748e-02,\n",
       "                      -1.1732e-02,  1.6592e-02,  5.2340e-02, -3.6885e-02,  3.8373e-02,\n",
       "                       4.7293e-02,  1.7706e-02,  1.7757e-02,  5.6780e-02, -2.7802e-02,\n",
       "                       4.0350e-02, -3.8210e-02, -1.8828e-02, -3.8739e-02,  4.7174e-02,\n",
       "                      -5.4332e-02,  2.2648e-03,  3.3615e-02,  4.1068e-02, -1.5986e-03,\n",
       "                      -1.1323e-02, -2.0751e-02, -2.4754e-02,  3.6097e-02, -4.4010e-02,\n",
       "                      -4.4245e-02, -2.7153e-02,  1.4802e-02,  4.4124e-02,  4.2744e-02,\n",
       "                      -7.6576e-02, -1.6852e-02, -4.5980e-02, -5.3155e-02, -1.1953e-02,\n",
       "                       2.9302e-02,  5.7573e-02,  5.5257e-02,  3.2667e-02,  2.0511e-02,\n",
       "                      -3.3028e-02,  3.3438e-02, -6.3979e-02, -1.8739e-02,  2.9726e-02,\n",
       "                      -5.7888e-03, -2.0835e-02, -6.5871e-02, -2.5244e-02,  3.8520e-02,\n",
       "                      -6.9970e-02, -3.5144e-02, -4.8639e-02,  2.0771e-02], device='cuda:0')),\n",
       "             ('blocks.4.ln2.weight',\n",
       "              tensor([1.0036, 1.0778, 1.0289, 1.0415, 1.0170, 1.0900, 0.9880, 1.0951, 1.0783,\n",
       "                      0.9991, 1.0564, 1.0089, 1.0263, 1.0315, 1.0544, 1.0242, 1.0568, 1.0617,\n",
       "                      1.0662, 1.0654, 1.0570, 1.0296, 1.0773, 1.0190, 1.0323, 1.0785, 1.0711,\n",
       "                      1.0626, 1.0415, 1.0190, 1.0715, 1.0063, 1.0220, 1.0680, 1.0540, 1.0280,\n",
       "                      1.0511, 1.0578, 1.0398, 1.0547, 1.0679, 1.1192, 1.0149, 1.0254, 1.0054,\n",
       "                      1.0672, 1.0812, 1.0263, 1.0418, 0.9973, 1.0433, 1.0099, 1.0423, 1.0783,\n",
       "                      1.0444, 1.0316, 1.0398, 1.0316, 1.0441, 1.0270, 1.0724, 1.0393, 1.0363,\n",
       "                      1.0384, 1.0157, 1.0437, 1.0205, 1.0277, 1.0187, 1.0595, 1.0537, 1.0491,\n",
       "                      1.0306, 1.0232, 1.0390, 1.0471, 1.0418, 1.0481, 1.0411, 1.0875, 1.0738,\n",
       "                      1.0375, 1.0498, 1.0242, 1.0610, 1.0272, 0.9964, 1.0583, 1.0674, 0.9845,\n",
       "                      1.0329, 1.0678, 1.0527, 1.0611, 1.0402, 1.0271, 1.0423, 1.0576, 1.0671,\n",
       "                      1.0340, 1.0592, 1.0192, 1.0420, 1.0171, 1.0498, 1.0546, 1.0082, 1.0300,\n",
       "                      1.0502, 1.0485, 1.0071, 1.0904, 1.0453, 1.0551, 1.0585, 1.0882, 0.9895,\n",
       "                      1.0160, 1.0313, 1.0735, 1.0435, 1.0608, 1.0849, 1.0569, 1.0575, 1.0468,\n",
       "                      1.0387, 1.0406, 0.9931, 1.0616, 1.0814, 1.0583, 1.0348, 1.0342, 1.0739,\n",
       "                      1.0533, 0.9945, 1.0431, 1.1281, 1.0344, 1.0494, 1.0101, 1.0451, 1.0492,\n",
       "                      1.0320, 1.0695, 1.0482, 1.0726, 1.0054, 1.0286, 1.0622, 1.0560, 1.0464,\n",
       "                      1.0426, 1.0387, 0.9839, 1.0290, 1.0102, 1.0243, 1.0426, 1.0511, 1.0554,\n",
       "                      1.0234, 0.9903, 1.0592, 1.0466, 1.0158, 1.0474, 0.9916, 1.0388, 1.0290,\n",
       "                      1.0679, 1.0061, 1.0279, 1.0355, 1.0450, 1.0233, 1.0198, 1.0431, 1.0031,\n",
       "                      1.0245, 1.0301, 1.0339, 0.9941, 1.0299, 1.0517, 1.0488, 1.0645, 1.0008,\n",
       "                      1.0870, 1.0651, 1.0240, 1.0283, 1.0384, 1.0720, 1.0058, 1.0438, 1.0134,\n",
       "                      1.0615, 1.0735, 1.0317, 1.0078, 1.0722, 1.0632, 1.0406, 1.0521, 1.0317,\n",
       "                      1.0454, 1.0261, 1.0215, 1.0304, 1.0212, 1.0605, 1.0485, 1.0314, 1.0067,\n",
       "                      1.0628, 1.0463, 1.0686, 0.9955, 1.0887, 1.0251, 1.0625, 1.0457, 1.0195,\n",
       "                      1.0479, 1.0250, 1.0532, 1.0224, 1.0486, 1.0625, 1.0275, 1.0549, 0.9710,\n",
       "                      1.0398, 1.0411, 1.0281, 1.0482, 0.9953, 1.0312, 1.1135, 1.0649, 1.0760,\n",
       "                      1.1001, 1.0655, 1.0799, 1.0581, 1.0594, 1.0250, 1.0559, 1.0430, 1.0047,\n",
       "                      1.0555, 1.0307, 1.0302, 1.0958, 1.0411, 1.0684, 1.0619, 1.0080, 1.0278,\n",
       "                      1.0236, 1.0275, 1.0576, 1.0509, 1.0810, 1.0407, 1.0377, 1.0424, 1.0534,\n",
       "                      1.0471, 1.0797, 1.0412, 1.0085, 1.0496, 1.0628, 1.0158, 1.0322, 1.0891,\n",
       "                      1.0705, 1.0933, 1.0581, 1.0217, 1.0507, 1.0324, 1.0108, 1.0363, 1.0086,\n",
       "                      1.0274, 1.0640, 1.0346, 1.0441, 1.0118, 1.0446, 1.0868, 1.0473, 1.0186,\n",
       "                      1.0700, 1.0746, 1.0377, 1.0398, 1.0055, 1.0663, 1.0544, 1.0509, 1.0614,\n",
       "                      1.1116, 1.0146, 1.0471, 1.0603, 1.0721, 1.0414, 1.0601, 1.0219, 1.0485,\n",
       "                      1.0503, 1.1164, 1.0454, 1.0673, 1.0527, 1.0308, 1.0756, 1.0864, 1.0687,\n",
       "                      1.0259, 1.0438, 1.0291, 1.0237, 1.1025, 1.0684, 1.0551, 1.0045, 1.0486,\n",
       "                      1.0666, 1.0458, 1.0316, 1.0305, 1.0110, 1.0429, 1.0640, 1.0678, 1.0021,\n",
       "                      1.0038, 1.0655, 1.0698, 1.0561, 1.0308, 1.0362, 1.0118, 1.0031, 1.0324,\n",
       "                      1.0315, 1.0504, 1.0148, 1.0676, 0.9939, 1.0715, 1.0123, 1.0626, 1.0473,\n",
       "                      1.0179, 0.9924, 1.0132, 1.0906, 1.0113, 1.0870, 1.0728, 1.0015, 1.0662,\n",
       "                      1.0328, 1.0680, 1.0324, 1.0286, 1.0543, 1.0780, 1.0573, 1.0246, 1.0488,\n",
       "                      0.9751, 1.0235, 1.0222, 1.0263, 1.0202, 1.0356], device='cuda:0')),\n",
       "             ('blocks.4.ln2.bias',\n",
       "              tensor([ 4.7302e-03,  1.3368e-01,  8.5619e-02, -7.4140e-02, -6.9062e-02,\n",
       "                       1.4113e-01, -1.1311e-03,  1.0314e-01, -9.0051e-02, -7.3336e-02,\n",
       "                      -8.8888e-02,  3.9285e-02,  7.3022e-02, -7.0549e-02,  1.1054e-01,\n",
       "                       9.8270e-02, -1.1119e-01, -1.0030e-01, -8.2009e-02,  1.2346e-01,\n",
       "                      -9.7052e-02, -1.1223e-01, -1.2664e-01,  7.5301e-02,  1.2645e-01,\n",
       "                       1.5918e-01,  1.2659e-01, -1.2530e-01, -5.0169e-02, -5.9893e-02,\n",
       "                       1.1671e-01, -1.1653e-02,  3.9946e-02,  1.0291e-01,  4.3166e-02,\n",
       "                       5.4088e-02, -1.3509e-01, -1.1707e-01, -8.3226e-02, -7.4234e-02,\n",
       "                      -1.1322e-01,  1.4583e-01, -2.2114e-02, -1.2783e-01,  2.4029e-02,\n",
       "                      -9.4001e-02, -1.0777e-01,  5.5039e-02,  7.5679e-02, -8.1173e-02,\n",
       "                       7.6777e-02,  1.0908e-01,  8.1641e-02, -1.2405e-01, -9.0541e-02,\n",
       "                       8.2769e-02, -9.5827e-02,  5.1067e-02, -9.9745e-02,  6.0476e-02,\n",
       "                      -4.9855e-02, -7.7818e-02,  9.5057e-02,  9.5986e-03,  1.5108e-02,\n",
       "                       9.2847e-02, -6.5556e-02,  9.4233e-02,  4.9465e-02,  1.0454e-01,\n",
       "                       7.6811e-02,  4.9834e-02,  4.6042e-02, -4.2634e-02, -6.7939e-02,\n",
       "                       1.0441e-01, -1.2182e-02, -9.1843e-02, -5.3970e-02, -1.3154e-02,\n",
       "                       6.0470e-02,  4.2602e-02, -1.0380e-01, -2.8352e-02, -1.5332e-01,\n",
       "                      -8.9470e-02,  7.4481e-02, -1.9265e-03,  9.0188e-02, -6.5683e-02,\n",
       "                       4.6129e-02,  7.0694e-02,  1.1095e-01, -7.4629e-02,  4.4772e-02,\n",
       "                       1.7579e-02, -1.1413e-01, -1.0271e-01, -1.7051e-01, -6.6289e-02,\n",
       "                      -7.0251e-02,  4.6903e-02,  1.1801e-01,  4.7139e-02,  1.1695e-01,\n",
       "                      -7.1513e-02,  3.9822e-02,  8.9462e-02,  3.4538e-02, -1.1049e-01,\n",
       "                      -1.9336e-02,  1.8952e-01, -6.3520e-02,  1.0935e-01, -1.2163e-01,\n",
       "                       4.6948e-02,  1.8766e-02,  1.7325e-02,  6.8150e-02, -1.1070e-01,\n",
       "                       6.7542e-02, -1.4047e-01,  8.6544e-02,  1.0739e-01,  7.4192e-02,\n",
       "                       4.3346e-02,  6.3898e-02,  8.4765e-02, -3.2611e-02, -1.4869e-01,\n",
       "                       4.8176e-02,  1.6180e-01,  1.0180e-01,  5.3494e-02, -1.0738e-01,\n",
       "                       1.2440e-01, -1.4867e-02, -9.2370e-02, -1.3074e-01, -9.1123e-02,\n",
       "                      -6.7524e-02,  8.4278e-02, -7.9353e-02, -5.3278e-02, -3.9720e-02,\n",
       "                       1.4217e-01, -1.0277e-01,  5.7686e-02,  1.8290e-02, -5.8353e-02,\n",
       "                      -7.4045e-02, -8.4182e-02, -2.6375e-02,  1.3714e-01,  9.6561e-02,\n",
       "                      -5.9059e-02,  6.7417e-02, -9.8455e-02, -1.0784e-01, -1.1736e-01,\n",
       "                      -1.0619e-01,  6.7341e-02, -6.1139e-02, -7.8680e-02,  1.3516e-01,\n",
       "                      -1.0390e-01, -1.1701e-01,  7.4316e-02,  1.1789e-02,  5.6862e-02,\n",
       "                       7.3264e-02, -4.3622e-02,  2.5117e-02,  7.4179e-02,  8.6299e-02,\n",
       "                      -1.0472e-01,  9.7280e-02,  3.0827e-02,  7.6789e-02, -9.3274e-03,\n",
       "                      -2.1937e-02,  7.7914e-02,  8.6223e-03,  5.4227e-02, -1.0302e-01,\n",
       "                      -5.5887e-05, -8.1099e-02,  6.8410e-02,  1.2061e-01, -1.2694e-01,\n",
       "                      -8.6927e-02, -1.0055e-01, -8.1552e-02,  7.7540e-02, -1.0255e-01,\n",
       "                       1.3071e-02, -5.6102e-02, -1.2847e-01, -4.1506e-02,  1.0862e-01,\n",
       "                       5.5525e-02,  6.3324e-02,  9.2892e-02, -1.5521e-01,  1.9904e-02,\n",
       "                      -1.0617e-01, -7.4772e-02, -9.3807e-02,  7.6084e-02, -4.8841e-02,\n",
       "                       5.8243e-02, -1.2843e-02, -1.5110e-01,  1.0502e-01, -7.3712e-02,\n",
       "                       5.5518e-03,  8.9821e-02, -3.5265e-02,  1.1536e-01, -4.0886e-02,\n",
       "                       1.1132e-01, -3.5837e-02, -8.6496e-02, -1.0433e-01, -7.9431e-02,\n",
       "                      -1.0856e-01,  8.6704e-02, -1.0601e-01,  3.9739e-02, -8.1964e-02,\n",
       "                      -1.2578e-01, -9.4192e-03, -1.0062e-01,  3.8941e-02,  8.4044e-02,\n",
       "                      -6.6719e-02, -7.9335e-02,  9.2641e-02,  7.0228e-02, -4.3094e-02,\n",
       "                      -1.1572e-01,  1.2255e-01, -1.3034e-01,  1.6351e-01,  1.3867e-01,\n",
       "                       5.1410e-02, -1.0764e-01, -1.0850e-01,  6.5377e-02,  1.0242e-01,\n",
       "                       2.1611e-02, -6.2438e-02,  8.9474e-02, -8.6907e-02, -7.8912e-02,\n",
       "                      -1.7581e-01, -1.0964e-01,  1.0431e-01,  9.1951e-02, -9.7691e-02,\n",
       "                       5.9435e-02,  4.5878e-02, -5.6159e-02, -9.4661e-02,  2.6266e-02,\n",
       "                       5.5782e-02, -7.7855e-02,  7.9038e-02, -1.2370e-01,  1.5243e-01,\n",
       "                       7.0215e-02,  1.0145e-01, -9.8108e-02,  2.4790e-02, -5.0341e-02,\n",
       "                      -9.3143e-02,  1.4614e-02, -4.2867e-02, -1.2445e-01, -1.0174e-01,\n",
       "                      -9.0845e-02, -6.4100e-02, -2.5720e-02, -4.0381e-02,  7.5315e-02,\n",
       "                      -3.9592e-02, -1.4090e-01, -4.8104e-02, -7.8327e-02,  6.2074e-02,\n",
       "                       6.4601e-02,  1.0084e-01,  2.2772e-02,  7.7438e-02, -1.2033e-01,\n",
       "                       7.0036e-02, -4.8895e-02,  1.2216e-01,  1.2299e-01,  7.3145e-02,\n",
       "                      -5.3274e-02, -2.6248e-02, -1.0827e-01,  6.5115e-02,  1.0712e-01,\n",
       "                      -1.4822e-01,  1.4562e-01, -4.8130e-02,  1.1387e-01, -7.7189e-02,\n",
       "                       1.3788e-01, -1.0480e-01,  9.8920e-02,  2.6985e-02,  6.9528e-02,\n",
       "                       9.2279e-02,  1.7938e-01, -9.4075e-02, -7.6827e-02, -1.1503e-01,\n",
       "                       5.0444e-02,  1.6479e-01,  1.4301e-01,  9.5692e-02,  2.3328e-02,\n",
       "                       9.0069e-02, -8.4561e-02,  5.4368e-02, -1.6481e-01,  1.2694e-01,\n",
       "                       6.2811e-02, -5.1521e-02, -6.2571e-02,  1.4437e-01, -5.2099e-02,\n",
       "                      -2.9954e-02, -1.5124e-02,  4.9730e-02, -1.1576e-01,  7.9603e-02,\n",
       "                      -7.1553e-02,  9.0832e-02, -1.6735e-02,  7.9910e-02, -1.1012e-01,\n",
       "                       1.0987e-01, -7.7786e-02, -7.5440e-02,  1.3304e-02, -3.2032e-02,\n",
       "                       1.0104e-01,  1.6986e-02,  1.2356e-01, -3.7403e-02,  9.7819e-02,\n",
       "                       2.3409e-02,  1.2298e-01, -2.7800e-02, -1.4898e-01, -1.0278e-01,\n",
       "                       1.0834e-01,  8.1587e-02,  1.6128e-03,  1.4723e-01, -4.5908e-03,\n",
       "                      -1.0091e-01, -1.4955e-01, -5.9062e-02, -1.0356e-01, -1.0160e-01,\n",
       "                       1.3165e-01, -2.1612e-02,  3.8572e-02,  8.1855e-02, -9.2197e-02,\n",
       "                       8.2087e-02, -6.2656e-02,  1.0938e-01,  1.0390e-03,  4.9807e-03,\n",
       "                       8.0127e-02,  6.9611e-02,  1.2697e-01, -2.7721e-02], device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.0.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.0.key.weight',\n",
       "              tensor([[ 0.0261, -0.0348,  0.0273,  ..., -0.0032, -0.0039,  0.0125],\n",
       "                      [-0.0047,  0.0095,  0.0078,  ...,  0.0066,  0.0564,  0.0278],\n",
       "                      [ 0.0692, -0.0422,  0.0120,  ...,  0.0048, -0.0220, -0.0091],\n",
       "                      ...,\n",
       "                      [ 0.0504, -0.1139,  0.0881,  ..., -0.0197, -0.0247, -0.0115],\n",
       "                      [-0.0039,  0.0208,  0.0941,  ..., -0.0592, -0.0038, -0.0085],\n",
       "                      [-0.0042,  0.0404, -0.0316,  ...,  0.0324,  0.0393, -0.0190]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.0.query.weight',\n",
       "              tensor([[-0.0319,  0.0092,  0.0547,  ...,  0.0279, -0.0176, -0.0335],\n",
       "                      [ 0.0181, -0.0647, -0.0378,  ..., -0.0470, -0.0258,  0.0825],\n",
       "                      [ 0.0221,  0.0234,  0.0293,  ...,  0.0367, -0.0059, -0.0073],\n",
       "                      ...,\n",
       "                      [-0.0201,  0.0139,  0.0335,  ...,  0.0144,  0.0468, -0.0224],\n",
       "                      [ 0.0568, -0.0123, -0.0009,  ..., -0.0216,  0.0021,  0.0882],\n",
       "                      [-0.0437, -0.0100,  0.0146,  ...,  0.0143,  0.0107, -0.0294]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.0.value.weight',\n",
       "              tensor([[ 0.0235, -0.0257, -0.0217,  ...,  0.0038, -0.0172, -0.0099],\n",
       "                      [-0.0165, -0.0153,  0.0177,  ...,  0.0083, -0.0012,  0.0024],\n",
       "                      [-0.0197, -0.0283,  0.0019,  ...,  0.0129,  0.0268, -0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0235,  0.0257, -0.0176,  ...,  0.0038,  0.0270,  0.0008],\n",
       "                      [ 0.0188, -0.0367, -0.0119,  ...,  0.0194,  0.0315, -0.0107],\n",
       "                      [-0.0104, -0.0026, -0.0158,  ..., -0.0184, -0.0308,  0.0058]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.1.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.1.key.weight',\n",
       "              tensor([[ 0.0103,  0.0374, -0.0293,  ...,  0.0010,  0.0368, -0.0159],\n",
       "                      [-0.0354,  0.0261, -0.0424,  ..., -0.0296, -0.0580, -0.0273],\n",
       "                      [-0.0215,  0.0146, -0.0101,  ..., -0.0138, -0.0477, -0.0342],\n",
       "                      ...,\n",
       "                      [-0.0314, -0.0318,  0.0198,  ...,  0.0203,  0.0172, -0.0056],\n",
       "                      [-0.0203,  0.0316,  0.0026,  ..., -0.0165,  0.0214,  0.0019],\n",
       "                      [-0.0202, -0.0103, -0.0017,  ...,  0.0093,  0.0254, -0.0116]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.1.query.weight',\n",
       "              tensor([[-0.0233, -0.0019,  0.0878,  ...,  0.0754, -0.0107, -0.0602],\n",
       "                      [ 0.0267,  0.0394, -0.0113,  ..., -0.0189, -0.0112, -0.0543],\n",
       "                      [ 0.0766, -0.0594,  0.0035,  ..., -0.0304, -0.0153, -0.0041],\n",
       "                      ...,\n",
       "                      [ 0.0227, -0.0497, -0.0401,  ...,  0.0066, -0.0578, -0.0028],\n",
       "                      [-0.0014,  0.0505, -0.0633,  ..., -0.0108, -0.0169,  0.0185],\n",
       "                      [-0.0070, -0.0313, -0.0291,  ...,  0.0591,  0.0039, -0.0521]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.1.value.weight',\n",
       "              tensor([[ 0.0234,  0.0146,  0.0343,  ..., -0.0159,  0.0129,  0.0153],\n",
       "                      [-0.0055, -0.0275, -0.0036,  ..., -0.0082,  0.0143, -0.0014],\n",
       "                      [ 0.0273,  0.0030, -0.0068,  ...,  0.0163, -0.0402, -0.0303],\n",
       "                      ...,\n",
       "                      [ 0.0367, -0.0421,  0.0136,  ..., -0.0250, -0.0198, -0.0096],\n",
       "                      [-0.0451,  0.0240,  0.0214,  ...,  0.0005,  0.0096,  0.0138],\n",
       "                      [ 0.0032, -0.0204, -0.0348,  ...,  0.0032,  0.0216, -0.0276]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.2.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.2.key.weight',\n",
       "              tensor([[ 0.0428, -0.0200, -0.0341,  ..., -0.0198,  0.0074, -0.0413],\n",
       "                      [-0.0254, -0.0181,  0.0113,  ..., -0.0439,  0.0463, -0.0497],\n",
       "                      [-0.0256, -0.0262,  0.0289,  ..., -0.0320, -0.0231, -0.0279],\n",
       "                      ...,\n",
       "                      [ 0.0298, -0.0315,  0.0146,  ...,  0.0215, -0.0048,  0.0230],\n",
       "                      [ 0.1053, -0.0192, -0.0114,  ..., -0.0223,  0.0128,  0.0643],\n",
       "                      [-0.0306, -0.0185,  0.0246,  ...,  0.0560, -0.0151, -0.0390]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.2.query.weight',\n",
       "              tensor([[-0.0458, -0.0188, -0.0601,  ..., -0.0424,  0.0112,  0.0383],\n",
       "                      [-0.0656,  0.0053,  0.0085,  ..., -0.0151,  0.0392,  0.0418],\n",
       "                      [-0.0005,  0.0746,  0.0147,  ...,  0.0327,  0.0003, -0.0555],\n",
       "                      ...,\n",
       "                      [-0.0108,  0.0036, -0.0489,  ..., -0.0089,  0.0105, -0.0248],\n",
       "                      [ 0.0579,  0.0483, -0.0573,  ..., -0.0082, -0.0127,  0.0280],\n",
       "                      [ 0.0112, -0.0142,  0.0174,  ...,  0.0147, -0.0031, -0.0299]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.2.value.weight',\n",
       "              tensor([[ 0.0120, -0.0015, -0.0003,  ...,  0.0168, -0.0141, -0.0014],\n",
       "                      [ 0.0048, -0.0104,  0.0086,  ..., -0.0463,  0.0179,  0.0116],\n",
       "                      [ 0.0173, -0.0259, -0.0314,  ...,  0.0144, -0.0189,  0.0370],\n",
       "                      ...,\n",
       "                      [ 0.0129, -0.0212,  0.0241,  ..., -0.0265,  0.0100,  0.0295],\n",
       "                      [ 0.0225, -0.0405, -0.0071,  ...,  0.0194,  0.0137,  0.0173],\n",
       "                      [ 0.0117,  0.0028, -0.0052,  ...,  0.0229,  0.0022, -0.0373]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.3.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.3.key.weight',\n",
       "              tensor([[ 0.0159, -0.0633, -0.0075,  ...,  0.0509, -0.0063,  0.0082],\n",
       "                      [ 0.0062, -0.0041,  0.0023,  ..., -0.0642, -0.0190,  0.0477],\n",
       "                      [ 0.0554,  0.0076, -0.0093,  ...,  0.0551,  0.0008, -0.0214],\n",
       "                      ...,\n",
       "                      [ 0.0272,  0.0021,  0.0302,  ..., -0.0049, -0.0635,  0.0235],\n",
       "                      [ 0.0495,  0.0159, -0.0797,  ..., -0.0078, -0.0275, -0.0426],\n",
       "                      [-0.0029,  0.0066, -0.0192,  ..., -0.0241,  0.0780,  0.0234]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.3.query.weight',\n",
       "              tensor([[ 0.0684,  0.0210,  0.0304,  ..., -0.0033,  0.0248,  0.0272],\n",
       "                      [ 0.0029, -0.0236,  0.0071,  ...,  0.0284,  0.0442, -0.0469],\n",
       "                      [ 0.0055, -0.0297,  0.0394,  ..., -0.0564, -0.0139, -0.0075],\n",
       "                      ...,\n",
       "                      [-0.0603,  0.0080, -0.0338,  ...,  0.0157, -0.0038, -0.0407],\n",
       "                      [-0.0022,  0.0550, -0.0074,  ..., -0.0084,  0.0328, -0.0194],\n",
       "                      [-0.0160,  0.0168,  0.0686,  ..., -0.0164,  0.0147, -0.0372]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.3.value.weight',\n",
       "              tensor([[-1.4933e-02, -2.3731e-02,  1.8223e-03,  ..., -2.8423e-03,\n",
       "                       -1.8609e-03, -2.0064e-02],\n",
       "                      [-4.4886e-02, -1.4341e-02, -1.5787e-02,  ..., -6.2211e-04,\n",
       "                        4.2490e-05, -2.7657e-02],\n",
       "                      [ 3.7902e-03,  2.6408e-02, -2.5452e-02,  ..., -2.8716e-02,\n",
       "                        1.5443e-02, -1.0251e-03],\n",
       "                      ...,\n",
       "                      [ 4.0982e-02,  3.6682e-02,  3.0192e-03,  ...,  2.2294e-02,\n",
       "                        2.2546e-02, -2.4812e-03],\n",
       "                      [-4.8236e-03, -3.1971e-03, -4.3585e-02,  ..., -3.9618e-03,\n",
       "                       -4.9403e-02,  1.4460e-02],\n",
       "                      [ 1.9700e-02, -3.3000e-03,  3.6838e-02,  ...,  2.7211e-03,\n",
       "                       -1.3353e-03,  2.8108e-02]], device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.4.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.4.key.weight',\n",
       "              tensor([[ 0.0098, -0.0028, -0.0299,  ..., -0.0386,  0.0260,  0.0759],\n",
       "                      [-0.0493,  0.0499,  0.0273,  ...,  0.0166,  0.0350,  0.0250],\n",
       "                      [-0.0005,  0.0152,  0.0370,  ...,  0.0304, -0.0236, -0.0544],\n",
       "                      ...,\n",
       "                      [ 0.0094, -0.0119,  0.0265,  ...,  0.0364,  0.0014,  0.0325],\n",
       "                      [ 0.0135,  0.0323, -0.0442,  ...,  0.0383, -0.0379, -0.0232],\n",
       "                      [ 0.0092, -0.0076, -0.0501,  ..., -0.0216, -0.0962, -0.0535]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.4.query.weight',\n",
       "              tensor([[ 0.0278,  0.0227,  0.0220,  ..., -0.0273,  0.0291, -0.0053],\n",
       "                      [ 0.0201,  0.0331, -0.0427,  ..., -0.0645, -0.0761,  0.0186],\n",
       "                      [ 0.0418, -0.0143, -0.0627,  ..., -0.0104, -0.0271,  0.0010],\n",
       "                      ...,\n",
       "                      [-0.0178,  0.0122, -0.0099,  ..., -0.0031,  0.0542,  0.0137],\n",
       "                      [ 0.0222,  0.0046, -0.0551,  ...,  0.0407,  0.0001,  0.0388],\n",
       "                      [-0.0289, -0.0314,  0.0247,  ...,  0.0394,  0.0472, -0.0911]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.4.value.weight',\n",
       "              tensor([[ 0.0253,  0.0314, -0.0043,  ...,  0.0233, -0.0149,  0.0372],\n",
       "                      [-0.0053, -0.0210, -0.0211,  ..., -0.0081, -0.0364, -0.0048],\n",
       "                      [ 0.0075, -0.0061, -0.0131,  ...,  0.0201, -0.0105,  0.0215],\n",
       "                      ...,\n",
       "                      [-0.0054,  0.0034,  0.0362,  ...,  0.0439, -0.0213, -0.0238],\n",
       "                      [ 0.0317,  0.0245,  0.0148,  ..., -0.0224, -0.0016,  0.0172],\n",
       "                      [-0.0560,  0.0086,  0.0189,  ..., -0.0068,  0.0005, -0.0161]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.5.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.5.key.weight',\n",
       "              tensor([[ 7.0662e-03,  1.2952e-02, -1.7433e-02,  ...,  3.8814e-02,\n",
       "                        3.8359e-02,  1.3897e-02],\n",
       "                      [ 2.0728e-02, -5.4690e-02, -7.3041e-03,  ...,  1.5379e-02,\n",
       "                       -2.9914e-02, -2.3580e-02],\n",
       "                      [-4.7283e-02,  3.1122e-02, -3.2899e-02,  ..., -5.2709e-02,\n",
       "                       -2.3569e-02,  6.7417e-03],\n",
       "                      ...,\n",
       "                      [ 7.0088e-03, -1.4382e-05,  4.1061e-02,  ..., -2.0019e-02,\n",
       "                        8.9762e-02,  2.0313e-02],\n",
       "                      [ 3.5893e-02,  9.3403e-03,  6.1010e-02,  ...,  4.7648e-02,\n",
       "                        3.4971e-02,  2.5378e-02],\n",
       "                      [-8.2162e-02, -8.8545e-03, -2.0527e-02,  ...,  1.5585e-02,\n",
       "                        3.1284e-02, -6.2433e-02]], device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.5.query.weight',\n",
       "              tensor([[-0.0149, -0.0650,  0.0219,  ..., -0.0372,  0.0251, -0.0095],\n",
       "                      [-0.0280,  0.0023,  0.0313,  ...,  0.0072, -0.0158, -0.0192],\n",
       "                      [ 0.0508,  0.0083, -0.0369,  ...,  0.0324,  0.0014,  0.0063],\n",
       "                      ...,\n",
       "                      [ 0.0565, -0.0512, -0.0586,  ..., -0.0343, -0.0516, -0.0323],\n",
       "                      [ 0.0283, -0.0166, -0.0444,  ..., -0.0230, -0.0232,  0.0243],\n",
       "                      [-0.0621,  0.0037,  0.0024,  ...,  0.0542,  0.0183, -0.0008]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.heads.5.value.weight',\n",
       "              tensor([[-0.0043, -0.0429, -0.0177,  ..., -0.0065,  0.0101, -0.0248],\n",
       "                      [-0.0103,  0.0026, -0.0087,  ...,  0.0276,  0.0277, -0.0279],\n",
       "                      [ 0.0146, -0.0135, -0.0294,  ...,  0.0061,  0.0458,  0.0187],\n",
       "                      ...,\n",
       "                      [-0.0183, -0.0071,  0.0152,  ..., -0.0179,  0.0326, -0.0104],\n",
       "                      [ 0.0066,  0.0341,  0.0335,  ..., -0.0144, -0.0008,  0.0270],\n",
       "                      [ 0.0061,  0.0252,  0.0388,  ...,  0.0229, -0.0067, -0.0093]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.sa.proj.weight',\n",
       "              tensor([[ 3.8039e-03,  2.1948e-02, -5.2099e-03,  ...,  3.5816e-03,\n",
       "                       -3.2554e-02,  2.7162e-02],\n",
       "                      [-2.0726e-02,  9.5391e-03,  2.7726e-02,  ..., -3.1507e-02,\n",
       "                       -2.4560e-03, -4.6712e-03],\n",
       "                      [-7.9559e-04,  4.1981e-03,  2.3858e-02,  ..., -1.3327e-02,\n",
       "                        4.3413e-02, -3.8888e-02],\n",
       "                      ...,\n",
       "                      [ 1.2280e-02, -1.8596e-02, -5.0918e-03,  ..., -1.6436e-02,\n",
       "                       -1.9082e-02, -7.0481e-02],\n",
       "                      [ 4.3302e-03, -3.1022e-04, -9.6821e-03,  ...,  1.4498e-02,\n",
       "                       -5.2752e-03,  3.6495e-02],\n",
       "                      [ 2.5283e-03,  2.9194e-02,  5.4959e-03,  ..., -2.2179e-02,\n",
       "                       -9.1570e-05,  4.2634e-02]], device='cuda:0')),\n",
       "             ('blocks.5.sa.proj.bias',\n",
       "              tensor([ 1.2962e-02, -7.2019e-03,  4.9504e-04, -7.0720e-03, -1.7317e-03,\n",
       "                      -1.0227e-02,  9.2496e-03, -9.9886e-03,  1.6149e-03,  5.2558e-03,\n",
       "                      -3.8842e-03,  1.5318e-03,  3.5675e-05, -1.1136e-02, -8.9353e-03,\n",
       "                       7.8780e-03,  4.1551e-03, -3.1023e-03, -3.4446e-03, -4.5927e-03,\n",
       "                      -9.6556e-04, -1.2432e-03,  3.4247e-03,  7.8812e-03, -1.6648e-04,\n",
       "                      -3.1697e-03, -8.5386e-03,  1.2321e-02, -1.0567e-02, -1.2968e-02,\n",
       "                      -6.5191e-03, -8.7435e-03,  1.0632e-02,  9.0891e-04,  4.4413e-03,\n",
       "                       9.9219e-03, -2.5764e-03,  2.3892e-03, -4.0837e-04, -1.6378e-03,\n",
       "                      -4.7304e-03, -6.6657e-03, -4.5534e-03,  3.5329e-03,  1.1674e-03,\n",
       "                       2.7973e-03,  7.5043e-03,  4.6408e-03, -5.9035e-04, -9.0684e-03,\n",
       "                       1.6035e-03,  1.0935e-02,  8.5165e-03,  1.1847e-02, -1.8934e-03,\n",
       "                       1.5765e-02, -8.4693e-03,  7.9464e-03,  6.4176e-03,  1.0202e-02,\n",
       "                      -1.8085e-02,  6.4827e-03,  1.3553e-03,  8.8959e-03,  1.0481e-02,\n",
       "                       6.4858e-04, -1.4511e-03, -3.7438e-03, -2.3625e-03, -6.9091e-03,\n",
       "                       7.3660e-03,  6.6031e-03,  1.2946e-03, -3.2560e-03,  5.8904e-03,\n",
       "                       5.0922e-03,  4.9792e-03,  3.0744e-03, -1.1621e-03, -6.7287e-03,\n",
       "                       7.6651e-03,  7.7161e-03, -4.7832e-05, -2.0443e-02,  5.3582e-03,\n",
       "                       4.5578e-03,  1.2029e-02,  1.3945e-02,  6.3013e-03,  2.0809e-02,\n",
       "                       2.8204e-03, -3.9134e-03,  7.7638e-04, -2.0819e-03,  1.2603e-02,\n",
       "                       4.2630e-03, -3.6698e-03,  7.8426e-03, -7.8804e-04, -7.3309e-03,\n",
       "                      -6.4019e-03,  1.1725e-02, -4.1095e-03,  1.1337e-02, -6.4028e-05,\n",
       "                      -5.8086e-03,  1.4370e-02,  5.1670e-03,  2.2501e-03, -4.5875e-03,\n",
       "                      -1.3112e-02, -4.2093e-03, -1.9848e-03, -2.4943e-05, -1.0579e-03,\n",
       "                       6.3433e-03,  1.5199e-02,  3.3016e-03,  7.2929e-03, -4.6914e-03,\n",
       "                      -3.0280e-03,  8.1868e-03, -7.3525e-04,  2.8614e-03, -2.9409e-03,\n",
       "                       8.5610e-03, -7.4109e-06,  1.6280e-03, -2.9883e-02, -2.1136e-03,\n",
       "                       8.4623e-03, -8.3975e-03, -1.2011e-03, -6.3756e-03, -6.1390e-03,\n",
       "                      -6.4619e-03, -5.0852e-03,  4.6001e-03,  4.7867e-03, -1.4489e-02,\n",
       "                      -6.8220e-03,  1.1208e-02, -6.4811e-03, -5.7995e-03, -9.0178e-03,\n",
       "                       1.0432e-04,  8.3565e-03,  1.0637e-02,  8.2551e-03, -1.0763e-02,\n",
       "                      -1.2389e-02, -7.2224e-03,  1.8697e-02, -9.4662e-03,  9.8712e-03,\n",
       "                      -1.6328e-03,  2.9702e-03, -4.6343e-03,  5.0815e-03,  1.9204e-03,\n",
       "                       1.0827e-03,  2.5099e-03,  1.8538e-03,  5.4055e-03,  5.7125e-03,\n",
       "                       5.6235e-03,  4.2112e-03,  3.9047e-03, -2.3114e-02, -8.0876e-04,\n",
       "                      -2.1600e-03, -6.3959e-03, -1.7866e-03,  9.7210e-03, -1.3102e-02,\n",
       "                      -2.6061e-03,  4.4475e-03, -8.6160e-03,  1.0389e-03, -2.0337e-02,\n",
       "                      -7.2969e-03,  4.3975e-03, -2.1009e-02,  1.7196e-02, -7.1660e-03,\n",
       "                      -1.2925e-02,  1.4188e-03,  8.4704e-03,  4.8184e-03,  5.3915e-03,\n",
       "                      -1.1404e-02,  2.3643e-03,  1.1247e-03,  1.3798e-03, -1.8828e-03,\n",
       "                      -3.0327e-04,  1.0157e-03,  1.0002e-02, -1.3952e-02, -3.9152e-03,\n",
       "                       5.2425e-03,  2.6270e-03,  5.3816e-03,  2.6541e-03,  1.0521e-02,\n",
       "                       1.7468e-03, -9.6794e-03,  5.6906e-04, -6.5355e-04, -6.7484e-03,\n",
       "                      -3.4968e-03, -9.1486e-03,  1.0517e-02, -1.7643e-03, -2.9383e-03,\n",
       "                      -1.4144e-02, -4.9588e-03, -1.4732e-02,  1.1536e-03,  2.0514e-02,\n",
       "                      -5.9236e-04, -8.0774e-04, -2.3918e-03, -1.0480e-02,  1.5101e-04,\n",
       "                       4.6417e-03, -1.8491e-03,  4.8055e-04,  2.9549e-03, -5.1616e-03,\n",
       "                      -3.4045e-03,  7.5593e-03, -9.9176e-03,  1.2982e-02, -1.3112e-03,\n",
       "                      -2.2932e-03, -1.0139e-02,  1.5868e-02,  9.4728e-04, -6.0135e-03,\n",
       "                      -5.6032e-03, -8.2308e-03,  3.6394e-03, -8.7287e-03, -2.7024e-03,\n",
       "                       3.1828e-03,  2.3718e-03, -2.3364e-03,  8.3155e-03, -3.2358e-03,\n",
       "                       1.4684e-02,  1.8213e-03,  6.0685e-03, -6.5023e-03, -8.7417e-03,\n",
       "                      -1.4539e-04,  1.3959e-02,  1.6247e-02, -8.8159e-03,  4.3205e-04,\n",
       "                       1.1177e-02,  1.5120e-02, -1.2348e-02,  1.5129e-03,  4.9553e-03,\n",
       "                       8.4264e-03, -7.3196e-03, -2.8914e-03,  9.4332e-03,  3.6395e-03,\n",
       "                       2.3542e-03,  1.8682e-04,  7.5975e-03,  6.5521e-03, -9.9149e-03,\n",
       "                      -7.9612e-04,  2.4844e-03,  9.7400e-03,  3.1241e-03,  6.6434e-04,\n",
       "                       3.4720e-03, -5.4926e-03, -2.8010e-03, -1.1396e-02, -2.9062e-03,\n",
       "                       2.9482e-03,  1.0352e-02, -2.9998e-03,  3.5929e-03,  9.4897e-03,\n",
       "                       1.3994e-02, -3.7309e-03,  6.7643e-03,  2.2352e-03,  1.6778e-03,\n",
       "                       5.5100e-03, -1.8675e-02, -4.1021e-03, -4.6189e-03,  7.0874e-03,\n",
       "                      -8.4630e-03, -5.2904e-03, -2.7742e-03,  3.4605e-03, -1.0892e-02,\n",
       "                      -4.0916e-04, -1.0317e-02, -1.0808e-02,  6.7150e-03, -1.9965e-03,\n",
       "                      -2.8251e-03,  5.4105e-03, -4.1518e-03, -1.0288e-03,  2.8529e-03,\n",
       "                       1.0443e-02, -1.4755e-02,  7.2707e-03, -1.1405e-03, -1.7749e-03,\n",
       "                       8.5274e-03, -1.2642e-03, -2.9051e-03,  2.6774e-03,  1.0558e-03,\n",
       "                      -9.0098e-04, -1.0762e-02,  8.2062e-03,  4.8067e-03, -7.9630e-03,\n",
       "                       1.2033e-03,  1.9251e-03, -1.7752e-02, -5.7566e-03, -1.4297e-02,\n",
       "                      -9.6832e-03, -1.6986e-03, -2.0035e-02, -7.5113e-03,  6.1028e-03,\n",
       "                       1.3270e-03,  9.4872e-03,  7.8096e-03,  3.0499e-03, -1.2074e-03,\n",
       "                       2.6614e-03, -7.0524e-03, -6.3566e-03, -5.6891e-03, -1.0075e-02,\n",
       "                       7.1046e-03,  9.4267e-03, -1.1422e-02, -1.7624e-03,  1.3532e-02,\n",
       "                       1.2455e-02, -1.1689e-03, -1.0136e-02, -3.6643e-04,  5.1598e-03,\n",
       "                       1.8787e-02,  2.4978e-03,  1.0651e-02, -4.9022e-03,  1.0635e-02,\n",
       "                       5.4378e-03, -3.3743e-03, -1.2700e-02,  1.2175e-02,  8.8654e-03,\n",
       "                      -1.4691e-02, -1.3102e-03,  1.1223e-02,  1.0944e-02,  8.5868e-03,\n",
       "                      -3.1838e-03,  1.2220e-02,  5.3177e-04,  1.4395e-02, -1.6379e-02,\n",
       "                       1.2740e-02,  2.7657e-04, -9.8824e-04, -9.2284e-03], device='cuda:0')),\n",
       "             ('blocks.5.ffwd.net.0.weight',\n",
       "              tensor([[ 2.8617e-02, -1.3987e-01, -1.8023e-02,  ...,  4.3463e-02,\n",
       "                       -4.1178e-03,  3.9265e-02],\n",
       "                      [-1.0577e-02, -2.1329e-03, -4.1153e-02,  ..., -1.2099e-02,\n",
       "                        6.6690e-03,  2.1741e-02],\n",
       "                      [ 3.1183e-02, -3.0618e-02, -1.1277e-02,  ..., -7.7661e-02,\n",
       "                       -2.0767e-02, -3.2440e-02],\n",
       "                      ...,\n",
       "                      [ 5.3224e-05, -4.9317e-02, -1.9080e-03,  ..., -7.5474e-03,\n",
       "                        8.6371e-04,  9.4361e-03],\n",
       "                      [ 3.4127e-03, -6.1578e-02, -4.3859e-02,  ...,  5.3370e-02,\n",
       "                       -1.6909e-02,  3.5825e-02],\n",
       "                      [-1.9928e-02,  2.1702e-03, -1.2632e-02,  ..., -4.8539e-02,\n",
       "                       -3.1676e-02, -5.0360e-03]], device='cuda:0')),\n",
       "             ('blocks.5.ffwd.net.0.bias',\n",
       "              tensor([-0.0243, -0.0183, -0.0302,  ..., -0.0225, -0.0219, -0.0287],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.ffwd.net.2.weight',\n",
       "              tensor([[-0.0175, -0.0077, -0.0125,  ..., -0.0181,  0.0123,  0.0113],\n",
       "                      [-0.0641, -0.0219,  0.0385,  ..., -0.0628,  0.0166, -0.0008],\n",
       "                      [-0.0377, -0.0203,  0.0133,  ..., -0.0065,  0.0488, -0.0399],\n",
       "                      ...,\n",
       "                      [ 0.0029, -0.0040,  0.0218,  ..., -0.0222, -0.0151, -0.0013],\n",
       "                      [ 0.0367,  0.0224,  0.0172,  ..., -0.0006,  0.0540, -0.0217],\n",
       "                      [-0.0310, -0.0547, -0.0102,  ...,  0.0347, -0.0206,  0.0590]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.ffwd.net.2.bias',\n",
       "              tensor([ 2.3617e-03, -9.2247e-03,  1.0801e-03,  5.0664e-03,  2.2428e-03,\n",
       "                      -1.5578e-03, -2.0078e-03, -6.7928e-03,  6.9789e-04,  1.1923e-02,\n",
       "                       1.8324e-02, -2.5250e-02, -6.4866e-03,  7.2370e-03, -5.8859e-03,\n",
       "                      -3.2827e-03,  1.7457e-02,  1.7179e-02,  1.4274e-02, -7.3566e-03,\n",
       "                       2.4364e-03,  1.6981e-02,  1.0012e-02, -6.8353e-03, -1.3472e-02,\n",
       "                      -5.9711e-03, -1.1406e-02,  1.2424e-02, -1.9175e-03,  6.5038e-03,\n",
       "                      -5.0330e-03,  7.7717e-03, -1.4009e-02, -5.8003e-03, -1.8527e-02,\n",
       "                      -1.4077e-02,  1.9578e-02,  1.2886e-02,  3.2781e-03,  3.5289e-03,\n",
       "                       8.2694e-04, -6.7270e-03,  1.0822e-02,  1.2309e-02, -1.6495e-02,\n",
       "                       6.3350e-03,  1.3317e-02, -1.4189e-02, -1.1082e-02,  5.0284e-03,\n",
       "                      -6.3023e-03, -1.7209e-02,  1.9349e-03,  1.2444e-02, -6.7094e-04,\n",
       "                      -4.6668e-03, -6.0642e-03, -1.0630e-02,  7.3231e-03, -1.2931e-02,\n",
       "                       3.8693e-03,  1.9128e-02, -1.0676e-02, -3.5753e-03, -7.9235e-03,\n",
       "                      -8.8826e-03,  1.0563e-02, -6.9824e-03, -1.8404e-02, -6.4959e-03,\n",
       "                       3.6247e-05, -2.1528e-02, -8.0916e-03,  9.0517e-03,  2.9917e-03,\n",
       "                      -6.8921e-03, -1.0120e-02,  2.7003e-03,  6.1637e-03,  1.2258e-02,\n",
       "                      -8.9245e-04, -6.6020e-03,  5.2362e-03,  8.5397e-03,  2.3452e-03,\n",
       "                       1.6072e-02, -6.5132e-03, -1.3701e-02, -1.3372e-02,  4.8105e-03,\n",
       "                      -7.1863e-03, -1.2490e-02, -8.3312e-03,  9.4811e-03, -7.6166e-03,\n",
       "                      -1.3384e-02,  3.6266e-03,  2.0107e-02,  6.9363e-03,  7.8619e-03,\n",
       "                      -6.6999e-03, -1.1168e-02, -7.0141e-03, -7.5304e-03, -9.2404e-03,\n",
       "                       1.2736e-02, -7.6073e-05, -1.4762e-02, -9.1217e-03,  6.1222e-03,\n",
       "                       1.8618e-02, -1.3100e-03,  3.9244e-03, -1.3524e-02,  1.6833e-02,\n",
       "                       4.9530e-04, -4.8979e-03, -1.1092e-02, -1.2105e-02,  7.8749e-03,\n",
       "                      -1.6164e-03,  1.2015e-02, -1.3550e-02, -1.3683e-02,  3.4657e-04,\n",
       "                      -2.2609e-02, -1.0320e-02, -9.8229e-03, -5.0621e-03,  9.0238e-03,\n",
       "                      -1.4537e-02, -1.0336e-02, -1.4987e-02, -6.4050e-03,  8.9849e-03,\n",
       "                      -1.1749e-02,  7.3887e-03,  1.8172e-02,  2.4955e-03, -6.1121e-03,\n",
       "                       4.8081e-03, -1.8103e-02,  6.5104e-03,  9.8865e-03,  4.8111e-03,\n",
       "                      -8.6387e-03,  1.1496e-02, -1.6576e-02, -3.0571e-03,  9.9260e-03,\n",
       "                      -7.8070e-04,  8.2412e-03,  4.0544e-03, -1.2883e-02, -9.4366e-03,\n",
       "                       1.0970e-02, -1.3125e-02,  4.2204e-03,  1.5205e-02,  4.1971e-03,\n",
       "                       1.5663e-02, -2.1913e-04,  1.5930e-02,  4.8394e-03, -8.3503e-03,\n",
       "                       1.0956e-02,  1.5943e-02, -1.6380e-02, -1.1390e-02, -6.1460e-03,\n",
       "                      -5.3447e-03,  1.2541e-02, -3.7779e-03, -1.2462e-02, -9.7070e-03,\n",
       "                       7.0673e-03, -1.7997e-02, -5.7659e-03, -1.0944e-02,  9.2608e-03,\n",
       "                       9.9949e-03, -2.0627e-02,  1.3022e-02,  6.6143e-03,  1.2170e-02,\n",
       "                       5.1004e-03,  1.5507e-02, -1.5561e-02, -5.6177e-03,  8.7148e-03,\n",
       "                       8.8829e-03,  4.5883e-03,  8.8685e-04, -1.3978e-02,  3.1647e-03,\n",
       "                       1.0184e-03,  1.0144e-02,  1.1277e-02,  1.9754e-03, -7.7423e-03,\n",
       "                      -9.9453e-03, -1.5967e-02, -1.2407e-02,  1.6397e-03, -6.2149e-03,\n",
       "                       9.3733e-03,  1.4336e-02,  9.1396e-03,  1.5327e-03,  7.5160e-03,\n",
       "                      -3.1002e-03,  8.2978e-03,  1.2012e-02, -5.3995e-03,  1.4038e-02,\n",
       "                       7.0323e-03, -8.2317e-03,  1.9847e-02, -7.7780e-03,  6.8108e-03,\n",
       "                      -6.5492e-03,  1.1475e-02,  2.0823e-02,  5.8466e-03,  1.4912e-02,\n",
       "                       8.1673e-03, -1.4444e-02,  1.0746e-02, -5.1589e-03,  3.9919e-03,\n",
       "                       9.5719e-03, -9.8191e-03,  9.1053e-03, -1.2123e-02, -2.4639e-03,\n",
       "                       1.9884e-03,  5.8287e-03, -1.1518e-02, -1.5818e-02,  6.2640e-03,\n",
       "                       4.8277e-03, -9.0364e-03, -5.3924e-04, -6.7261e-03, -1.7400e-03,\n",
       "                      -5.3719e-03,  5.1034e-03,  8.4499e-03, -6.3831e-03, -1.0679e-02,\n",
       "                       3.2624e-04, -4.2282e-03, -8.5685e-03,  6.5321e-03,  2.4341e-02,\n",
       "                       1.2511e-02,  1.8255e-02, -3.8754e-04, -1.6138e-02,  1.2097e-02,\n",
       "                      -7.4699e-03, -1.1128e-02,  1.0342e-02,  4.6462e-03,  1.6393e-02,\n",
       "                      -9.2370e-03,  1.2920e-02, -9.7129e-03,  6.6159e-03, -6.3988e-03,\n",
       "                      -1.0778e-02, -9.4928e-03,  1.2782e-02, -6.3046e-03,  1.1594e-02,\n",
       "                       7.8133e-03,  7.1001e-04,  2.2214e-03,  1.8705e-02,  1.2968e-02,\n",
       "                       9.7246e-03,  3.3695e-04,  1.3301e-02, -1.5220e-03, -1.6671e-02,\n",
       "                      -1.5600e-03,  2.2658e-02,  3.3299e-04,  1.8840e-02, -8.5477e-03,\n",
       "                      -1.5911e-03, -7.7227e-03, -6.6794e-03, -1.0493e-02,  3.9934e-03,\n",
       "                      -1.3514e-02,  1.7811e-02, -1.4321e-02, -1.0847e-02, -5.9733e-03,\n",
       "                       1.5976e-02,  1.5788e-02,  4.2135e-03, -5.6624e-03, -8.2865e-03,\n",
       "                       1.1126e-02, -1.4363e-03,  1.4636e-02, -7.8831e-03,  1.2802e-02,\n",
       "                      -4.6479e-03,  1.2105e-02, -1.7437e-02, -3.8801e-03, -1.0898e-02,\n",
       "                      -8.7321e-03, -1.2673e-02,  8.1320e-03,  6.2738e-03,  7.0449e-03,\n",
       "                      -9.2161e-03, -9.3190e-03, -9.7758e-03, -9.5975e-03, -2.6293e-03,\n",
       "                      -9.9908e-03,  1.0697e-02, -2.1261e-03,  1.5099e-02, -1.0014e-02,\n",
       "                      -8.4084e-03,  4.4286e-03,  1.1130e-02, -5.5319e-03,  1.0327e-02,\n",
       "                       4.0002e-04,  1.6380e-02,  8.9206e-03,  1.0496e-02, -1.3331e-02,\n",
       "                       2.0063e-02, -1.0833e-02, -5.2380e-03, -1.0709e-02,  7.3857e-03,\n",
       "                      -6.6350e-03,  2.0074e-02,  6.6798e-03,  3.1380e-03,  2.4867e-03,\n",
       "                      -1.1510e-02, -8.3260e-03, -1.2978e-02,  1.0031e-02, -1.6450e-02,\n",
       "                      -9.2575e-03, -1.7832e-02,  1.5587e-02,  3.4475e-03,  1.5291e-02,\n",
       "                       4.7741e-03, -1.6389e-02, -5.3921e-03, -7.6850e-03, -9.5383e-03,\n",
       "                       2.2846e-02,  3.5712e-03,  4.6168e-03,  9.3566e-03,  1.4537e-02,\n",
       "                      -2.2843e-03,  1.0137e-02, -7.4180e-03, -1.9371e-02,  7.6009e-03,\n",
       "                      -1.4898e-02, -4.5408e-03, -5.4480e-03, -7.8375e-03,  8.1409e-03,\n",
       "                      -3.0220e-03, -1.0280e-02, -6.6061e-03,  5.2675e-03], device='cuda:0')),\n",
       "             ('blocks.5.ln1.weight',\n",
       "              tensor([0.9365, 0.9078, 0.9277, 0.9464, 0.9251, 0.9261, 0.9116, 0.9008, 0.9306,\n",
       "                      0.9345, 0.9358, 0.9211, 0.9107, 0.9060, 0.9088, 0.9177, 0.8992, 0.9578,\n",
       "                      0.8909, 0.9108, 0.9512, 0.9140, 0.9264, 0.9418, 0.9460, 0.9308, 0.9478,\n",
       "                      0.9142, 0.8919, 0.9134, 0.9161, 0.9511, 0.9160, 0.8938, 0.8970, 0.9310,\n",
       "                      0.9087, 0.9287, 0.9049, 0.9144, 0.9525, 0.9079, 0.9358, 0.9357, 0.9141,\n",
       "                      0.9265, 0.9226, 0.8970, 0.9143, 0.9553, 0.9633, 0.9077, 0.9384, 0.9822,\n",
       "                      0.9190, 0.8861, 0.9048, 0.9138, 0.9493, 0.8867, 0.9044, 0.9178, 0.9135,\n",
       "                      0.9155, 0.9179, 0.9191, 0.9204, 0.9417, 0.9150, 0.9503, 0.9329, 0.9748,\n",
       "                      0.9224, 0.9086, 0.9528, 0.9278, 0.9310, 0.9317, 0.8615, 0.9139, 0.9271,\n",
       "                      0.9078, 0.9271, 0.9162, 0.9248, 0.9300, 0.9545, 0.9125, 0.9264, 0.9277,\n",
       "                      0.9119, 0.9246, 0.9511, 0.9172, 0.9455, 0.9417, 0.9335, 0.9188, 0.8905,\n",
       "                      0.9467, 0.9110, 0.9511, 0.9355, 0.9259, 0.9480, 0.8964, 0.9135, 0.9256,\n",
       "                      0.9142, 0.9099, 0.9068, 0.9246, 0.9280, 0.8984, 0.9260, 0.8833, 0.9131,\n",
       "                      0.9239, 0.9118, 0.9472, 0.9273, 0.9574, 0.9251, 0.9317, 0.9239, 0.9196,\n",
       "                      0.9341, 0.8990, 0.8367, 0.9521, 0.9294, 0.9577, 0.9505, 0.9073, 0.9035,\n",
       "                      0.9500, 0.9374, 0.9551, 0.9422, 0.8769, 0.9145, 0.8943, 0.9254, 0.8958,\n",
       "                      0.9051, 0.9460, 0.9139, 0.9152, 0.9113, 0.9178, 0.9410, 0.9186, 0.9225,\n",
       "                      0.9423, 0.9234, 0.9221, 0.9397, 0.9016, 0.9577, 0.9516, 0.9515, 0.9209,\n",
       "                      0.8852, 0.9605, 0.9394, 0.9348, 0.9372, 0.9443, 0.8800, 0.9475, 0.9217,\n",
       "                      0.9462, 0.9367, 0.8932, 0.9732, 0.9271, 0.9236, 0.9304, 0.9413, 0.9088,\n",
       "                      0.9283, 0.9203, 0.9169, 0.8298, 0.9058, 0.9295, 0.9275, 0.9539, 0.9191,\n",
       "                      0.9118, 0.9674, 0.9496, 0.9091, 0.9309, 0.9427, 0.9436, 0.9105, 0.9405,\n",
       "                      0.9115, 0.9055, 0.9313, 0.9549, 0.9407, 0.9513, 0.8977, 0.9439, 0.9494,\n",
       "                      0.9285, 0.9138, 0.9239, 0.9446, 0.9141, 0.9369, 0.9137, 0.9114, 0.9137,\n",
       "                      0.9177, 0.9182, 0.9129, 0.9042, 0.9187, 0.8943, 0.9344, 0.9461, 0.9171,\n",
       "                      0.9278, 0.9376, 0.9335, 0.9506, 0.9073, 0.9182, 0.9132, 0.9142, 0.9100,\n",
       "                      0.8880, 0.9186, 0.9220, 0.9233, 0.9455, 0.9283, 0.8883, 0.9279, 0.9198,\n",
       "                      0.9380, 0.9232, 0.9087, 0.9509, 0.8665, 0.9335, 0.9544, 0.8925, 0.9166,\n",
       "                      0.9310, 0.9039, 0.9081, 0.9443, 0.9613, 0.9146, 0.9459, 0.9312, 0.8990,\n",
       "                      0.9187, 0.9198, 0.9313, 0.8892, 0.9169, 0.8631, 0.9126, 0.9246, 0.9191,\n",
       "                      0.9128, 0.8935, 0.9370, 0.9121, 0.9014, 0.9233, 0.9304, 0.9295, 0.9151,\n",
       "                      0.9393, 0.9002, 0.9377, 0.9464, 0.9882, 0.9285, 0.8960, 0.9041, 0.9583,\n",
       "                      0.9423, 0.8793, 0.9386, 0.9338, 0.9211, 0.9177, 0.9309, 0.9202, 0.8869,\n",
       "                      0.9251, 0.9330, 0.8854, 0.9391, 0.9539, 0.9293, 0.8948, 0.9152, 0.9375,\n",
       "                      0.9219, 0.9153, 0.9055, 0.9493, 0.8700, 0.9387, 0.9326, 0.9353, 0.9090,\n",
       "                      0.9163, 0.9807, 0.9628, 0.9079, 0.9431, 0.9559, 0.9234, 0.9499, 0.8865,\n",
       "                      0.9111, 0.9072, 0.9361, 0.9263, 0.9685, 0.9464, 0.8966, 0.9277, 0.9348,\n",
       "                      0.9450, 0.9127, 0.9072, 0.9404, 0.9412, 0.9134, 0.9349, 0.9254, 0.9066,\n",
       "                      0.9270, 0.9228, 0.8919, 0.8799, 0.9235, 0.9566, 0.9095, 0.9335, 0.9611,\n",
       "                      0.9338, 0.9235, 0.9150, 0.8890, 0.8971, 0.9289, 0.9052, 0.9355, 0.8997,\n",
       "                      0.8399, 0.9227, 0.9141, 0.9086, 0.9206, 0.9280, 0.8983, 0.8574, 0.9142,\n",
       "                      0.9549, 0.9254, 0.9308, 0.9128, 0.9076, 0.9233, 0.9103, 0.9559, 0.9215,\n",
       "                      0.9132, 0.9363, 0.8822, 0.9331, 0.9375, 0.9065], device='cuda:0')),\n",
       "             ('blocks.5.ln1.bias',\n",
       "              tensor([-0.0058, -0.0053,  0.0004,  0.0145,  0.0193, -0.0346,  0.0086, -0.0434,\n",
       "                       0.0384,  0.0107,  0.0284, -0.0147, -0.0332,  0.0494, -0.0270, -0.0527,\n",
       "                       0.0147,  0.0141,  0.0536, -0.0098,  0.0259,  0.0450,  0.0297, -0.0552,\n",
       "                      -0.0430, -0.0502, -0.0331,  0.0237,  0.0528,  0.0338, -0.0402,  0.0056,\n",
       "                       0.0033, -0.0441, -0.0044,  0.0249,  0.0461,  0.0278,  0.0519,  0.0409,\n",
       "                       0.0116, -0.0421,  0.0425,  0.0254, -0.0282,  0.0279,  0.0168,  0.0059,\n",
       "                      -0.0533,  0.0526, -0.0149, -0.0652, -0.0467,  0.0151,  0.0420, -0.0576,\n",
       "                       0.0582, -0.0525, -0.0153, -0.0020,  0.0494,  0.0070, -0.0098, -0.0075,\n",
       "                      -0.0635, -0.0537,  0.0153, -0.0372, -0.0125, -0.0417, -0.0230, -0.0158,\n",
       "                      -0.0362,  0.0179,  0.0252, -0.0581, -0.0145,  0.0490,  0.0206,  0.0216,\n",
       "                      -0.0081, -0.0508,  0.0286,  0.0446,  0.0301,  0.0368, -0.0469, -0.0565,\n",
       "                      -0.0033, -0.0293,  0.0087, -0.0232, -0.0104,  0.0128, -0.0283, -0.0132,\n",
       "                       0.0579,  0.0480,  0.0667,  0.0544,  0.0321, -0.0455, -0.0334, -0.0384,\n",
       "                      -0.0499,  0.0675, -0.0380, -0.0570, -0.0404,  0.0157,  0.0146, -0.0401,\n",
       "                       0.0022, -0.0432,  0.0323, -0.0266, -0.0407, -0.0092, -0.0368,  0.0301,\n",
       "                      -0.0086,  0.0392, -0.0033, -0.0279, -0.0422, -0.0220, -0.0059, -0.0467,\n",
       "                       0.0525,  0.0234, -0.0179, -0.0223, -0.0099, -0.0020,  0.0600, -0.0306,\n",
       "                       0.0208,  0.0021,  0.0391,  0.0620,  0.0564, -0.0712,  0.0630,  0.0073,\n",
       "                       0.0410, -0.0308,  0.0658, -0.0401, -0.0036,  0.0513,  0.0432,  0.0496,\n",
       "                      -0.0280, -0.0241, -0.0443,  0.0112, -0.0240,  0.0593,  0.0095,  0.0458,\n",
       "                       0.0454, -0.0318, -0.0003,  0.0077, -0.0459,  0.0150,  0.0233, -0.0650,\n",
       "                       0.0500, -0.0049, -0.0493,  0.0247,  0.0011, -0.0409,  0.0404,  0.0431,\n",
       "                      -0.0553, -0.0055, -0.0194,  0.0400,  0.0193, -0.0187,  0.0122, -0.0974,\n",
       "                       0.0481,  0.0201,  0.0113, -0.0383, -0.0391,  0.0390,  0.0246,  0.0396,\n",
       "                       0.0324, -0.0580,  0.0324, -0.0012,  0.0092, -0.0012,  0.0246, -0.0212,\n",
       "                      -0.0079, -0.0329, -0.0275,  0.0546, -0.0333,  0.0537,  0.0376,  0.0026,\n",
       "                      -0.0018,  0.0232, -0.0017,  0.0302,  0.0447, -0.0192,  0.0477,  0.0305,\n",
       "                      -0.0432,  0.0322, -0.0457, -0.0448, -0.0313,  0.0179,  0.0275,  0.0010,\n",
       "                      -0.0003, -0.0196, -0.0345,  0.0462, -0.0163,  0.0445,  0.0790, -0.0189,\n",
       "                       0.0676, -0.0595, -0.0111,  0.0528,  0.0284, -0.0350, -0.0213,  0.0284,\n",
       "                       0.0618, -0.0369,  0.0410, -0.0395, -0.0489, -0.0274,  0.0033,  0.0561,\n",
       "                      -0.0539, -0.0274, -0.0427,  0.0129, -0.0223,  0.0483,  0.0582,  0.0472,\n",
       "                      -0.0067, -0.0575, -0.0225,  0.0360, -0.0483, -0.0430,  0.0497,  0.0317,\n",
       "                      -0.0031, -0.0679,  0.0560, -0.0092,  0.0393, -0.0351, -0.0089, -0.0347,\n",
       "                       0.0133, -0.0371,  0.0510,  0.0235, -0.0081,  0.0145,  0.0135,  0.0176,\n",
       "                       0.0405,  0.0449, -0.0078,  0.0290, -0.0038,  0.0225,  0.0087,  0.0081,\n",
       "                       0.0344, -0.0527, -0.0256, -0.0176, -0.0231, -0.0383,  0.0288, -0.0117,\n",
       "                       0.0600, -0.0338, -0.0441, -0.0411,  0.0509,  0.0399,  0.0434, -0.0309,\n",
       "                       0.0038,  0.0340, -0.0368,  0.0184, -0.0472,  0.0036, -0.0600,  0.0335,\n",
       "                      -0.0070,  0.0002, -0.0436, -0.0563, -0.0218, -0.0090,  0.0272,  0.0347,\n",
       "                      -0.0196, -0.0486, -0.0176, -0.0268,  0.0002, -0.0517,  0.0506, -0.0261,\n",
       "                       0.0040, -0.0045, -0.0032,  0.0328,  0.0534, -0.0387,  0.0558,  0.0406,\n",
       "                       0.0300,  0.0336,  0.0533, -0.0264,  0.0371, -0.0479, -0.0218, -0.0399,\n",
       "                       0.0571, -0.0648, -0.0052,  0.0426,  0.0432,  0.0102, -0.0128, -0.0224,\n",
       "                      -0.0126,  0.0341, -0.0483, -0.0485, -0.0356,  0.0194,  0.0321,  0.0361,\n",
       "                      -0.0775, -0.0174, -0.0464, -0.0833, -0.0107,  0.0074,  0.0694,  0.0567,\n",
       "                       0.0275,  0.0063, -0.0398,  0.0346, -0.0723, -0.0286,  0.0041,  0.0075,\n",
       "                      -0.0224, -0.0579, -0.0487,  0.0340, -0.0754, -0.0295, -0.0572,  0.0172],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.5.ln2.weight',\n",
       "              tensor([1.0049, 1.0607, 1.0487, 1.0298, 1.0655, 1.0763, 1.0298, 1.0649, 1.0481,\n",
       "                      1.0159, 1.0169, 1.0296, 1.0693, 1.0257, 1.0405, 1.0797, 1.0442, 1.0792,\n",
       "                      1.0550, 1.0219, 1.0704, 1.0559, 1.0569, 1.0647, 1.0626, 1.0580, 1.0760,\n",
       "                      1.0375, 1.0509, 1.0635, 1.0597, 1.0446, 1.0072, 1.0728, 1.0643, 1.0565,\n",
       "                      1.0493, 1.0561, 1.0407, 1.0402, 1.0751, 1.1155, 1.0455, 1.1001, 1.0363,\n",
       "                      1.0599, 1.1046, 1.0734, 1.0490, 1.0285, 1.0631, 1.0455, 1.0392, 1.0938,\n",
       "                      1.0683, 1.0617, 1.0662, 1.0619, 1.0665, 1.0562, 1.0576, 1.0861, 1.0446,\n",
       "                      1.0581, 1.0480, 1.0623, 1.0116, 1.0488, 1.0550, 1.0560, 1.0461, 1.0641,\n",
       "                      1.0670, 1.0406, 1.0817, 1.0686, 1.0337, 1.0931, 1.0395, 1.0720, 1.0721,\n",
       "                      1.0986, 1.1125, 1.0253, 1.0670, 1.0637, 1.0294, 1.0584, 1.0726, 1.0063,\n",
       "                      1.0764, 1.0665, 1.0690, 1.0579, 1.0516, 1.0360, 1.0689, 1.0652, 1.0654,\n",
       "                      1.0486, 1.0473, 1.0714, 1.0838, 1.0507, 1.0537, 1.0461, 1.0230, 1.0198,\n",
       "                      1.0489, 1.0681, 1.0375, 1.0681, 1.0521, 1.0232, 1.0687, 1.0746, 1.0329,\n",
       "                      1.0484, 1.0597, 1.0427, 1.0807, 1.0584, 1.0660, 1.0272, 1.0467, 1.0550,\n",
       "                      1.0253, 1.0806, 0.9651, 1.0535, 1.0815, 1.0726, 1.0325, 1.0355, 1.0619,\n",
       "                      1.0451, 1.0322, 1.0488, 1.1045, 1.0523, 1.0449, 1.0566, 1.0653, 1.0707,\n",
       "                      1.0276, 1.0661, 1.0866, 1.0309, 1.0049, 1.0471, 1.0717, 1.0222, 1.0555,\n",
       "                      1.0716, 1.0472, 1.0008, 1.0735, 1.0903, 1.0618, 1.0669, 1.0530, 1.0085,\n",
       "                      1.0459, 1.0156, 1.0317, 1.0673, 1.0566, 1.0996, 1.0374, 1.0687, 1.0160,\n",
       "                      1.0451, 1.0483, 1.0373, 1.0601, 1.0411, 1.0202, 1.0434, 1.0290, 1.0653,\n",
       "                      1.0433, 1.0624, 1.0287, 1.0102, 1.0468, 1.0635, 1.0325, 1.0712, 1.0429,\n",
       "                      1.0846, 1.0404, 1.0559, 1.0580, 1.0706, 1.0994, 1.0243, 1.0313, 1.0217,\n",
       "                      1.0706, 1.0970, 1.0238, 1.0247, 1.0381, 1.0916, 1.0340, 1.0706, 1.0655,\n",
       "                      1.0320, 1.0006, 1.0508, 1.0554, 1.0371, 1.0457, 1.0277, 1.0734, 1.0136,\n",
       "                      1.0853, 1.0434, 1.0863, 0.9973, 1.0270, 1.0264, 1.0920, 1.0443, 1.0410,\n",
       "                      1.0887, 1.0890, 1.0815, 1.0151, 1.0469, 1.0618, 1.0040, 1.0526, 1.0276,\n",
       "                      1.0367, 1.0717, 1.0271, 1.1207, 1.0074, 1.0185, 1.1203, 1.0685, 1.1006,\n",
       "                      1.0887, 1.0665, 1.0694, 1.0372, 1.0504, 1.0374, 1.0659, 1.0579, 1.0400,\n",
       "                      1.0487, 1.0891, 1.0376, 1.1046, 1.0547, 1.1133, 1.0663, 1.0387, 1.0593,\n",
       "                      1.0574, 1.0415, 1.0545, 1.0551, 1.0646, 1.0608, 1.0169, 1.0533, 1.0447,\n",
       "                      1.0645, 1.0667, 1.0643, 1.0368, 1.0904, 1.0502, 1.0187, 1.0487, 1.0609,\n",
       "                      1.0665, 1.0950, 1.0677, 1.0314, 1.0328, 1.0300, 1.0509, 1.0556, 1.0422,\n",
       "                      1.0716, 1.0789, 1.0150, 1.0755, 1.0284, 1.0629, 1.1221, 1.0674, 1.0500,\n",
       "                      1.0633, 1.0598, 1.0680, 1.0311, 1.0164, 1.0592, 1.0258, 1.0677, 1.1112,\n",
       "                      1.0940, 1.0523, 1.0767, 1.0379, 1.0805, 1.0476, 1.0204, 1.0416, 1.0557,\n",
       "                      1.0430, 1.1241, 1.0758, 1.0898, 1.0850, 1.0728, 1.0570, 1.0602, 1.0586,\n",
       "                      1.0555, 1.0668, 1.0537, 1.0532, 1.0434, 1.0910, 1.0633, 1.0676, 1.0788,\n",
       "                      1.0753, 1.0528, 1.0245, 1.0421, 1.0527, 1.0592, 1.0389, 1.1092, 1.0545,\n",
       "                      1.0083, 1.0525, 1.0832, 1.0739, 1.0552, 1.0368, 1.0302, 1.0247, 1.0561,\n",
       "                      1.0509, 1.0744, 1.0176, 1.1149, 1.0393, 1.0728, 1.0526, 1.0721, 1.0610,\n",
       "                      1.0276, 1.0460, 1.0705, 1.1218, 1.0280, 1.0857, 1.0542, 1.0606, 1.0467,\n",
       "                      1.0686, 1.1264, 1.0064, 1.0443, 1.0781, 1.0755, 1.0594, 1.0023, 1.0564,\n",
       "                      1.0398, 1.0719, 1.0407, 1.0545, 1.0643, 1.0046], device='cuda:0')),\n",
       "             ('blocks.5.ln2.bias',\n",
       "              tensor([ 0.0553,  0.1200,  0.0542, -0.0564, -0.0849,  0.1141,  0.0021,  0.0918,\n",
       "                      -0.0757, -0.0157, -0.0434,  0.0235,  0.1090,  0.0015,  0.1141,  0.1577,\n",
       "                      -0.0557, -0.0615, -0.0620,  0.1011, -0.1094, -0.0764, -0.0998,  0.0938,\n",
       "                       0.1071,  0.1489,  0.0761, -0.1044, -0.0722, -0.0768,  0.1255,  0.0003,\n",
       "                       0.0128,  0.1189,  0.0573,  0.1338, -0.1001, -0.0710, -0.1081, -0.1131,\n",
       "                      -0.0934,  0.1307, -0.0336, -0.1590, -0.0032, -0.0688, -0.1268,  0.0783,\n",
       "                       0.0732, -0.0246,  0.0701,  0.0872,  0.0978, -0.1482, -0.0989,  0.0464,\n",
       "                      -0.1153,  0.0925, -0.1186,  0.0358, -0.0386, -0.0869,  0.0468,  0.0139,\n",
       "                       0.0354,  0.0833, -0.0138,  0.0735,  0.0698,  0.1091,  0.0860,  0.0632,\n",
       "                       0.0659, -0.0781, -0.0734,  0.1304,  0.0325, -0.1589, -0.0046, -0.0308,\n",
       "                       0.0095,  0.1255, -0.1568,  0.0073, -0.1225, -0.0989,  0.1175,  0.0178,\n",
       "                       0.0780, -0.0501,  0.0728,  0.0725,  0.0902, -0.0947,  0.0421,  0.0205,\n",
       "                      -0.1153, -0.1127, -0.1496, -0.0428,  0.0028,  0.0439,  0.1435,  0.0526,\n",
       "                       0.1072, -0.0648,  0.0348,  0.0423,  0.0088, -0.0887, -0.0210,  0.1038,\n",
       "                      -0.0651,  0.1014, -0.0429,  0.0631,  0.0358,  0.0155,  0.1084, -0.1332,\n",
       "                       0.0999, -0.1294,  0.1457,  0.0951,  0.1109,  0.0265,  0.0124,  0.1064,\n",
       "                      -0.0455, -0.1037,  0.0150,  0.0953,  0.0576,  0.0619, -0.0962,  0.1321,\n",
       "                       0.0059, -0.0802, -0.1165, -0.1181, -0.0865,  0.0480, -0.0717, -0.1060,\n",
       "                      -0.0481,  0.1177, -0.1302,  0.0320, -0.0164, -0.0037, -0.0775, -0.0531,\n",
       "                      -0.0204,  0.1166,  0.0532, -0.0580,  0.0596, -0.1476, -0.1249, -0.0887,\n",
       "                      -0.1159,  0.0824, -0.0578, -0.1013,  0.0890, -0.1426, -0.0729,  0.1418,\n",
       "                       0.0285,  0.0905,  0.0927, -0.0885,  0.0464,  0.0858,  0.1234, -0.1183,\n",
       "                       0.0092,  0.0062,  0.1008, -0.0152, -0.0120,  0.0520, -0.0186,  0.1007,\n",
       "                      -0.1001,  0.0081, -0.0024,  0.0259,  0.1063, -0.1166, -0.0690, -0.0976,\n",
       "                      -0.0447,  0.0320, -0.1408,  0.0094, -0.0400, -0.0837, -0.0743,  0.0820,\n",
       "                       0.0798,  0.0593,  0.0727, -0.1469, -0.0082, -0.0950, -0.0805, -0.0832,\n",
       "                       0.0174, -0.0743,  0.0173, -0.0123, -0.1053,  0.0606, -0.0718, -0.0231,\n",
       "                       0.1059, -0.0386,  0.1686, -0.0122,  0.1152, -0.0598, -0.0675, -0.0664,\n",
       "                      -0.0362, -0.1074,  0.1126, -0.0753,  0.0360, -0.0153, -0.1074,  0.0252,\n",
       "                      -0.1157, -0.0006,  0.0371, -0.0740, -0.0485,  0.1620,  0.0406, -0.0630,\n",
       "                      -0.1142,  0.1048, -0.1180,  0.1129,  0.1214,  0.0642, -0.0531, -0.0849,\n",
       "                       0.0373,  0.0806,  0.0513, -0.0621,  0.0318, -0.1349, -0.0465, -0.1389,\n",
       "                      -0.0957,  0.1315,  0.0511, -0.0892,  0.0483,  0.0165, -0.0607, -0.1635,\n",
       "                       0.0263,  0.0990, -0.1196,  0.0624, -0.0442,  0.1064,  0.1210,  0.0774,\n",
       "                      -0.0984, -0.0057, -0.0769, -0.0742,  0.0239, -0.0163, -0.0853, -0.1478,\n",
       "                      -0.1007, -0.0157,  0.0021, -0.0636,  0.1208, -0.0259, -0.1046, -0.0952,\n",
       "                      -0.0395,  0.0871,  0.0208,  0.1335,  0.0226,  0.0706, -0.1436,  0.0839,\n",
       "                      -0.0774,  0.0860,  0.1048,  0.0986, -0.0708, -0.0026, -0.0981,  0.0380,\n",
       "                       0.1192, -0.1151,  0.1607, -0.0575,  0.1590, -0.0235,  0.0799, -0.0821,\n",
       "                       0.0887,  0.0119,  0.0194,  0.0651,  0.1472, -0.1270, -0.1033, -0.1180,\n",
       "                       0.1056,  0.1268,  0.1400,  0.0717, -0.0115,  0.0687, -0.0961,  0.0486,\n",
       "                      -0.1190,  0.1470,  0.0195, -0.0801, -0.1282,  0.1251, -0.0341,  0.0175,\n",
       "                       0.0023,  0.0249, -0.0685,  0.0193, -0.1667,  0.0976, -0.0321,  0.0903,\n",
       "                      -0.1458,  0.0965, -0.1225, -0.0743, -0.0173, -0.0130,  0.1288,  0.0402,\n",
       "                       0.0961, -0.0607,  0.1428,  0.0420,  0.0997, -0.0438, -0.1078, -0.1318,\n",
       "                       0.1247,  0.0589,  0.0087,  0.1248, -0.0334, -0.1029, -0.1599, -0.0873,\n",
       "                      -0.0637, -0.1025,  0.1244,  0.0133,  0.0438,  0.1073, -0.1482,  0.0794,\n",
       "                      -0.0233,  0.0715,  0.0497, -0.0772,  0.1059,  0.0407,  0.1185, -0.0050],\n",
       "                     device='cuda:0')),\n",
       "             ('ln_f.weight',\n",
       "              tensor([1.7196, 1.6682, 1.7194, 1.6953, 1.6951, 1.6758, 1.6772, 1.7171, 1.7232,\n",
       "                      1.6706, 1.7136, 1.6845, 1.6774, 1.7049, 1.6563, 1.6642, 1.6986, 1.6379,\n",
       "                      1.7167, 1.7147, 1.7218, 1.7301, 1.6963, 1.6349, 1.6556, 1.6264, 1.6929,\n",
       "                      1.6898, 1.6055, 1.7006, 1.6799, 1.7100, 1.6813, 1.6636, 1.5784, 1.7020,\n",
       "                      1.6700, 1.7006, 1.6622, 1.6734, 1.6807, 1.7109, 1.6766, 1.7053, 1.6497,\n",
       "                      1.6248, 1.6646, 1.6608, 1.6764, 1.6811, 1.6583, 1.6424, 1.6834, 1.6901,\n",
       "                      1.6999, 1.6937, 1.6445, 1.6737, 1.6688, 1.6971, 1.6168, 1.6758, 1.6766,\n",
       "                      1.7178, 1.7220, 1.6428, 1.7223, 1.6933, 1.6911, 1.6748, 1.6778, 1.6370,\n",
       "                      1.7175, 1.6662, 1.6692, 1.6691, 1.6645, 1.6985, 1.6889, 1.6502, 1.6944,\n",
       "                      1.6582, 1.5931, 1.7125, 1.7244, 1.6859, 1.6759, 1.6414, 1.6959, 1.6283,\n",
       "                      1.6653, 1.6957, 1.7138, 1.6873, 1.6763, 1.6198, 1.6833, 1.7101, 1.6507,\n",
       "                      1.6604, 1.7073, 1.6985, 1.6802, 1.6753, 1.6841, 1.6341, 1.7141, 1.7300,\n",
       "                      1.6993, 1.6868, 1.6676, 1.7131, 1.7217, 1.7258, 1.5714, 1.7010, 1.6680,\n",
       "                      1.7409, 1.5705, 1.7263, 1.7060, 1.6835, 1.6414, 1.6231, 1.6852, 1.6394,\n",
       "                      1.6863, 1.6654, 1.4711, 1.7156, 1.6678, 1.6796, 1.6853, 1.7092, 1.6637,\n",
       "                      1.6672, 1.6978, 1.7164, 1.6738, 1.3890, 1.6621, 1.6499, 1.6206, 1.7165,\n",
       "                      1.6866, 1.6721, 1.6633, 1.7023, 1.7121, 1.6398, 1.6527, 1.7069, 1.6569,\n",
       "                      1.7066, 1.7013, 1.6672, 1.6824, 1.6293, 1.6783, 1.6814, 1.6934, 1.7310,\n",
       "                      1.6503, 1.6289, 1.7041, 1.6769, 1.7309, 1.6568, 1.6798, 1.7011, 1.6873,\n",
       "                      1.5909, 1.6523, 1.6863, 1.6064, 1.6916, 1.6512, 1.6708, 1.6886, 1.4731,\n",
       "                      1.6834, 1.7035, 1.6961, 1.3108, 1.7238, 1.6686, 1.6956, 1.6892, 1.6829,\n",
       "                      1.6909, 1.6550, 1.6736, 1.7150, 1.6720, 1.6871, 1.7113, 1.7252, 1.6667,\n",
       "                      1.6920, 1.6806, 1.7192, 1.6490, 1.7064, 1.6897, 1.7048, 1.7262, 1.6514,\n",
       "                      1.6262, 1.6623, 1.7006, 1.6395, 1.6889, 1.7016, 1.7107, 1.7090, 1.6703,\n",
       "                      1.6464, 1.6830, 1.6406, 1.6175, 1.7235, 1.6583, 1.6444, 1.6950, 1.6601,\n",
       "                      1.5511, 1.5814, 1.6378, 1.7181, 1.6998, 1.5250, 1.7145, 1.6719, 1.7232,\n",
       "                      1.6724, 1.6986, 1.6722, 1.6473, 1.7648, 1.6236, 1.6611, 1.6572, 1.6658,\n",
       "                      1.6572, 1.7129, 1.7205, 1.6716, 1.6172, 1.7298, 1.7130, 1.6940, 1.6355,\n",
       "                      1.6720, 1.6745, 1.6476, 1.7041, 1.6792, 1.4966, 1.6544, 1.6462, 1.6917,\n",
       "                      1.6677, 1.6499, 1.6341, 1.6915, 1.6623, 1.7008, 1.6994, 1.6596, 1.6943,\n",
       "                      1.6803, 1.6900, 1.6644, 1.7139, 1.6818, 1.6921, 1.6725, 1.6404, 1.6803,\n",
       "                      1.6900, 1.5160, 1.5482, 1.6776, 1.6614, 1.6622, 1.6579, 1.6289, 1.6777,\n",
       "                      1.5962, 1.6600, 1.7231, 1.7098, 1.6729, 1.6962, 1.7134, 1.6928, 1.5345,\n",
       "                      1.6740, 1.7028, 1.7096, 1.7056, 1.6881, 1.7097, 1.7153, 1.6796, 1.6755,\n",
       "                      1.7008, 1.6837, 1.6174, 1.6589, 1.6014, 1.6937, 1.6694, 1.6454, 1.7099,\n",
       "                      1.6447, 1.6701, 1.6861, 1.7381, 1.6947, 1.7268, 1.7146, 1.6263, 1.6754,\n",
       "                      1.7033, 1.6941, 1.6867, 1.7350, 1.7031, 1.6794, 1.7066, 1.6834, 1.6534,\n",
       "                      1.6830, 1.6310, 1.6596, 1.4939, 1.6478, 1.6259, 1.6994, 1.7037, 1.6915,\n",
       "                      1.6475, 1.7386, 1.6578, 1.5691, 1.6892, 1.6925, 1.6958, 1.6748, 1.6891,\n",
       "                      1.6696, 1.7031, 1.6882, 1.5883, 1.6842, 1.6860, 1.7294, 1.6852, 1.7111,\n",
       "                      1.4321, 1.7224, 1.6668, 1.6424, 1.6884, 1.6094, 1.6927, 1.4594, 1.7118,\n",
       "                      1.6738, 1.6883, 1.7188, 1.6520, 1.6938, 1.6986, 1.6559, 1.6859, 1.5507,\n",
       "                      1.6783, 1.7113, 1.5397, 1.6874, 1.6767, 1.6928], device='cuda:0')),\n",
       "             ('ln_f.bias',\n",
       "              tensor([ 0.1439,  0.1612,  0.1863, -0.1950, -0.1671,  0.1740,  0.1477,  0.1866,\n",
       "                      -0.1991, -0.1964, -0.1919,  0.1439,  0.1703, -0.1510,  0.2132,  0.2023,\n",
       "                      -0.1919, -0.1444, -0.1557,  0.2282, -0.1951, -0.2058, -0.2433,  0.2375,\n",
       "                       0.1673,  0.2383,  0.1688, -0.1695, -0.1104, -0.1588,  0.1851, -0.1706,\n",
       "                       0.1980,  0.1637,  0.1431,  0.1544, -0.1764, -0.1675, -0.1901, -0.1410,\n",
       "                      -0.1821,  0.1433, -0.1790, -0.1954,  0.1419, -0.1473, -0.2176,  0.1960,\n",
       "                       0.1765, -0.1226,  0.1313,  0.1432,  0.1465, -0.1967, -0.1401,  0.1307,\n",
       "                      -0.1933,  0.1273, -0.1396,  0.1945, -0.1214, -0.1738,  0.1673,  0.1447,\n",
       "                       0.1902,  0.1761, -0.1682,  0.1462,  0.1606,  0.2008,  0.1582,  0.1704,\n",
       "                       0.1924, -0.1866, -0.1967,  0.1567,  0.1861, -0.2216, -0.1835, -0.1159,\n",
       "                       0.1541,  0.1400, -0.1546, -0.1203, -0.1823, -0.1386,  0.2172,  0.1484,\n",
       "                       0.1885,  0.0541,  0.1643,  0.1744,  0.1754, -0.2006,  0.1952,  0.1440,\n",
       "                      -0.1539, -0.1803, -0.1507, -0.1711, -0.1307,  0.1568,  0.1908,  0.1496,\n",
       "                       0.2250, -0.1243,  0.1435,  0.1536,  0.1088, -0.2124, -0.1679,  0.2342,\n",
       "                      -0.1488,  0.1808, -0.1165,  0.1574,  0.1535,  0.1533,  0.1244, -0.1678,\n",
       "                       0.1893, -0.2068,  0.1810,  0.1235,  0.1549,  0.1529,  0.1232,  0.1251,\n",
       "                      -0.0852, -0.1927,  0.1602,  0.1520,  0.1317,  0.1681, -0.1420,  0.1809,\n",
       "                      -0.1322, -0.1834, -0.1537, -0.1010, -0.1628,  0.1360, -0.1167, -0.1921,\n",
       "                      -0.1710,  0.1815, -0.1517,  0.1316,  0.1635, -0.1453, -0.1329, -0.2103,\n",
       "                       0.1403,  0.2098,  0.1585, -0.1882,  0.1589, -0.1392, -0.1853, -0.1948,\n",
       "                      -0.1913,  0.1558, -0.1796, -0.0855,  0.2088, -0.1131, -0.1800,  0.1353,\n",
       "                      -0.1091,  0.1690,  0.1980, -0.1244,  0.1731,  0.1178,  0.0772, -0.1897,\n",
       "                       0.1566, -0.0250,  0.1692, -0.0961, -0.1674,  0.1530, -0.1450,  0.0787,\n",
       "                      -0.1550, -0.1337, -0.1692,  0.1450,  0.1783, -0.1776, -0.1609, -0.1755,\n",
       "                      -0.1521,  0.1435, -0.1588, -0.0732, -0.1583, -0.2213, -0.1329,  0.2442,\n",
       "                       0.1618,  0.1400,  0.2080, -0.1896,  0.1500, -0.1765, -0.2393, -0.1567,\n",
       "                       0.1925, -0.1610,  0.1845, -0.1508, -0.2163,  0.1493, -0.1775, -0.1627,\n",
       "                       0.1542, -0.1608,  0.1325,  0.1291,  0.1732, -0.1377, -0.1418, -0.1445,\n",
       "                      -0.1775, -0.1653,  0.1529, -0.2348,  0.2013, -0.1567, -0.1157,  0.1956,\n",
       "                      -0.1673,  0.1170,  0.1732, -0.1608, -0.2159,  0.1333,  0.1699, -0.1254,\n",
       "                      -0.1612,  0.1694, -0.1682,  0.1435,  0.1601,  0.1698, -0.1636, -0.1169,\n",
       "                       0.1498,  0.1925,  0.1577,  0.1347,  0.2009, -0.1506, -0.1388, -0.1695,\n",
       "                      -0.1779,  0.1179,  0.1596, -0.1694,  0.1112,  0.1868, -0.1443, -0.2365,\n",
       "                       0.1274,  0.1539, -0.1506,  0.1620, -0.2102,  0.1520,  0.1898,  0.1549,\n",
       "                      -0.2114,  0.1337, -0.1947, -0.2242,  0.2057,  0.1709, -0.1720, -0.1929,\n",
       "                      -0.1324, -0.1261, -0.1448, -0.1570,  0.1807,  0.1476, -0.1913, -0.1959,\n",
       "                      -0.1336,  0.1204,  0.1528,  0.1725,  0.1362,  0.1509, -0.2044,  0.1574,\n",
       "                      -0.1150,  0.1398,  0.2051,  0.1756, -0.1568, -0.1493, -0.1631,  0.1765,\n",
       "                       0.1837, -0.1736,  0.1795, -0.1684,  0.1521, -0.1472,  0.1152, -0.1695,\n",
       "                       0.1806, -0.0243,  0.1393,  0.1781,  0.2113, -0.1845, -0.1625, -0.1512,\n",
       "                       0.1780,  0.2199,  0.2473,  0.1385,  0.1747,  0.1729, -0.2297,  0.1782,\n",
       "                      -0.1697,  0.2208,  0.1756, -0.2140, -0.2165,  0.1710, -0.1213, -0.1241,\n",
       "                      -0.1115, -0.1421, -0.1520,  0.1908, -0.1681,  0.1856,  0.1545,  0.1615,\n",
       "                      -0.2062,  0.1299, -0.1910, -0.1619, -0.1892, -0.1718,  0.1907,  0.1399,\n",
       "                       0.1799, -0.1527,  0.1336,  0.1201,  0.1860, -0.1740, -0.1605, -0.1936,\n",
       "                       0.1083,  0.1710,  0.1316,  0.1446,  0.1353, -0.1471, -0.1778, -0.1013,\n",
       "                      -0.1829, -0.2409,  0.1974, -0.1500,  0.1354,  0.1539, -0.1712,  0.1751,\n",
       "                       0.1630,  0.1184,  0.1290, -0.1435,  0.1318,  0.1872,  0.2179, -0.1389],\n",
       "                     device='cuda:0')),\n",
       "             ('lm_head.weight',\n",
       "              tensor([[ 0.0816,  0.0497,  0.0421,  ..., -0.1766, -0.0995, -0.0923],\n",
       "                      [-0.0885,  0.0414, -0.0208,  ...,  0.0417,  0.1488, -0.0730],\n",
       "                      [-0.0652, -0.0168, -0.0312,  ...,  0.0555, -0.1299, -0.0547],\n",
       "                      ...,\n",
       "                      [-0.0418, -0.0179, -0.0133,  ..., -0.0412, -0.0486,  0.0097],\n",
       "                      [-0.1603, -0.0477, -0.0987,  ...,  0.0424, -0.0253,  0.0810],\n",
       "                      [ 0.0721,  0.0587, -0.0668,  ..., -0.0870,  0.0382, -0.0448]],\n",
       "                     device='cuda:0')),\n",
       "             ('lm_head.bias',\n",
       "              tensor([-0.0667,  0.0653, -0.0156,  ..., -0.0538, -0.0349, -0.0226],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"Next Word Predictor_EPOCH400_v31.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2aee20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s.split(\" \")] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ' '.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aafb2663",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d17b158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neighbour grass is always green envy fiddler bond key sob easeful inherits unbarbed directed therewith'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"neighbour grass is always green\".lower()\n",
    "context = np.array(encode(text)).reshape(-1,len(text.split(' ')))[:3]\n",
    "context =torch.tensor(context, dtype=torch.int32,device=device) \n",
    "answer = m.generate(context, max_new_tokens=10)[0].tolist()\n",
    "decode(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf726c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a9f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
